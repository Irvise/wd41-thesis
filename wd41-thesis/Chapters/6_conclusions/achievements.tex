%************************************************************************************************************************************
\section[Achievements and Recommendations]{Main Achievements and Recommendations for Future Work}\label{sec:conclusions_achievements}
%************************************************************************************************************************************

% Main Objectives Revisited
The thesis proposed the application of a set of methods adapted from the applied literature with the goal of quantifying the uncertainty of model parameters in a \gls[hyper=false]{th} system code.
The application of each method was illustrated and demonstrated on the basis of a reflood experiment simulation model in the \gls[hyper=false]{trace} code.
According to Section~\ref{sub:intro_objectives} the listed objectives of the proposed methods were to:
\begin{itemize}
	\item analyze and better understand the inputs/outputs relationship in a computer simulation with uncertain input;
	\item approximate the inputs/outputs relationship of a complex computer simulation for a faster evaluation; and,
	\item calibrate the physical model parameters against various relevant experimental data.
\end{itemize}

% What was done
During the course of this doctoral research, each of these methods was investigated and applied to the running example of the \gls[hyper=false]{feba} reflood facility simulation model in the \gls[hyper=false]{trace} code. 
Each of these applications was aimed to illustrate the particularities -- and difficulties -- of applying the method to the \gls[hyper=false]{trace} model as well as to demonstrate the values of the method.
Chapters~\ref{ch:gsa}--\ref{ch:bayesian_calibration} provided a detailed account on the methods and their applications, of which the main achievements are highlighted below.
Given the limited scope and duration of the project, many difficulties found along the way remained unaddressed, and are the basis for the proposed recommendations.

% PREMIUM
The thesis project was initiated by the participation of \glsentryshort{psi}-\glsentryshort{lrs} to the \gls[hyper=false]{oecd}/\gls[hyper=false]{nea} \gls[hyper=false]{premium} benchmark.
The work related to that participation also constitutes a portion -- and achievement -- of the thesis project.

% Publications
Four papers were presented in international conferences \cite{Wicaksono2014,Wicaksono2014a,Wicaksono2015,Wicaksono2016}, a journal article was published \cite{Wicaksono2016b}, and two contributions were submitted \cite{Wicaksono2016a,Zerkak2016} to the \gls[hyper=false]{premium} project and included in the NEA reports \cite{Reventos2016,Sanz2017}.

%-----------------------------------------------------
\subsection{Contributions to OECD/NEA PREMIUM Project}
%-----------------------------------------------------

% Contributions to PREMIUM
The \gls[hyper=false]{trace} model of \gls[hyper=false]{feba} was successfully developed within that context and became the basis for several follow-up studies.
\marginpar{TRACE model of FEBA}
The model is stable and is relatively quick to run allowing even a relatively brute force sensitivity analysis method to be applied.
It is now part of the in-house \gls[hyper=false]{trace} code validation database at \glsentryshort{lrs}.

% Prior quantification
The prior uncertainties of the input parameters were quantified under the supervision of thermal-hydraulics experts at \glsentryshort{lrs} \cite{Zerkak2016}.
\marginpar{Contribution to PREMIUM, prior uncertainty quantification}
The quantified uncertainties were then propagated both in the \gls[hyper=false]{trace} models of \gls[hyper=false]{feba} and PERICLES (another reflood facility not presented in this thesis).
The results of the propagation submitted to the \gls[hyper=false]{premium} project were deemed satisfactory as it served the purpose of the prior quantification.
That is, the prediction uncertainties of both facilities were wide but covered the experimental data well, confirming that the prior range was not underestimated.

% trace simexp
Still within the context of \gls[hyper=false]{premium}, a python scripting tool was developed to assist conducting computer experiment on the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}.
\marginpar{\texttt{trace-simexp}}
The tool \texttt{trace-simexp} has reached a stable version, is well documented, and has been applied in several follow-up studies within and outside the scope of the present doctoral research.

\paragraph{Recommendations for Future Work}\mbox{}\\

Although stable, the current version of \texttt{trace-simexp} has been only tested so far for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}.
Extension to other \gls[hyper=false]{trace} models are feasible.
However, depending on the complexity of those models and the computing infrastructure, further development of the tool might become unrealistic. 
In the long run, it would be better to opt for the use of an integrated uncertainty framework (e.g., \texttt{UQLab} \cite{Marelli2014}, \texttt{Dakota} \cite{Adams2017}, \texttt{OpenTurns} \cite{Baudin2017}, \texttt{Uranie} \cite{Gaudier2010}).
Typically, such framework supports an application programming interface (API) to make a connection with an external simulation model or to a third-party program.
It does require an initial effort of getting acquainted with the terminologies the framework but in the long run for a generic complex model these are the preferred solution.

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection[Implementation and application of GSA methods]{Implementation and application of GSA methods (to analyze and better understand the inputs/outputs relationship in a computer simulation with uncertain input)}
%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

% Sensitivity Analysis, Morris
The size of the initial selection of input parameters, as exemplified in \gls[hyper=false]{premium}, can be large. 
\marginpar{Implementation and application of screening methods}
Lacking prior knowledge, the selection should also include all the parameters that are vaguely perceived as important. 
The implementation and the application of screening methods (Morris screening method and Sobol' total-effect indices), as demonstrated in this thesis for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba},
allows for a quick, systematic, and quantitative screening of the initial set of input parameters in a global manner (i.e., simultaneous perturbation over the whole range of parameter uncertainties).
In the case studied here, more than half of the initial parameters were found to be non-influential to the relevant outputs of the reflood simulation.

% FDA
In accordance with the aim of increasing the understanding of inputs/outputs relationship in a simulation, a novel set of \glspl[hyper=false]{qoi} was derived using \gls[hyper=false]{fda} techniques to characterize the overall time-dependent output variation.
\marginpar{Application of GSA coupled with FDA}
It was able to capture the most essential features of the model behavior through its time-dependent output, thus significantly departing from the more conventional \glspl[hyper=false]{qoi} (e.g., minimum, maximum, or time-average scalar value) that have been used so far in similar \gls[hyper=false]{sa} studies of \gls[hyper=false]{th} simulation model.
The resulting \glspl[hyper=false]{qoi} were then coupled with the \gls[hyper=false]{gsa} methods (Sobol' main- and total-effect indices) to investigate, quantitatively, the effect of the input parameters on the overall time-dependent outputs.
When considering \gls[hyper=false]{fda}-based QoIs, which better represents the whole transient of an output, it was found that the important parameters and the nature of their interactions were changing during the transient.
The nature of these interactions, however, remains to be investigated and is outside the scope of this thesis.

% GSA
Finally, the implementations of the employed \gls[hyper=false]{gsa} methods were developed in-house as a python module to allow full internal control on the implementation.
\marginpar{\texttt{gsa-module}}
The module \texttt{gsa-module} has been documented and tested against a suite of test functions.
It was applied to obtain all the results presented in Chapter~\ref{ch:gsa}.

\paragraph{Recommendations for Future Work}\mbox{}\\

% Advanced FDA
The landmark registration procedure to separate the phase and amplitude variations in a functional data set is one of the most straightforward procedure available.
However, landmarks might not always be visible and miss-specification might affect the downstream analysis.
As seen in this thesis, slight residual variation during quenching persisted after registration which caused an inflated (artifact) sensitivity indices around the vicinity of quenching.
Therefore, a more automatic registration technique is worth investigating and applied to different types of functional data of interest in \gls[hyper=false]{th} simulation.

% sobol decomposition and metamodel
In this thesis, \glsfirst[hyper=false]{mc} simulation was used to estimate the Sobol' sensitivity indices (main- and total-effect).
Though it was considered affordable for the analysis of the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}, this will become a bottleneck for an application of the method to wide range of computationally expensive transient simulations.
In such cases, an alternative approach to compute the indices is required.

% gsa module and uncertainty framework
The \texttt{gsa-module} was developed during the course of the thesis with the idea of implementing the available methods from the literature in a quick manner; 
without having to deal with the learning curve of adopting existing framework.
Additionally, such an approach allows for a full control on the implementation.
The structure of the module makes it easy to be extended for other \gls[hyper=false]{gsa} methods.
However, state of the art uncertainty quantification frameworks such as the ones mentioned above are much more powerful and some are actively developed with substantial user base.  
Thus, for an advanced \gls[hyper=false]{gsa} methods that are already implemented in any such frameworks, it is worthwhile to simply adopt the frameworks in the future.  


%---------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection[Development and validation of a TRACE metamodel]{Development and validation of a TRACE metamodel (to approximate the inputs/outputs relationship of a complex computer simulation for a faster evaluation)}
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------

% Metamodeling and Bayesian
\glsfirst[hyper=false]{gp} metamodeling has been demonstrated for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}, which has high-dimensional outputs.
In this thesis, the high-dimensionality of the outputs was treated by \gls[hyper=false]{pca} resulting in a \gls[hyper=false]{gp} \gls[hyper=false]{pc} metamodel.
The validation and testing steps then showed that the error of the metamodel across the prior range of input parameters were within a reasonable range.
In other words, it managed to approximate the important features of the inputs/outputs relationship of the reflood simulation model in \gls[hyper=false]{trace}.
Using the \glsfirst[hyper=false]{gp} \glsfirst[hyper=false]{pc} as a surrogate for TRACE, the prediction for arbitrary input parameter values could be made much faster
(i.e., $< 5\,[s]$ per metamodel evaluation vs. $6 - 15\,[min]$ per TRACE).
The thesis has also demonstrated the applicability of \gls[hyper=false]{pca} to reduce the high dimension of the output.
The technique performed best for relatively smooth outputs (in this particular application, the pressure drop and liquid carryover transients), while it performed worse for reconstructing an output exhibiting strong discontinuity (e.g., the clad temperature output exhibited a discontinuity around quenching).
Finally, though many practical aspects were involved in the construction of the metamodel, the work in the thesis concluded that the size of the training sample (i.e., the actual number of code runs) was the most important factor;
if they can be afforded, more runs should be conducted.

\paragraph{Recommendations for Future Work}\mbox{}\\

The worse performance of the \gls[hyper=false]{pca} on reconstructing the clad temperature output was, in turn, due to the use of \gls[hyper=false]{pca} as the linear dimension reduction.
\marginpar{Alternative dimension reduction technique}
As such, a first step of improvement in this regard can be aimed toward replacing PCA with another more advanced, dimension reduction tool.
Simulations with high-dimensional outputs, either in time or space, are typical in \gls[hyper=false]{th} analysis.
It is thus worth investigating the application of different dimension reduction techniques, linear (extension of \gls[hyper=false]{pca}, e.g., \cite{Zhang2005}) or nonlinear (e.g., isomap \cite{Tenenbaum2000} and locally linear embedding (LLE) \cite{Roweis2000}).
Many of such developments are made in the area of image processing.
Indeed as shown in Chapter~\ref{ch:gp_metamodel}, a $1$-dimensional time-dependent \gls[hyper=false]{trace} simulation output can be represented as an image.

Furthermore, \gls[hyper=false]{gp} metamodel is not the only available metamodeling technique.
\marginpar{Alternative metamodeling techniques}
The response surface method was traditionally employed for \gls[hyper=false]{th} system analysis but more advanced techniques are currently available such as the ones mentioned in Section~\ref{sub:intro_statistical_metamodeling}. 
The investigation on their applicability -- the predictive performance and the computational cost of construction -- for a variety of \gls[hyper=false]{th} models is of interest in its own right.

Finally, the step proposed in this thesis is to conduct sensitivity analysis before constructing the metamodel.
\marginpar{Alternative workflow}
In that case, metamodeling error can be excluded from the sensitivity analysis.
However, it is also possible to construct the metamodel before moving on to the sensitivity analysis step.
Some metamodeling techniques allow the metamodeling and sensitivity analysis to be combined while providing an estimate of the associated error.
In particular \gls[hyper=false]{pce} allows the computation of Sobol' sensitivity indices by post-processing the resulting coefficients of the expansion \cite{Sudret2008}.

%-----------------------------------------------------------------------------------------------------------------
\subsection{Bayesian calibration of the TRACE reflood model parameters against various relevant experimental data}
%-----------------------------------------------------------------------------------------------------------------

% Main Achievements
Bayesian calibration was successfully applied quantify the uncertainty of the selected \gls[hyper=false]{trace} reflood model parameters on the basis of the \gls[hyper=false]{feba} experiments.
Different posterior distributions of the model parameters, corresponding to different calibration assumptions, were formulated and directly sampled from using an \gls[hyper=false]{mcmc} ensemble sampler.
The uncertainty propagation of each resulting posterior samples was conducted on all \gls[hyper=false]{feba} tests and the results were compared in terms of \emph{informativeness} (the width of the prediction uncertainty band) and \emph{calibration score} (consistency with the experimental data and coverage by the uncertainty band).

The value of incorporating model bias term in the calibration process has also been demonstrated.
Without the model bias term, the calibration results exhibited stronger symptom of overfitting,
i.e., although the prediction uncertainty band was narrower, more experimental data points fell outside the band.
At the same time, the posterior uncertainties from the calibration scheme with model bias term resulted in a particular correlation structure that might be overly specific to the calibration data.
Indeed, though better in terms of calibration score with respect to the scheme without model bias term, the posterior with bias term had consistently lower calibration scores across all \gls[hyper=false]{feba} tests compared to that of the prior.
By removing a strongly correlated parameter from the calibration -- and keeping it at its prior uncertainty -- the resulting posterior prediction uncertainty was found to have a much improved informativeness with similar level of calibration scores across all \gls[hyper=false]{feba} tests compared to that of the prior.
Therefore, it can be argued that the calibration resulted in a posterior range and a posterior correlation structure which were, by construction, specific to the calibration data. 
However, if the calibration data was deemed not sufficiently large or comprehensive enough (here it was based on one \gls[hyper=false]{feba} test run) then care should be taken to avoid overfitting. 
In this particular case, the calibration by excluding a strongly correlated parameter was proved to be a compromise and a pragmatic solution.

Another type of parameter non-identifiability was also encountered during the calibration process.
The non-identifiability due to the parameter insensitivity with respect to a type of experimental data was solved by employing a calibration scheme that incorporated multiple types of experimental data simultaneously.

\paragraph{Recommendations for Future Work}\mbox{}\\

% Hierarchical Model
It is worth noting that the calibration conducted in the present doctoral research was based only on the data from one \gls[hyper=false]{feba} test.
The formulation of the model bias term only considered the bias from one experimental boundary conditions.
The applicability of the resulting posterior uncertainty is only applicable insofar that the assumed bias from the calibration data is valid for the test data (i.e., all the other \gls[hyper=false]{feba} tests).
For \gls[hyper=false]{feba}, different experimental conditions leads to different bias structure of the model, which was apparent in the case of test Nos. $223$ and $218$ (i.e., tests with lower system backpressure compared to test for the calibration).
Indeed the propagation of the posterior uncertainties performed poorly in such situation.
 
A more comprehensive calibration procedure should therefore take into account the difference in the bias structure from different experimental conditions.
One possibility is to concatenate all the experimental data of \gls[hyper=false]{feba} into a single calibration process resulting in a posterior uncertainty of model parameters that takes into account all available experimental boundary conditions of \gls[hyper=false]{feba}. 
However, to understand better the difference between the conditions, an alternative approach is the hierarchical modeling \cite{Unal2011}, which allows the model parameters to take different posterior uncertainties depending on the experimental conditions, while at the same time allowing sharing information from the data across different experimental conditions.
This approach might give a better insight on the model validity and reveal its discrepancy for a particular experimental condition in a more precise manner.

% Correlated parameters, implication and practicalities, f
It should be noted that the model parameters are not of primary interest themselves.
Their calibration against experimental data are aimed at increasing the confidence in their application for the actual plant analysis (or, to a lesser degree, integral effect test facilities).
In its own right, the presence of correlation in the model parameters presents a challenge in the application of the posterior uncertainties.
Model parameters uncertainty are typically assumed to be independent a priori.
After calibration such assumption might not hold anymore.
A consistent propagation of uncertainty should consider the correlation structure that is informed by experimental data.
As was observed in this thesis, the correlation structure of the posterior might not be readily represented as a familiar multivariate Gaussian.
Once more, in this thesis, the issue was sidestep by using directly the posterior samples for the uncertainty propagation thus implicitly capturing the correlation structure.
On how to summarize this correlation structure for the purpose of uncertainty propagation remains an open question 

% 
Regarding computational aspects of the Bayesian calibration, \gls[hyper=false]{mcmc} sampler is the backbone of the method.
In this thesis only one kind of sampler was used and no direct comparison on its performance was made against different kind of sampler.
For robustness, it is necessary to extend the verification study using different \gls[hyper=false]{mcmc} samplers.