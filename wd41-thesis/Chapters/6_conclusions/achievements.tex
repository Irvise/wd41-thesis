%************************************************************************************************************************************
\section[Achievements and Recommendations]{Main Achievements and Recommendations for Future Work}\label{sec:conclusions_achievements}
%************************************************************************************************************************************

% Main Objectives Revisited
The thesis proposed the application of a set of methods adapted from the applied literature with the ultimate goal to quantify the uncertainty of model parameters in a \gls[hyper=false]{th} system code.
The application of each method was illustrated and demonstrated on the basis of a reflood experiment simulation model in the \gls[hyper=false]{trace} code.
According to Section~\ref{sub:intro_objectives} the listed objectives of the proposed methods were to:
\begin{itemize}
	\item analyze and better understand the inputs/outputs relationship in a computer simulation with uncertain input;
	\item approximate the inputs/outputs relationship of a complex computer simulation for a faster evaluation; and,
	\item calibrate the physical model parameters against various relevant experimental data.
\end{itemize}

% What was done
During the course of this doctoral research, each of these methods were investigated and it was applied to the running example of the \gls[hyper=false]{feba} reflood facility simulation model in the \gls[hyper=false]{trace} code. 
Each of these applications was aimed to illustrate the particularities -- and difficulties -- of applying the method the \gls[hyper=false]{trace} model as well as to demonstrate the values of the method.
Chapters~\ref{ch:gsa}--\ref{ch:bayesian_calibration} provided a detailed account on the methods and their applications, of which the main achievements are highlighted below.
Given the limited scope and duration of the project, many difficulties found along the way remained unaddressed, and they are the basis for the proposed recommendations.

% PREMIUM
The thesis project was initiated by the participation of \glsentryshort{lrs} at \gls[hyper=false]{psi} in the \gls[hyper=false]{oecd}/\gls[hyper=false]{nea} \gls[hyper=false]{premium} benchmark.
The work related to that participation also constitutes a portion -- and achievement -- of the thesis project.

% Publications
Finally, four papers were presented in international conferences \cite{Wicaksono2014,Wicaksono2014a,Wicaksono2015,Wicaksono2016}, a journal article was published \cite{Wicaksono2016b}, and two contributions were submitted \cite{Wicaksono2016a,Zerkak2016} to the \gls[hyper=false]{premium} project and included in the NEA reports \cite{Reventos2016,Sanz2017}.

%-----------------------------------------------------
\subsection{Contributions to OECD/NEA PREMIUM Project}
%-----------------------------------------------------

% Contributions to PREMIUM
The work related to the contribution to the \gls[hyper=false]{premium} project comprises the bulk of Chapter~\ref{ch:trace_reflood}.
The \gls[hyper=false]{trace} model of \gls[hyper=false]{feba} was successfully developed within that context and became the basis for several follow-up studies.
\marginpar{TRACE model of FEBA}
The model was stable and was relatively quick to run allowing even a relatively brute force sensitivity analysis method to be applied.
It is now part of the in-house \gls[hyper=false]{trace} code validation database at \glsentryshort{lrs}.

% Prior quantification
The prior uncertainties of the input parameters were quantified under a close supervision by thesis supervisor at \glsentryshort{psi} \cite{Zerkak2016}.
\marginpar{Contribution to PREMIUM, prior uncertainty quantification}
The quantified uncertainties were then propagated both on the \gls[hyper=false]{trace} models of \gls[hyper=false]{feba} and PERICLES (another reflood facility not presented in this thesis).
The result of the propagation submitted to the \gls[hyper=false]{premium} project were deemed satisfactory as it served the purpose of the prior quantification.
That is, the prediction uncertainties of both facilities were wide but covered the experimental data well, confirming that the prior range was not underestimated.

% trace simexp
Still within the context of \gls[hyper=false]{premium}, a python scripting tool was developed to assist in conducting computer experiment on the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}.
\marginpar{\texttt{trace-simexp}}
The tool \texttt{trace-simexp} has reached a stable version, is well documented, and has been applied for several follow-up studies within and without the scope of the present doctoral research.

\paragraph{Recommendations for Future Work}\mbox{}\\

Although stable, the current version of \texttt{trace-simexp} has been only tested so far for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba}.
Extension to other \gls[hyper=false]{trace} models are feasible.
However, depending on the complexity of those models, further development of the tool might become unrealistic and it would be better to opt for the use of an integrated uncertainty framework 
(e.g., \texttt{UQLab}, \texttt{Dakota}, \texttt{OpenTurns}, \texttt{Uranie}).
Typically, such framework supports an application programming interface (API) to a make connection with an external simulation model or to a third-party program.
It does require initial effort of getting acquainted with the terminologies of each framework, some are easier than the other, but in the long run for a generic complex model they might be the solution.

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Implementation and application of GSA methods (to analyze and better understand the inputs/outputs relationship in a computer simulation with uncertain input)}
%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

% Sensitivity Analysis, Morris
Three conference papers \cite{Wicaksono2014,Wicaksono2014a,Wicaksono2015} and a journal article \cite{Wicaksono2016b} made up Chapter~\ref{ch:gsa}.
The size of the initial selection of input parameters, as exemplified in \gls[hyper=false]{premium}, can be large. 
\marginpar{Implementation and application of screening methods}
Lacking prior knowledge, the selection should also include all the parameters that are vaguely perceived as important. 
The implementation and the application of screening methods (Morris screening method and Sobol' total-effect indices), as demonstrated in this thesis for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba},
allows for a quick, systematic, and quantitative screening of the initial set of input parameters in a global manner (i.e. simultaneous perturbation over the whole range of parameter uncertainties).
In the case studied here, more than half of the initial selection were found to be non-influential to the reflood simulation.

% FDA
In conjunction with that, \gls[hyper=false]{fda} techniques to characterize the variation of functional data set was investigated and successfully applied to analyze the variation in reflood curves.
In essence, the application of the \gls[hyper=false]{fda} techniques resulted in the representation of the time-dependent output in a reduced space.
Indeed, time- and space-dependent outputs are ubiquitous in the \gls[hyper=false]{th} analysis thus dimension reduction techniques are worth investigating.
\marginpar{Application of GSA coupled with FDA}
\gls[hyper=false]{gsa} methods and \gls[hyper=false]{fda} techniques were then coupled together to decompose the variance of the output in the reduced space.
This was done through the implementation and application of \gls[hyper=false]{mc} methods to estimate the Sobol', main- and total-effect, indices.  
The sensitivity analysis reveals interesting behavior of the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba} in terms of interactions

% GSA
The implementations of the employed \gls[hyper=false]{gsa} methods were developed in-house as a python module to allow full internal control.
\marginpar{\texttt{gsa-module}}
The module \texttt{gsa-module} is documented and was tested against a suite of test functions and applied to obtain all the results presented in Chapter~\ref{ch:gsa}.

\paragraph{Recommendations for Future Work}\mbox{}\\

% screening methods

% sobol decomposition
 
% gsa module and uncertainty framework

%---------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Development and validation of a TRACE metamodel (to approximate the inputs/outputs relationship of a complex computer simulation for a faster evaluation)}
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------

% Metamodeling and Bayesian
\glsfirst[hyper=false]{gp} metamodeling was demonstrated for the \gls[hyper=false]{trace} model of \gls[hyper=false]{feba} having high-dimensional outputs.
In this thesis, the high-dimensionality of the outputs was treated by \gls[hyper=false]{pca} resulting in a \gls[hyper=false]{gp} \gls[hyper=false]{pc} metamodel.
The validation and testing steps then showed that the error of the metamodel across the prior range of input parameters were within a reasonable range.
In other words, it managed to approximate the important features of the inputs/outputs relationship of the reflood simulation model in \gls[hyper=false]{trace}.
Using the \glsfirst[hyper=false]{gp} \glsfirst[hyper=false]{pc} as the surrogate for TRACE run, the prediction for arbitrary input parameters values could be made much faster
(i.e., $< 5\,[s]$ per metamodel evaluation vs. $6 - 15\,[min]$ per TRACE).

The thesis has demonstrated the applicability of a linear dimension reduction technique to reduce the high dimension of the output.
The technique performed best for relatively smooth output (in this particular application, the pressure drop and liquid carryover transients), while it performed worse for reconstructing an output exhibiting strong discontinuity (i.e., the cladding temperature output exhibited a discontinuity around quenching).
Finally, though many practical aspects were involved in the construction of the metamodel, the work in the thesis concluded that the size of training samples (i.e., the actual code runs) was the most important factor;
if they can be afforded, more runs should be conducted.

\paragraph{Recommendations for Future Work}\mbox{}\\

The worse performance of the \gls[hyper=false]{pca} on reconstructing the cladding temperature output was, in turn, due to the use of \gls[hyper=false]{pca} as the linear dimension reduction.
\marginpar{Alternative dimension reduction technique}
Furthermore, the size of training size might get inflated because of this worse performance:
large training samples was required not because the inputs/outputs relationship was particularly complex, but because reconstruction of the output with small error required more samples. 
As such, a first step of improvement in this regard can be aimed toward replacing PCA with another, more advanced dimension reduction tool.
Simulations with high-dimensional outputs, either in time or space, are typical in \gls[hyper=false]{th} analysis.
It is thus worth investigating the application of different dimension reduction techniques, linear (extension of \gls[hyper=false]{pca}, e.g., \cite{Zhang2005}) or nonlinear (e.g., isomap \cite{Tenenbaum2000} and locally linear embedding (LLE) \cite{Roweis2000}).
Many of such developments are made in the area of image processing.
Indeed as shown in Chapter~\ref{ch:gp_metamodel}, a $1$-dimensional time-dependent \gls[hyper=false]{trace} simulation output can be represented as an image.

Furthermore, \gls[hyper=false]{gp} metamodel is not the only available metamodeling technique.
\marginpar{Alternative metamodeling techniques}
The response surface method was traditionally employed for \gls[hyper=false]{th} system analysis but more advanced techniques are currently available such as the ones mentioned in Section~\ref{sub:intro_statistical_metamodeling}. 
The investigation on their applicability -- the predictive performance and the computational cost of construction -- for a variety of \gls[hyper=false]{th} models is of interest in its own right.

Finally, the step proposed in this thesis is to conduct sensitivity analysis before to construct the metamodel.
\marginpar{Alternative workflow}
In that case, metamodeling error can be excluded from the sensitivity analysis.
However, it is also possible to construct the metamodel in advanced before moving on to the sensitivity analysis step.
Some of metamodeling techniques allows the metamodeling and sensitivity analysis to be combined while providing an estimate of the associated error.
In particular \gls[hyper=false]{pce}, allows the computation of Sobol' sensitivity indices to be computed by post-processing the resulting coefficients of the expansion \cite{Sudret2008}.

%-----------------------------------------------------------------------------------------------------------------
\subsection{Bayesian calibration of the TRACE reflood model parameters against various relevant experimental data}
%-----------------------------------------------------------------------------------------------------------------

% Main Achievements

\paragraph{Recommendations for Future Work}\mbox{}\\