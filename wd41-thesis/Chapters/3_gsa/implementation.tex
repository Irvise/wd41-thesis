\section{Implementation}\label{sec:sa_implementation}

An implementation of the Morris method and a Monte Carlo method to estimate the main- and total-effect sensitivity indices has been developed using the Python programming language 
for the purpose of this work, to allow for well-controlled parametric and convergence studies.
The implementation follows a black box approach of sensitivity analysis. 
It deals with the generation of design of experiment (which is used to evaluate the model or code) and the post-processing of output to obtain the select measures of sensitivity.
In the following, the basic procedures that underlie the implementation of both methods are laid out.
More details on the programming aspects of the implementation (the so-called \texttt{gsa-module}) can be found in Appendix~\ref{app:gsa_module}.

\subsection{The Morris Method}\label{sub:sa_morris}

The implementation of the Morris method follows four sequential steps. 
\textsc{First}, an OAT design matrix consisting of $N_R$ replications is created by randomly sampling the nominal (base) points as well as the perturbed points for each parameter.
A replication in an OAT design consists of $1$ nominal point with $D$ (number of dimensions/parameters) additional perturbed points.
In each of the perturbed points, only one parameter change its value relative to the base.
Different replication yields different nominal point and the associated perturbed points.

\textsc{Second}, each point in the design matrix is scaled to the corresponding point in the $D$-dimensional parameter space of the model parameters.

\textsc{Third}, the model is evaluated for each (rescaled) point in the design matrix. The total number of model evaluations for a given design matrix of size $N_R$ is $N_R \times (D + 1)$.

\textsc{Fourth} and finally, the $N_R$ elementary effects $EE_d$ for each parameter and each trajectory are computed for a selected \gls{qoi}.
Their statistical summaries ($mu_d$, $\mu_d^*$, $\sigma_d$) are then computed, 
and the ranking of the parameters can be constructed based on $\mu_d^*$ for a selected \gls{qoi}.
The $\mu_d^*$ can be directly used to rank the parameters to systematically identify 
and screen out noninfluential parameters (low $\mu_d^*$) from the relatively influential ones (high $\mu_d^*$)~\cite{Campolongo2007}.

\subsection{The Sobol'-Saltelli Method}\label{sub:sa_sobol_saltelli}

In principle, the estimation of the Sobol' indices defined by Eqs.~(\ref{eq:sa_main_effect_index}) and (\ref{eq:sa_total_effect_index}) can be directly carried out using \gls{mc} simulation.
\marginpar{brute force \\ Monte Carlo}
The most straightforward, though rather naive, implementation of \gls{mc} simulation to conduct the estimation is using two nested loops for the computation of the conditional variance and expectation appeared in both equations.

In the estimation of the main-effect index of parameter $x_d$, for instance, the outer loop samples values of $X_d$ while the inner loop samples values of $\mathbf{X}_{\sim d}$ (anything else other than $x_d$). T
The samples, in turn, are used to evaluate the model output.
In the inner loop, the mean of the model output (for a given value of $X_d$ but over many values of $\mathbf{X}_{\sim d}$) is taken. 
Afterwards, in the outer loop, the variance of the model output (over many values of $X_d$) is taken.
This approach can easily become prohibitively expensive as the nested structure requires two $N^2$ model evaluations per input dimension for either the main-effect and total-effect indices, while $N$ (the size of \gls{mc} samples) are typically in the range of $10^2 - 10^4$ for a reliable estimate. 
   
Sobol' \cite{Sobol2001} and Saltelli \cite{Saltelli2002} proposed an alternative approach that circumvent the nested structure of \gls{mc} simulation to estimate the indices.
The formulation starts by expressing the the expectation and variance operators in their integral form.
As the following formulation is defined on a unit hypercube of $D$-dimension parameter space where each parameter is a uniform and independent random variable,
explicit writing of the distribution within the integration as well as the integration range are excluded for conciseness.

First, the variance operator shown in the numerator of Eq.~(\ref{eq:sa_main_effect_index}) is written as
\begin{equation}
  \begin{split}
    \mathbb{V}_{d}[\mathbb{E}_{\sim d}[Y|X_d]] & = \mathbb{E}_{d}[\mathbb{E}_{\sim d}^{2}[Y|X_d]] - \left(\mathbb{E}_{d}[\mathbb{E}_{\sim d}[Y|X_d]]\right)^2 \\ 
                                               & = \int \mathbb{E}_{\sim d}^{2}[Y|X_d] dx_d - \left(\int \mathbb{E}_{\sim d}[Y|X_d] dx_d\right)^2
  \end{split}
\label{eq:ss_variance_integral}
\end{equation}
The notation $\mathbb{E}_{\sim \circ}[\circ | \circ]$ was already explained in Section~\ref{sub:sa_hdmr}, 
while $\mathbb{E}_{\circ} [\circ]$ corresponds to the marginal expectation operator 
where the integration is carried out over the range of parameters specified in the subscript. 

Next, consider the term conditional expectation shown in Eq.~(\ref{eq:ss_variance_integral}), which per definition reads
\begin{equation}
  \mathbb{E}_{\sim d} [Y|X_d] = \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d}
\label{eq:ss_expectation_integral}
\end{equation}
Note that $\mathbf{x} = \{\mathbf{x}_{\sim d}, x_d\}$.

Following the first term of Eq.~(\ref{eq:ss_variance_integral}), by squaring Eq.~(\ref{eq:ss_expectation_integral})
and by defining a dummy vector variable $\mathbf{x}^{\prime}_{\sim d}$, 
the product of the two integrals can be written in terms of a single multiple integrals
\begin{equation}
  \begin{split}
    \mathbb{E}_{\sim d}^{2} [Y|X_d] & = \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d} \cdot \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d} \\
                                    & = \int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d}
  \end{split}
\label{eq:ss_multiple_integrals}
\end{equation}

Returning to the full definition of variance of conditional expectation in Eq.~(\ref{eq:ss_variance_integral}),
\begin{equation}
  \begin{split}
    \mathbb{V}_{d}[\mathbb{E}_{\sim d}[Y|X_d]] & = \int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d} \\
                                               & \quad - \left(\int f(\mathbf{x}) d\mathbf{x}\right)^2
  \end{split}
\label{eq:ss_variance_integral_single}
\end{equation}

Finally, the main-effect sensitivity index can be written as an integral as follows:
\begin{equation}
  \begin{split}
    S_d & = \frac{\mathbb{V}_d [\mathbb{E}_{\sim d} [Y|X_d]]}{\mathbb{V}[Y]} \\
        & = \frac{\int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x} - \left(\int f(\mathbf{x}) d\mathbf{x}\right)^2}{\int f(\mathbf{x})^2 d\mathbf{x} - \left( \int f(\mathbf{x}) d\mathbf{x}\right)^2}
  \end{split}
\label{eq:ss_main_effect_integral}
\end{equation}
The integral form given above dispenses with the nested structure of multiple integrals in the original definition of main-effect index.
The multidimensional integration is over $2 \times D - 1$ dimensions 
and it is the basis of estimating sensitivity index using \gls{mc} simulation in this thesis, hereinafter referred to as the Sobol'-Saltelli method.
The same procedure applies to derive the total effect-index which yields,
\begin{equation}
  \begin{split}
    ST_d & = \frac{\mathbb{E}_{\sim d}[\mathbb{V}_{d}[Y|\mathbf{X}_{\sim d}]]}{\mathbb{V}[Y]} \\
        & = \frac{\int f^2(\mathbf{x}) d\mathbf{x} - \int \int f(\mathbf{x}_{\sim d}, x^{\prime}_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{d} d\mathbf{x}}{\int f(\mathbf{x})^2 d\mathbf{x} - \left( \int f(\mathbf{x}) d\mathbf{x}\right)^2}
  \end{split}
\label{eq:ss_total_effect_integral}
\end{equation}

As it was for the Morris method, an implementation of the Sobol'-Saltelli method is also part of \texttt{gsa-module} python3 package (see Appendix~\ref{app:gsa_module} for detail). 
For $N$ number of \gls{mc} samples and $D$ number of model parameters, the \gls{mc} simulation procedure to estimate the sensitivity indices follows the sampling and resampling approach adopted in~\cite{Sobol2001,Saltelli2002,Homma1996} described in the following.

\textsc{First}, generate two $N \times D$ independent random samples from a uniform independent distribution in $D$-dimension, $[0,1]^D$:
\begin{equation}
A = 
\begin{pmatrix}
a_{11}  & \cdots  & a_{1D}\\
\vdots	& \ddots & \vdots\\
a_{N1}  & \cdots  & a_{ND}\\
\end{pmatrix}
;\quad B = 
\begin{pmatrix}
b_{11}  & \cdots  & b_{1D}\\
\vdots	& \ddots & \vdots\\
b_{N1}  & \cdots  & b_{ND}\\
\end{pmatrix}
\label{eq:ss_two_samples}
\end{equation}

\textsc{Second}, construct $D$ additional design of experiment matrices where each matrix is matrix $A$ with the $d$-th column substituted by the $d$-th column of matrix $B$:\begin{equation}
  \begin{split}
  & A_{B}^1 = 
  \begin{pmatrix}
    b_{11}  & \cdots  & a_{1D}\\
    \vdots	& \ddots & \vdots\\
    b_{N1}  & \cdots  & a_{ND}\\
  \end{pmatrix} \\
  & A_{B}^{d} = 
  \begin{pmatrix}
    a_{11}  & \cdots & b_{1d} & \cdots & a_{1D}\\
    \vdots	& \cdots & \vdots & \cdots & \vdots\\
    a_{N1}  & \cdots & b_{Nd} & \cdots & a_{ND}\\
  \end{pmatrix} \\
  & A_{B}^{D} = 
  \begin{pmatrix}
    a_{11}  & \cdots  & b_{1D}\\
    \vdots	& \ddots & \vdots\\
    a_{N1}  & \cdots  & b_{ND}\\
  \end{pmatrix}
  \end{split}
\label{eq:ss_sampling_resampling}
\end{equation}

\textsc{Third}, rescale each element in the matrices of samples to the actual values of model parameters according to their actual range of variation through iso-probabilistic transformation.

\textsc{Fourth}, evaluate the model multiple times using input vectors that correspond to each row of $A$, $B$, and all the $A_B^d$.

\textsc{Fifth} and finally, extract the \gls{qoi}s from all the outputs and recast them as vectors.
The main-effect and total-effect indices are then estimated using the estimators described below.

For the main-effect sensitivity index, two estimators are considered.
One is proposed by Saltelli~\cite{Saltelli2002}, and the other, as an alternative, is proposed by Janon et al~\cite{Janon2014}.
The latter proved to be more efficient, especially for a large variation around a parameter estimate~\cite{Iooss2015,Janon2014}.

The first term in the numerator of Eq.~(\ref{eq:ss_main_effect_integral}) is the same for both estimators and is given by
\begin{equation}
  \int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d} \approx \frac{1}{N}\sum_{n=1}^N f(B)_n \cdot f(A_B^d)_n
\label{eq:ss_first_term}
\end{equation}
where the subscript $n$ corresponds to the row of the sampled model parameters 
such that $f(B)_n$ is the model output evaluated using inputs taken from the $n$-th row of matrix $B$ 
and $f(A_B^d)_n$ is the model output evaluated using inputs taken from the $n$-th row of matrix $A_B^K$.
The \gls{mc} estimator for the second term in the numerator and for the denominator differ for the two considered estimators.
They are given in Table~\ref{tab:ss_main_effect_estimator}.

\begin{table}[h]
	\myfloatalign
	\caption[Monte Carlo estimators to estimate the main-effect indices]{Two \gls{mc} estimators for the terms in Eq.~(\ref{eq:ss_main_effect_integral}) to estimate the main-effect indices (the sum is implicitly over all samples $N$)}
	\label{tab:ss_main_effect_estimator}
	\begin{tabularx}{\textwidth}{cll} \toprule
		\tableheadline{Estimator}     & \tableheadline{$\mathbb{E}^2[Y] = \left( \int f d\mathbf{x}\right)^2$} & \tableheadline{$\mathbb{V}[Y] = \int f^2 d\mathbf{x} - \left( \int f d\mathbf{x}\right)^2$} \\ \midrule 
		Saltelli             & $\frac{1}{N} \sum f(A)_n \cdot f(B)_n$  & $\frac{1}{N}\sum f(A)_n^2$  \\
    \cite{Saltelli2002}  &                                         & $-\left(\frac{1}{N}\sum f(A)_n\right)^2$\\
		Janon et al.         & $\left(\frac{1}{N} \sum \frac{f(B)_n + f(A_B^d)_n}{2}\right)^2$  & $\frac{1}{N} \sum \frac{f(B)_n^2 + f(A_B^d)_n^2}{2}$ \\
    \cite{Janon2014}     &                                                                  & $-\left(\frac{1}{N} \sum \frac{f(B)_n^2 + f(A_B^d)_n^2}{2}\right)^2$ \\
		\bottomrule
	\end{tabularx}
\end{table}

To estimate the total-effect sensitivity indices, the Jansen estimator~\cite{Jansen1999} is recommended in~\cite{Saltelli2010a}.
The estimator reads
\begin{equation}
  \hat{ST}_d = \frac{\frac{1}{2N}\sum_{n=1}^{N}\left(f(A)_n - f(A_B^d)_n\right)^2}{\mathbb{V}[Y]}
\label{eq:ss_jansen_estimator}
\end{equation}
where $\mathbb{V}[Y]$ is estimated by the Saltelli et al. estimator as prescribed in Table~\ref{tab:ss_main_effect_estimator}.

The computational cost associated with the estimation of all the main-effect and total-effect indices is $N \times (D + 2)$ code runs,
\marginpar{computational cost: \\ brute force Monte Carlo vs. Sobol'-Saltelli}
where $N$ is the number of \gls{mc} samples and $D$ is the number of parameters.
Compare this to the cost of brute force Monte Carlo of $2 \times D \times N^2$ to estimate all the main-effect and total-effect sensitivity indices. 

As an additional comparison, the cost for Morris method to compute the statistics of elementary effect is $N_R \times (D + 1)$ code runs,
\marginpar{computational cost: \\ Morris vs. Sobol'-Saltelli}
where $N_R$ is the number of OAT design replications.
In either methods, the number of samples $N$ (in the case of the Sobol'-Saltelli method) and replications $N_R$ (in the case of the Morris method)
determines the precision of the estimates.
A larger number of samples (and replications) increases the precision.
Note, however, that in practice the typical number of Morris replications is between $10^1 - 10^2$~\cite{Saltelli2010}, 
while the number of \gls{mc} samples for the Sobol' indices estimation amounts to $10^2 - 10^4$~\cite{Sobol2001}.
