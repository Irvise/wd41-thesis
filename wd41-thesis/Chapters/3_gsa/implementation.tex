\section{Implementation}\label{sec:sa_implementation}

\subsection{The Morris Method}

\subsection{The Sobol'-Saltelli Method}

In principle, the estimation of the Sobol' indices defined by Eqs.~(\ref{eq:sa_main_effect_index}) and (\ref{eq:sa_total_effect_index}) can be directly carried out using \gls{mc} simulation.
\marginpar{brute force \\ Monte Carlo}
The most straightforward, though rather naive, implementation of \gls{mc} simulation to conduct the estimation is using two nested loops for the computation of the conditional variance and expectation appeared in both equations.

In the estimation of the main-effect index of parameter $x_d$, for instance, the outer loop samples values of $X_d$ while the inner loop samples values of $\mathbf{X}_{\sim d}$ (anything else other than $x_d$). T
The samples, in turn, are used to evaluate the model output.
In the inner loop, the mean of the model output (for a given value of $X_d$ but over many values of $\mathbf{X}_{\sim d}$) is taken. 
Afterwards, in the outer loop, the variance of the model output (over many values of $X_d$) is taken.
This approach can easily become prohibitively expensive as the nested structure requires two $N^2$ model evaluations per input dimension for either the main-effect and total-effect indices, while $N$ (the size of \gls{mc} samples) are typically in the range of $10^2 - 10^4$ for a reliable estimate. 
   
Sobol' \cite{Sobol2001} and Saltelli \cite{Saltelli2002} proposed an alternative approach that circumvent the nested structure of \gls{mc} simulation to estimate the indices.
The formulation starts by expressing the the expectation and variance operators in their integral form.
As the following formulation is defined on a unit hypercube of $D$-dimension parameter space where each parameter is a uniform and independent random variable,
explicit writing of the distribution within the integration as well as the integration range are excluded for conciseness.

First, the variance operator shown in the numerator of Eq.~(\ref{eq:sa_main_effect_index}) is written as
\begin{equation}
  \begin{split}
    \mathbb{V}_{d}[\mathbb{E}_{\sim d}[Y|X_d]] & = \mathbb{E}_{d}[\mathbb{E}_{\sim d}^{2}[Y|X_d]] - \left(\mathbb{E}_{d}[\mathbb{E}_{\sim d}[Y|X_d]]\right)^2 \\ 
                                               & = \int \mathbb{E}_{\sim d}^{2}[Y|X_d] dx_d - \left(\int \mathbb{E}_{\sim d}[Y|X_d] dx_d\right)^2
  \end{split}
\label{eq:ss_variance_integral}
\end{equation}
The notation $\mathbb{E}_{\sim \circ}[\circ | \circ]$ was already explained in Section~\ref{sub:sa_hdmr}, 
while $\mathbb{E}_{\circ} [\circ]$ corresponds to the marginal expectation operator 
where the integration is carried out over the range of parameters specified in the subscript. 

Next, consider the term conditional expectation shown in Eq.~(\ref{eq:ss_variance_integral}), which per definition reads
\begin{equation}
  \mathbb{E}_{\sim d} [Y|X_d] = \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d}
\label{eq:ss_expectation_integral}
\end{equation}
Note that $\mathbf{x} = \{\mathbf{x}_{\sim d}, x_d\}$.

Following the first term of Eq.~(\ref{eq:ss_variance_integral}), by squaring Eq.~(\ref{eq:ss_expectation_integral})
and by defining a dummy vector variable $\mathbf{x}^{\prime}_{\sim d}$, 
the product of the two integrals can be written in terms of a single multiple integrals
\begin{equation}
  \begin{split}
    \mathbb{E}_{\sim d}^{2} [Y|X_d] & = \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d} \cdot \int f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}_{\sim d} \\
                                    & = \int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d}
  \end{split}
\label{eq:ss_multiple_integrals}
\end{equation}

Returning to the full definition of variance of conditional expectation in Eq.~(\ref{eq:ss_variance_integral}),
\begin{equation}
  \begin{split}
    \mathbb{V}_{d}[\mathbb{E}_{\sim d}[Y|X_d]] & = \int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d} \\
                                               & \quad - \left(\int f(\mathbf{x}) d\mathbf{x}\right)^2
  \end{split}
\label{eq:ss_variance_integral_single}
\end{equation}

Finally, the main-effect sensitivity index can be written as an integral as follows:
\begin{equation}
  \begin{split}
    S_d & = \frac{\mathbb{V}_d [\mathbb{E}_{\sim d} [Y|X_d]]}{\mathbb{V}[Y]} \\
        & = \frac{\int \int f(\mathbf{x}^{\prime}_{\sim d}, x_d) f(\mathbf{x}_{\sim d}, x_d) d\mathbf{x}^{\prime}_{\sim d} d\mathbf{x}_{\sim d} - \left(\int f(\mathbf{x}) d\mathbf{x}\right)^2}{\int f(\mathbf{x})^2 d\mathbf{x} - \left( \int f(\mathbf{x}) d\mathbf{x}\right)^2}
  \end{split}
\label{eq:ss_main_effect_integral}
\end{equation}
The integral form given above dispenses with the nested structure of multiple integrals in the original definition of main-effect index.
It is the basis of estimating sensitivity index using \gls{mc} simulation in this thesis, hereinafter referred to as the Sobol'-Saltelli method.
The same procedure applies to derive the total effect-index.

For $N$ number of \gls{mc} samples and $D$ number of model parameters, the \gls{mc} simulation procedure to estimate the sensitivity indices follows the sampling and resampling approach adopted in~\cite{Sobol2001,Saltelli2002,Homma1996}.

First, generate two $N \times D$ independent random samples from a uniform independent distribution in $D$-dimension, $[0,1]^D$:
\begin{equation}
A = 
\begin{pmatrix}
a_{11}  & \cdots  & a_{1D}\\
\vdots	& \ddots & \vdots\\
a_{N1}  & \cdots  & a_{ND}\\
\end{pmatrix}
;\quad B = 
\begin{pmatrix}
b_{11}  & \cdots  & b_{1D}\\
\vdots	& \ddots & \vdots\\
b_{N1}  & \cdots  & b_{ND}\\
\end{pmatrix}
\label{eq:ss_two_samples}
\end{equation}

Second, construct $D$ additional design of experiment matrices where each matrix is matrix $A$ with the $d$-th column substituted by the $d$-th column of matrix $B$:\begin{equation}
  \begin{split}
  & A_{B}^1 = 
  \begin{pmatrix}
    b_{11}  & \cdots  & a_{1D}\\
    \vdots	& \ddots & \vdots\\
    b_{N1}  & \cdots  & a_{ND}\\
  \end{pmatrix} \\
  & A_{B}^{d} = 
  \begin{pmatrix}
    a_{11}  & \cdots & b_{1d} & \cdots & a_{1D}\\
    \vdots	& \cdots & \vdots & \cdots & \vdots\\
    a_{N1}  & \cdots & b_{Nd} & \cdots & a_{ND}\\
  \end{pmatrix} \\
  & A_{B}^{D} = 
  \begin{pmatrix}
    a_{11}  & \cdots  & b_{1D}\\
    \vdots	& \ddots & \vdots\\
    a_{N1}  & \cdots  & b_{ND}\\
  \end{pmatrix}
  \end{split}
\label{eq:ss_two_samples}
\end{equation}

Third, rescale each element in the matrices of samples to the actual values of model parameters according to their actual range of variation through iso-probabilistic transformation.

Fourth, evaluate the model multiple times using input vectors that correspond to each row of $A$, $B$, and all the $A_B^d$.

Fifth, extract the \gls{qoi}s from all the outputs and recast them as vectors.
The main-effect and total-effect indices are then estimated using the estimators described below.

\begin{table}[h]
	\myfloatalign
	\caption[Monte Carlo estimators to estimate the main-effect indices]{Two \gls{mc} estimators for the terms in Eq.~(\ref{eq:ss_main_effect_integral}) to estimate the main-effect indices (the sum is implicitly over all samples $N$)}
	\label{tab:ss_main_effect_estimator}
	\begin{tabularx}{\textwidth}{cll} \toprule
		\tableheadline{Estimator}     & \tableheadline{$\mathbb{E}^2[Y] = \left( \int f d\mathbf{x}\right)^2$} & \tableheadline{$\mathbb{V}[Y] = \int f^2 d\mathbf{x} - \left( \int f d\mathbf{x}\right)^2$} \\ \midrule 
		Saltelli             & $\frac{1}{N} \sum f(A)_n \cdot f(B)_n$  & $\frac{1}{N}\sum f(A)_n^2 - \left(\frac{1}{N}\sum f(A)_n\right)^2$ \\
    \cite{Saltelli2002}  &                                         & \\
		Janon et al.         & $\left(\frac{1}{N} \sum \frac{f(B)_n + f(A_B^d)_n}{2}\right)^2$  & $\frac{1}{N} \sum \frac{f(B)_n^2 + f(A_B^d)_n^2}{2}$ \\
    \cite{Janon2014}     &                                                                  & $-\left(\frac{1}{N} \sum \frac{f(B)_n^2 + f(A_B^d)_n^2}{2}\right)^2$ \\
		\bottomrule
	\end{tabularx}
\end{table}

The computational cost associated with the estimation of all the main-effect and total-effect indices is $N \times (K + 2)$ code runs,
\marginpar{computational cost: \\ Morris vs. Sobol'-Saltelli}
where $N$ is the number of \gls{mc} samples and $K$ is the number of parameters.
As a comparison, the cost for Morris method to compute the statistics of elementary effect is $N_R \times (K + 1)$ code runs,
where $N_R$ is the number of OAT design replications.
In either methods, the number of samples $N$ (in the case of the Sobol'-Saltelli method) and replications $N_R$ (in the case of the Morris method)
determines the precision of the estimates.
A larger number of samples (and replications) increases the precision.
Note, however, that in practice the typical number of Morris replications is between $10^1 - 10^2$~\cite{Saltelli2010}, 
while the number of \gls{mc} samples for the Sobol' indices estimation amounts to $10^2 - 10^4$~\cite{Sobol2001}.