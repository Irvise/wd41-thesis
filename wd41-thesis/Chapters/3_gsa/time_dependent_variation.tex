%*********************************************************************************************
\section{Describing Variation of Time-Dependent Output}\label{sec:sa_time_dependent_variation}
%*********************************************************************************************

Ramsay and Silverman \cite{Ramsay2005} popularized \gls[hyper=false]{fda},
which refers to statistical analysis of data that are functions.
\marginpar{Functional Data Analysis (FDA)}
The main assumption of \gls[hyper=false]{fda}, as opposed to a more conventional multivariate analysis, is that data present sufficient smoothness, defined by existence of derivatives up to a certain order.
Another distinguishing feature of \gls[hyper=false]{fda}, as opposed to time-series analysis or spatial statistics, is the availability of numerous replications of such data (i.e., set of functions) produced by the same or similar underlying process. 
The role of \gls[hyper=false]{fda} in this work is to describe the overall variation within the data set $\bm{Y}$ using a reduced set of scalars and functions obtained from \glsentryfull{pca}.
The functions remain the same for all of the data set,
while the scalars vary as function of the sample.
Key here is that the required number of scalars is much smaller than the size of $\mathbf{Y}$ 
and, in turn, can be used as the \gls[hyper=false]{qoi} for \gls[hyper=false]{sa}.

%-----------------------------------------------------------------
\subsection{Functional Output Representation}\label{sub:sa_spline}
%-----------------------------------------------------------------

The assumption of continuity within a practically discrete data set (such as the numerical code output of Eq.~(\ref{eq:discrete_time})) is made explicit through a functional representation.
The recommended representation is through a linear combination of basis functions \cite{Ramsay2005}.
This thesis adopts the B-spline basis function \cite{Gillies2010} expansion because of its flexibility \cite{Eilers1996,Eilers2010} 
and the availability of its implementation in open numerical libraries \cite{RCT2017}.

% Basis function expansion
Within this framework, a function can be written using basis function expansion as,
\begin{equation}
	y_i (t) = \sum_{k = 1}^{K} c_{ik} \cdot \phi_k (t); \quad i = 1, 2, \cdots N
\label{eq:basis_function_expansion}
\end{equation}
where $K$ indicates the number of basis functions; 
$\phi_k (t)$ is the $k$-th basis function;
and $c_{ik}; \, k = 1, \dots, K$ are the basis coefficients for curve $i$.
These coefficients are fitted to the data set to construct curve $i$, with or without smoothing.

% Spline
A B-spline is constructed using piecewise polynomial (\emph{spline}) connected at selected points in the domain called \emph{knots}.
Let $\boldsymbol{\tau}=\{\tau_k; \, k = 0,1,\cdots, L\}$ be a sequence of knots, i.e., an ordered set of non-decreasing numbers that divide the function domain into $L$ sub-intervals.
Within each sub-interval, a piecewise polynomial of degree $p$ is defined.
At the interconnection (knots), adjacent polynomials are continuous with their derivatives up to $p-1$ matching up.
In other words, the spline is $p-1$ differentiable at the knots.

% B-Spline
The basis functions in the B-spline system are determined by the degree of the polynomial $p$ and the knot sequence $\boldsymbol{\tau}=\{\tau_k; \, k = 0,1,\cdots, L\}$.
However, per definition, there is no data point available on the left (right) of the leftmost (rightmost) knot.
As such, there is no differentiability (smoothness) condition to uphold at the boundaries and the resulting system of equations are underdetermined.
To resolve this issue, the endpoints can be repeated $p$ times and augmented into both ends of the knot sequence such that
\begin{equation}
	\tau_{-p} = \cdots = \tau_{0} \leq \tau_{1} \leq \cdots \leq \tau_{L} = \cdots = \tau_{L+p} 
\label{eq:augmented_knots}
\end{equation}

% Constructing B-Spline
This procedure will only reduce the order of continuity of the $p$ outer left and $p$ outer right basis functions of the domain such that the system of equations becomes fully determined.
As a result, the augmented knot sequence becomes
\begin{equation}
  \boldsymbol{\tau}=\{\tau_k; \, k = -p,-p+1,\cdots, 0, 1, \cdots, L + p\}
\label{eq:augmented_knots_sequence}
\end{equation}

% de Boor-Cox formula
The B-spline basis functions of degree $p$ can then be defined recursively on the augmented knot sequence using de Boor - Cox formula as follows, 
\begin{equation}
	\begin{split}
		B^0_k (t) = 
			\begin{cases}
				1, & \tau_k \leq t < \tau_{k+1} \\
			0, & \text{otherwise}
	\end{cases} \\
	B^p_k (t) =  \alpha^p_k (t) B^{p-1}_k + \left[1 - \alpha^p_{k+1} (t)\right] B^{p-1}_{k+1} \\
	\alpha^p_k (t) =
	\begin{cases}
				\frac{t - \tau_k}{\tau_{k+p}-\tau_k}, & \tau_{k+p} \neq \tau_k \\
			  0, & \text{otherwise}
	\end{cases}
	\end{split} 
\label{eq:deboor_cox}
\end{equation}
where $B_k^p (t)$ denotes the $k$-th B-spline of degree $p$.
The degree $p$ and the number of interior knots $L-1$ (i.e., excluding the end points), 
determine the number of spline basis functions according to $K = p + L$.
In other words, the B-spline basis functions are $\{B_k^p (t); \, k = -p,-p+1,\cdots,L-1\}$.

Fig.~\ref{fig:ch3_bspline} illustrates all the $14$ spline basis functions of degree $3$ over $10$ uniform interior knots (or $11$ sub-intervals) defined in $[0,1]$.
The resulting augmented knot sequence is $\boldsymbol{\tau}=\{0, 0, 0, 0, \frac{1}{11}, \cdots, \frac{10}{11}, 1, 1, 1, 1\}$.
The three leftmost and three rightmost basis functions are less smooth at the two boundaries than the other eight basis functions in the center. 
From leftmost to the right (rightmost to the left) the three basis functions are non-, once-, and twice-differentiable at the left (right) boundaries, respectively.
\bigfigure[pos=!tbhp,
					 opt={width=1.0\textwidth},
           label={fig:ch3_bspline},
           shortcaption={Spline basis functions of order $4$}]
{../figures/chapter3/figures/plotBSplineIllustration}
{The fourteen B-spline basis functions of degree $3$ (cubic) with $10$ uniform interior knots (shown in light dashed vertical lines) that divides the domain into $11$ sub-intervals.}

Returning to the formulation of Eq.~(\ref{eq:basis_function_expansion}),
a function $y_i$ is then represented by B-spline basis functions of a given order $B_k^p (t)$ and a knot sequence $\boldsymbol{\tau}$ as follows,
\begin{equation}
	y_i (t) = \sum_{k = -p}^{L-1} c_{ik} \cdot B^p_k (t); \quad i = 1, 2, \cdots N
\label{eq:basis_function_expansion}
\end{equation}

%----------------------------------------------------------------------
\subsection{Curve Registration by Landmarks}\label{sub:sa_registration}
%----------------------------------------------------------------------

% Summary Statistics in Data Set with Phase Variation
Essential to the idea of summary statistics of a data set is the measure of central tendency (e.g., the mean),
which characterizes a \emph{typical} realization.
\marginpar{variation in a functional data set: amplitude \& phase}
The measure of dispersion such as the variance, in turn, can be defined relative to the mean. 
Two types of variations are often simultaneously present in a functional data set: 
the variation in magnitude (\emph{vertical} variation) and the variation in phase (\emph{horizontal} variation).
The simultaneous presence of these two types of variations makes the definition of a mean function difficult \cite{Kneip1992}.

% Cross-sectional mean vs. Structural Mean
Fig.~\ref{fig:ch3_plot_phase_variation_illustration} illustrates this point.
\marginpar{Cross-sectional vs. structural mean}
For a functional data set that does not contain strong phase variations, 
a simple cross-sectional mean (average values across realizations taken at every argument values) does indeed represents a typical realization (Fig.~\ref{fig:ch3_plot_phase_variation_illustration_1}).
On the other hand, with a strong phase variation (often mixed with amplitude variation), 
the cross-sectional mean fails to produce a typical realization (Fig.~\ref{fig:ch3_plot_phase_variation_illustration_2}).
In this case,
according to Kneip (\cite{Kneip1992}) a more proper \emph{structural} mean of the data set can be defined instead
by first separating the phase and amplitude variations.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch3_plot_phase_variation_illustration},
                  maincaption={Two examples of functional data sets. (Left) without pronounced phase variations, cross-sectional mean can reflect a typical realization. (Right) with pronounced phase variations, cross-sectional mean differs from the notion of typical realization. \emph{Structural} mean derived after registration better represents a typical realization. The scales in the axes are arbitrary.},%
									mainshortcaption={Variation in functional data set, with and without phase variation.},
                  leftopt={width=0.46\textwidth},
                  leftlabel={fig:ch3_plot_phase_variation_illustration_1},
                  leftcaption={Without phase variation},
                  rightopt={width=0.46\textwidth},
                  rightlabel={fig:ch3_plot_phase_variation_illustration_2},
                  rightcaption={With strong phase variation}
                 ]
{../figures/chapter3/figures/plotPhaseVariationIllustration_1}
{../figures/chapter3/figures/plotPhaseVariationIllustration_2}

% Landmark registration
In order to obtain a meaningful structural mean in presence of strong amplitude and phase variations,
the two types of variation are first separated through a \emph{registration} procedure \cite{Wang1997,Ramsay1998}.
\marginpar{landmark registration}
The procedure transforms the time argument using a warping function to reduce the phase variation in the data set.
Specifically, the landmark based registration can be employed when the main features of the function of interest (i.e., reflood curve) are readily identifiable.
In a functional data set with phase variation, this particular type of registration forces important events in a curve (its \emph{landmarks}) to occur at the same time relative to a set of reference values.

This is illustrated in Fig.~\ref{fig:ch3_plot_registered_curves_illustration}.
The left panel shows a functional data set exhibiting phase variation, which is reduced by aligning its landmark, here the time of maximum of each realization (shown as vertical solid lines) to a reference value (shown as the vertical dashed line).
The structural mean (solid line curve) is simply computed as the cross-sectional mean of the registered curves shown on the right panel. 
The structural mean properly represents the inflection and the maximum points of a typical realization indeed, and therefore is more representative of the curves in the original data set.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch3_plot_registered_curves_illustration},
                  maincaption={Illustration of curve registration. Shown in the left, curves whose landmarks (solid vertical lines) are to be aligned with respect to a reference value (dashed vertical line). Shown in the right, the registered curves. The structural mean is shown as a thick solid line curve. The scales in the axes are arbitrary.},%
									mainshortcaption={Illustration of curve registration.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch3_plot_registered_curves_illustration_1},
                  leftcaption={Unregistered curves},
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch3_plot_registered_curves_illustration_2},
                  rightcaption={Registered curves, by landmark}
                 ]
{../figures/chapter3/figures/plotRegisteredCurvesIllustration_1}
{../figures/chapter3/figures/plotRegisteredCurvesIllustration_2}

More details on the properties of the warping functions used to transform the time argument for registration can be found in Appendix~\ref{app:landmark_registration}.

%----------------------------------------------------------------------
\subsection{Functional Principal Component Analysis}\label{sub:sa_fpca}
%----------------------------------------------------------------------

Separation of phase variation from magnitude variation by registration procedure allows for the definition of a proper mean function.
With respect to the mean, the notion of functional variation can then be defined.
In the following discussion it is assumed that the functional data set does not contain phase variation.
That is, the functions are fully registered.
If this is not the case then the previous step of registration might be required to first obtain a proper mean function.

% Covariance function
The covariance function of a set of function realizations $\{y_n(t);n = 1, 2, \cdots, N; t \in [t_a,t_b]\}$ from a random process $\mathcal{Y}$ is defined as
\begin{equation}
	\nu (t_1, t_2) \equiv \frac{1}{N} \sum_{n=1}^{N} (y_n(t_1) - \bar{y}(t_1)) \cdot (y_n(t_2) - \bar{y}(t_2))
\label{eq:covariance_function}
\end{equation}
where $\bar{y} (t)$ is the proper mean function.

To extract more meaningful information from the covariance function,
the function can be projected onto lower-dimensional space using an orthogonal decomposition.
This projection can be done through the \glsfirst[hyper=false]{fpca} (also known as the Karhunen-Lo\'eve transform (KLT), see Appendix~\ref{app:kl_theorem} for the underlying theorem):
\begin{equation}
	\nu (t_1, t_2) = \sum_{j=1}^{+\infty} \rho_j \cdot \xi_j(t_1) \cdot \xi_j(t_2)
\label{eq:kl_transform}
\end{equation}
where $\rho_j$ is a series of ordered eigenvalues of decreasing values; 
$\xi_j(t)$ is the corresponding series of orthogonal eigenfunctions (or the fPC).

The transformation of the covariance function into pairs of eigenvalues and eigenfunctions also allows each element of the original data set $\{y_n(t)\}^N_{n=1}$ to be represented as a series that is optimal in the root-mean-square-of-error sense:
\begin{equation}
  y_n(t) = \bar{y}(t) + \sum_{j=1}^{+\infty} \theta_{j,n} \cdot \xi_j (t); \quad n = 1, 2, \cdots, N
\label{eq:pod}
\end{equation}
where the fPC score $\theta_{j,n}$ associated with each realized function is defined by the orthogonality condition
\begin{equation}
  \theta_{j,n} = \int_{t_a}^{t_b} \left[y_n(t) - \bar{y}(t)\right] \cdot \xi_j (t) dt
\label{eq:pod_orthogonality}
\end{equation}

Eqs.~(\ref{eq:pod}) and (\ref{eq:pod_orthogonality}) imply that across realizations in the samples, 
$\{y_n(t)\}$ can be represented linearly using a common mean function and sums of deviation terms from the mean.
The deviation terms consist of a set of common eigenfunctions and a set of \gls[hyper=false]{fpc} scores.
As such, the random character of each realization is left to the score associated with each component and each realization.
Put differently, the eigenfunctions described the (common) modes of variations, 
while the scores quantify the strength of a particular mode~\cite{Wang2012}.
These scores will be used as the \gls[hyper=false]{qoi} in the subsequent global \gls[hyper=false]{sa}.
A way to compute the \gls[hyper=false]{fpc} and the associated scores can be found in Ref.~\cite{Wicaksono2014a}.