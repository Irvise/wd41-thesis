\section{Describing Variation of Time-Dependent Output}\label{sec:sa_time_dependent_variation}

Ramsay and Silverman \cite{Ramsay2005} popularized \gls{fda}, which refers to statistical analyses of data that are functions.
The main assumption of \gls{fda}, as opposed to a more conventional multivariate analysis, is that data present sufficient smoothness, defined by existence of derivatives up to a certain order.
Another distinguishing feature of \gls{fda}, as opposed to time-series analysis or spatial statistics, is the availability of numerous replications of such data (i.e., set of functions) produced by the same or similar underlying process. 
The goal of \gls{fda} related to this work is to describe the overall variation of a set of functions using a smaller set of scalars.
These scalars, in turn, can be used as the \gls{qoi} for \gls{sa}.

\subsection{Functional Output Representation}\label{sub:sa_spline}

The assumption of continuity within a practically discrete data set (such as the numerical code output of Eq.~(\ref{eq:discrete_time}) is made explicit through a functional representation.
The recommended representation is through a linear combinations of basis functions \cite{Ramsay2005}.
This thesis adopts the B-spline basis function \cite{Gillies2010} expansion because of its flexibility \cite{Eilers1996,Eilers2010} 
and the wide availability of its implementation in open numerical libraries \cite{RCT2017}.

Within this framework, a function can be written using basis function expansion as,
\begin{equation}
	y_i (t) = \sum_{k = 1}^{K} c_{ik} \cdot \phi_k (t); \quad i = 1, 2, \cdots N
\label{eq:basis_function_expansion}
\end{equation}
where $K$ indicates the number of basis functions. $\phi_k (t)$ is the $k$-th basis function, 
and $c_{ik}$ is the basis coefficient.
The latter is fitted to the data set to construct curve $i$ with or without smoothing (i.e., interpolating condition vs. penalized ).

A B-spline basis function consists of polynomial pieces connected at a point in the domain called \emph{knot}.
The B-spline basis function based on a specified set of knots $\{t_k; k = 1, 2, \cdots, K\}$ can be recursively defined as follows:

where $B_k^p (t)$ denotes the $k$-th B-spline of degree $p$.

Figure

\subsection{Curve Registration by Landmarks}\label{sub:sa_registration}

\subsection{Functional Principal Component Analysis}\label{sub:sa_fpca}

Separation of phase variation from magnitude variation by registration procedure allows for the definition of a proper mean function.
With respect to that, the notion of functional variation can be defined.
The covariance function of a set of function realizations $\{y_n(t);n = 1, 2, \cdots, N; t \in [t_a,t_b]\}$ from a random process $Y$ is defined as
\begin{equation}
	\nu (t_1, t_2) \equiv \frac{1}{N} \sum_{n=1}^{N} (y_n(t_1) - \bar{y}(t_1)) \cdot (y_n(t_2) - \bar{y}(t_2))
\label{eq:covariance_function}
\end{equation}

To extract more meaningful information from the covariance function, the function is often projected onto lower-dimensional space using an orthogonal decomposition.
This projection can be done through the functional principal component (fPC) analysis (fPCA) (also know as the Karhunen-Lo\'eve transform):
\begin{equation}
	\nu (t_1, t_2) = \sum_{j=1}^{+\infty} \rho_j \cdot \xi_j(t_1) \cdot \xi_j(t_2)
\label{eq:kl_transform}
\end{equation}
where $\rho_j$ is a series of ordered eigenvalues of decreasing values; 
$\xi_j(t)$ is the corresponding series of orthogonal eigenfunctions (or the fPC).

The transformation of the covariance function into pairs of eigenvalues and eigenfunctions also allows each element of the original dataset $\{y_n(t)\}$ to be represented as a series that is optimal in the root-mean-square-of-error sense:
\begin{equation}
  y_n(t) = \bar{y}(t) + \sum_{j=1}^{+\infty} \theta_{j,n} \cdot \xi_j (t); \quad n = 1, 2, \cdots, N
\label{eq:pod}
\end{equation}
were the fPC score $\theta_{j,n}$ associated with each realized function is defined by the orthogonality condition
\begin{equation}
  \theta_{j,n} = \int_(t_a)^(t_b) \left[y_n(t) - \bar{y}(y)\right] \cdot \xi_j (t) dt
\label{eq:pod_orthogonality}
\end{equation}

Eqs.~(\ref{eq:pod}) and (\ref{eq:pod_orthogonality}) imply that across realizations in the samples, 
$\{y_n(t)\}$ can represented linearly using a common mean function and sums of deviation terms from the mean.
The deviation terms consist of a set of common eigenfunctions and a set of fPC scores.
As such, the random character of each realization is left to the score associated with each component and each realization.
Put differently, the eigenfunctions described the (common) modes of variations, 
while the scores quantify the strength of a particular mode~\cite{Wang2012}.
These scores will be used as the \gls{qoi} in the subsequent global \gls{sa}. 