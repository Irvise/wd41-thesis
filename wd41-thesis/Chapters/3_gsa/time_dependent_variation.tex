\section{Describing Variation of Time-Dependent Output}\label{sec:sa_time_dependent_variation}

Ramsay and Silverman \cite{Ramsay2005} popularized \gls{fda}, which refers to statistical analyses of data that are functions.
The main assumption of \gls{fda}, as opposed to a more conventional multivariate analysis, is that data present sufficient smoothness, defined by existence of derivatives up to a certain order.
Another distinguishing feature of \gls{fda}, as opposed to time-series analysis or spatial statistics, is the availability of numerous replications of such data (i.e., set of functions) produced by the same or similar underlying process. 
The goal of \gls{fda} related to this work is to describe the overall variation of a set of functions using a smaller set of scalars.
These scalars, in turn, can be used as the \gls{qoi} for \gls{sa}.

\subsection{Functional Output Representation}\label{sub:sa_spline}

\subsection{Curve Registration by Landmarks}\label{sub:sa_registration}

\subsection{Functional Principal Component Analysis}\label{sub:sa_fpca}

Separation of phase variation from magnitude variation by registration procedure allows for the definition of a proper mean function.
With respect to that, the notion of functional variation can be defined.
The covariance function of a set of function realizations $\{y_n(t);n = 1, 2, \cdots, N; t \in [t_a,t_b]\}$ from a random process $Y$ is defined as
\begin{equation}
	\nu (t_1, t_2) \equiv \frac{1}{N} \sum_{n=1}^{N} (y_n(t_1) - \bar{y}(t_1)) \cdot (y_n(t_2) - \bar{y}(t_2))
\label{eq:covariance_function}
\end{equation}

To extract more meaningful information from the covariance function, the function is often projected onto lower-dimensional space using an orthogonal decomposition.
This projection can be done through the functional principal component (fPC) analysis (fPCA) (also know as the Karhunen-Lo\'eve transform):
\begin{equation}
	\nu (t_1, t_2) = \sum_{j=1}^{+\infty} \rho_j \cdot \xi_j(t_1) \cdot \xi_j(t_2)
\label{eq:kl_transform}
\end{equation}
where $\rho_j$ is a series of ordered eigenvalues of decreasing values; 
$\xi_j(t)$ is the corresponding series of orthogonal eigenfunctions (or the fPC).

The transformation of the covariance function into pairs of eigenvalues and eigenfunctions also allows each element of the original dataset $\{y_n(t)\}$ to be represented as a series that is optimal in the root-mean-square-of-error sense:
\begin{equation}
  y_n(t) = \bar{y}(t) + \sum_{j=1}^{+\infty} \theta_{j,n} \cdot \xi_j (t); \quad n = 1, 2, \cdots, N
\label{eq:pod}
\end{equation}
were the fPC score $\theta_{j,n}$ associated with each realized function is defined by the orthogonality condition
\begin{equation}
  \theta_{j,n} = \int_(t_a)^(t_b) \left[y_n(t) - \bar{y}(y)\right] \cdot \xi_j (t) dt
\label{eq:pod_orthogonality}
\end{equation}

Eqs.~(\ref{eq:pod}) and (\ref{eq:pod_orthogonality}) imply that across realizations in the samples, 
$\{y_n(t)\}$ can represented linearly using a common mean function and sums of deviation terms from the mean.
The deviation terms consist of a set of common eigenfunctions and a set of fPC scores.
As such, the random character of each realization is left to the score associated with each component and each realization.
Put differently, the eigenfunctions described the (common) modes of variations, 
while the scores quantify the strength of a particular mode~\cite{Wang2012}.
These scores will be used as the \gls{qoi} in the subsequent global \gls{sa}. 