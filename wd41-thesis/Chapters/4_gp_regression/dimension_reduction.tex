\newpage
%*************************************************************************** 
\section{Dealing with Multivariate Output}\label{sec:gp_dimension_reduction}
%***************************************************************************

% Introductory Paragraph, Multiple Output
The previous discussion on \gls[hyper=false]{gp} metamodel dealt with a single output (univariate) case.
Many computer simulations produce multivariate outputs\footnote{in this thesis the number of outputs are referred to as the \emph{dimension of the output parameter space}.}.
\marginpar{multivariate outputs}
A typical \gls[hyper=false]{trace} simulation, for example, produces flow variables as functions of time and space as its \emph{raw} outputs.
This is indeed the case for the reflood simulation problem presented in Chapter~\ref{ch:trace_reflood}.
As outlined in Chapter~\ref{ch:sensitivity_analysis}, some techniques can be used to transform the raw outputs into quantities of interest (the maximum, etc.) that are useful to answer the questions at hand.
However, in the calibration setting, some of these outputs have corresponding measurement data and need to be represented by the metamodel in their original form for a direct comparison.

% Introductory Paragraph, one suggested approach
An approach proposed in \cite{Kleijnen2000} is to represent the multiple outputs by metamodels separately.
\marginpar{separate univariate metamodel}
In other words, one metamodel is developed to represent each one of the multiple outputs individually.
Yet, for a very high-dimensional output (from tens to thousands), this approach is impractical as the numbers of metamodel to train becomes too numerous.
In addition to that, the outputs produced by the computer simulation are often highly correlated to each other.
As such, developing individual metamodels to represent the correlated outputs separately, especially when they are numerous, are wasteful. 

% The approach in this thesis
To cope with the problem of high-dimensionality of the outputs,
\marginpar{extension to multivariate case} 
this thesis adopted \gls[hyper=false]{lmc} \cite{Goulard1992,Paulo2012} coupled with \gls[hyper=false]{pca} technique \cite{Jolliffe2002,Higdon2008} to construct a tractable, multivariate version of \gls[hyper=false]{gp} metamodel.
The original \gls[hyper=false]{lmc} was formulated to model multivariate data in geostatistics that covary together over a region in a linear fashion, 
while \gls[hyper=false]{pca} is used here a data-driven dimensional reduction tool.
The resulting model consists of few \emph{independent, univariate} \gls[hyper=false]{gp} metamodels, each of which is the one presented in the previous section.

%---------------------------------------------------------------------
\subsection{Linear Model of Coregionalization (LMC)}\label{sub:gp_lmc}
%---------------------------------------------------------------------

% Linear Model of Coregionalization
The function that represents the computer code simulation $f$ is now cast in its multivariate version, $\mathbf{f}:\mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}^P$ where $P$ is the dimension of the output parameter space.
\marginpar{Linear model of coregionalization}
The \gls[hyper=false]{lmc} of the $P$-dimensional \gls[hyper=false]{gp} metamodel $\bm{Y}$ can be written as,
\begin{equation}
	\bm{Y}(\mathbf{x}) = \boldsymbol{\mu}(\mathbf{x}) + \boldsymbol{\Phi} \mathbf{w}(\mathbf{x}) + \boldsymbol{\epsilon}
\label{eq:lmc}
\end{equation}
where $\boldsymbol{\mu}$ is the $P$-dimensional mean vector of the multivariate process;
$\boldsymbol{\Phi}$ is a $P \times Q$ matrix, with $R \leq Q$;
$\boldsymbol{\epsilon}$ is a $P$-dimensional vector of li\-nearization error;
and $\mathbf{w}(\mathbf{x}) = (w_i(\mathbf{x}))$ is a $Q$-dimensional vector with univariate \glspl[hyper=false]{gp} as its elements,
\begin{equation}
	w_i(\mathbf{x}) \sim \mathcal{GP} (0, \sigma^2_i R_i(\mathbf{x}, \mathbf{x}^*))
\label{eq:lmc_weight}
\end{equation}
where $\sigma^2_i$ and $R_i$ are the process variance and correlation function associated with each element of the vector, respectively.
The term $\boldsymbol{\Phi} \mathbf{w}(\mathbf{x})$ describes the covariation between the multivariate outputs as function of model parameters.

%----------------------------------------------------------
\subsection{Principal Component Analysis}\label{sub:gp_pca}
%----------------------------------------------------------

% Principal Component Analysis
\gls[hyper=false]{pca} is then used as a data-driven approach to obtain the components of the \gls[hyper=false]{lmc} in Eq.~(\ref{eq:lmc}).
The term data-driven is used as the components are derived directly from the training samples.
The raw outputs of the training runs are first concatenated row-wise resulting in an $N \times P$ matrix $\mathbf{Y}(\mathbf{DM})$,
\begin{equation}
	\begin{split}
		\mathbf{Y}(\mathbf{DM}) & = 
			\begin{pmatrix}
																	& \vdots	& \\
				\rule[.5ex]{2.5em}{0.4pt}	& \mathbf{y}_n	&	\rule[.5ex]{2.5em}{0.4pt} \\
																	& \vdots	&
			\end{pmatrix} \\
		\mathbf{y}_n & = (y_{n1}, \cdots, y_{np}, \cdots, y_{nP}) \\
		             & = (y(\mathbf{x}_n)_1, \cdots, y(\mathbf{x}_n)_p, \cdots, y(\mathbf{x}_n)_P)
	\end{split}
\label{eq:raw_output}
\end{equation}
where $y(\mathbf{x}_n)_p$ is the $p$-th output dimension, evaluated using the $n$-th training sample.
Note that the notation above is similar to Eq.~(\ref{eq:discrete_time}) but now the dimension of the output $y_p$ is not only restricted to time, 
nor they have to be of the same (physical) dimension.
In the formulation below, the raw training outputs is always assumed to be dependent on the training sample and thus the notation $\mathbf{DM}$ is suppressed.

% The mean
The sample mean of the raw outputs is used to substitute the mean in the \gls[hyper=false]{lmc} formulation,
\begin{equation}
	\boldsymbol{\mu}(\mathbf{x}) = \bar{\mathbf{y}}^T
\label{eq:lmc_mean}
\end{equation}
The sample mean is obtained by taking the column-wise average of Eq.~(\ref{eq:raw_output}),
\begin{equation}
	\begin{split}
		\bar{\mathbf{y}} & = [\bar{y}_1, \cdots, \bar{y}_p, \cdots, \bar{y}_P] \\
		\bar{y}_p & = \frac{1}{N} \sum_{n=1}^{N} y(\mathbf{x}_n)_{p} \\
	\end{split}
\label{eq:sample_mean}
\end{equation}
Note that by the above, it implies the mean of the \gls[hyper=false]{lmc} is a constant vector.

% Standardization of output
As the output dimensions might be of different physical dimensions or measurement units, the raw outputs in Eq.~(\ref{eq:raw_output}) is centered and standardized to a unit norm (or equivalently, unit variance),
\begin{equation}
	\mathbf{Y}^* = (\mathbf{Y} - \mathbf{j}_N \bar{\mathbf{y}}) \, \text{diag}^{-1}(\boldsymbol{\sigma}_{\mathbf{y}})
\label{eq:standardization_raw_output}
\end{equation}
where $\mathbf{j}_N$ is the $N$-dimensional vector of ones;
$\text{diag}^{-1} (\circ)$ is the inverse of diagonal matrix, of which the vector argument is its diagonal elements;
and $\sigma_{\mathbf{y}}$ is the $P$-dimensional vector of column-wise standard deviation of $\mathbf{Y}$,
\begin{equation}
	\begin{split}
		\boldsymbol{\sigma}_{\mathbf{y}} & = [\sigma_{\mathbf{y}1}, \cdots, \sigma_{\mathbf{y}p}, \cdots, \sigma_{\mathbf{y}P}] \\
		\sigma_{\mathbf{y}p} & = \sqrt{\frac{1}{N-1} \sum_{n=1}^{N} (y(\mathbf{x}_n)_{p} - \bar{y}_p)^2}
	\end{split}
\label{eq:sample_standard_deviation}
\end{equation}

% Singular Value Decomposition
The standardized raw outputs $\mathbf{Y}^*$ is then decomposed by means of \gls[hyper=false]{svd} yielding,
\begin{equation}
	\mathbf{Y}^* = \mathbf{U} \mathbf{S} \mathbf{V}^T
\label{eq:svd_raw_outputs}
\end{equation}
where $\mathbf{U}$ is the $N \times N$ orthonormal, left singular matrix;
$\mathbf{S}$ is the $N \times N$ diagonal matrix of singular values;
and $\mathbf{V}$ is the $N \times P$ orthogonal, right singular matrix.

% Principal Component and Principal Component Scores
The matrix $\boldsymbol{\Phi}$ in Eq.~(\ref{eq:lmc}) is substituted by a set of empirical orthogonal basis functions obtained from the first $Q$ \emph{principal components} (eigenvectors) of the dataset, which is defined as
\begin{equation}
	\boldsymbol{\Phi} = [\mathbf{v_1}, \cdots, \mathbf{v}_q, \cdots, \mathbf{v}_Q]
\label{eq:pc_direction}
\end{equation}
where $\mathbf{v}_q$ is the $P$-dimensional column-vector taken from the $q$-th column of matrix $V$;
and $Q \leq P$.
The principal components of the dataset describe the main direction of the dataset.
The main direction, in turn, is defined such that the transformation of the data into the new coordinate system will maximize its variance.
The partial (explained) variance of the principal components can be obtained from diagonal elements of the matrix $\mathbf{S}^2/(N-1)$.

The partial variance obtained from the diagonal is automatically sorted in descending order with the top-left element being the largest.
As such, the first principal component is always associated with the largest partial variance.
Futhermore, the partial variance associated with a principal component also quantifies the strength of the component relative to the others.

Projection of the data into the principal components results in \emph{principal component scores} (PC scores),
\begin{equation}
	\mathbf{W} = \mathbf{Y}^* \mathbf{V} = \mathbf{U} \mathbf{S}
\label{eq:pc_scores}
\end{equation}
where $W$ is the $N \times Q$ matrix of principal component scores.
A unique set of $Q$ principal component scores are associated with each points in the multivariate dataset.
The scores describe the locations of the multivariate data points in the new coordinate system as defined by the principal components.

% Illustration here
\bigdoublefigure[pos=tbhp,
                 mainlabel={fig:pca_illustrate},
								 mainshortcaption={Random surface realizations},
                 maincaption={PCA},
                 leftopt={width=0.475\textwidth},
                 leftlabel={fig:pca_original},
                 leftcaption={Original data},
                 rightopt={width=0.475\textwidth},
                 rightlabel={fig:pca_transformed},
                 rightcaption={Transformed data},
                 ]
{../figures/chapter4/figures/plotPCA_1}
{../figures/chapter4/figures/plotPCA_2}

% Dimension Reduction
The dimension reduction takes place when the $Q$ is chosen such that $Q << P$.
Such selection is justified by a certain amount of partial variance explained by the first $Q$ principal components.

The singular values are related to the explained variance of the eigenvectors (i.e., their respective eigenvalues) by the following,
\begin{equation}
	\boldsymbol{\lambda} = \text{diag}\left(\frac{\mathbf{S}^2}{N-1}\right)
\label{eq:singular_values_variance}
\end{equation}
Selection of the number of principal components to retain usually justified by the.

% Truncation Error

%------------------------------------------------------------------------------
\subsection{Multivariate Gaussian Process Metamodel}\label{sub:gp_multivariate}
%------------------------------------------------------------------------------

% LMC - PCA
The multivariate output formulation of a \gls[hyper=false]{gp} metamodel based on the previous discussion is summarized as the following equation, 
where a prediction at an arbitrary input $\mathbf{x}_o \in \mathcal{X}$ is made,
\begin{equation}
		\bm{Y}(\mathbf{x}_o) = \bar{\mathbf{Y}} + \text{diag}(\boldsymbol{\sigma}_{\mathbf{y}}) (\boldsymbol{\Phi}_Q \bm{w} (\mathbf{x}_o) + \boldsymbol{\Phi}_{>Q} \mathbf{e})
\label{eq:lmc_pca}
\end{equation}
where $\bar{\bm{Y}}$ is the $P$-dimensional vector of sample mean (Eq.~(\ref{eq:sample_mean}));
and $\text{diag}(\boldsymbol{\sigma}_{\mathbf{y}})$ is the $P \times P$ diagonal matrix of sample standard deviation (Eq.~(\ref{eq:sample_standard_deviation})). 
The other elements in the equation is described below.

% The Retained Eigenvectors
$\boldsymbol{\Phi}_Q$, a $P \times Q$ matrix, is the first $Q$ columns of the PC loadings retained to reconstruct the multivariate output.
Specifically, $\boldsymbol{\Phi}_Q$ is,
\begin{equation}
		\boldsymbol{\Phi}_Q = (\boldsymbol{\phi}_1, \boldsymbol{\phi}_2, \cdots, \boldsymbol{\phi}_Q) ; \, Q < P
\label{eq:retained_pc}
\end{equation}
where $\boldsymbol{\phi}_i$ is the $P$-dimensional column vector of the $i$-th PC loading.

% The Standardized PC Scores, a Gaussian process
$\mathbf{w}$ is the $Q$-dimensional vector of standardized PC scores for each PC loadings, modeled as set of univariate, independent, zero-mean \gls[hyper=false]{gp},
\begin{equation}
		\begin{split}
			\bm{w} & = [\mathit{w}_1, \mathit{w}_2, \cdots, \mathit{w}_Q] ; \, Q < P \\
			 \mathit{w}_i (\circ) & \sim \mathcal{GP}(0, \sigma_i^2 R_i(\circ, \circ)) 
		\end{split}
\label{eq:retained_pc}
\end{equation}
where $\mathit{w}_i$ is the standardized PC scores of the $i$-th PC loading;
$\sigma_i^2$ and $R_i(\circ, \circ)$ are the process variance and the correlation function 
associated with \gls[hyper=false]{gp} of $\mathit{w}_i$, respectively.

% Conditioning the process by observed data
The observed data from the training samples is related to each of the $\mathit{w}_i$ by,
\begin{equation}
	\bm{w}_i(\mathbf{DM}) \equiv \mathbf{w}_i = \mathbf{Y}^* \boldsymbol{\phi}_i ; \, i = 1, \cdots, Q
\label{eq:data_w}
\end{equation}
That is, the observed data for the $i$-th standardized PC score is the projection of the standardized data on the $i$-th PC loading.
Conditioning the \gls[hyper=false]{gp} of $\mathit{w}_i$ by the observed data yields,
\begin{equation}
	\bm{w}_i (\mathbf{x}_o) | \mathbf{w}_i \sim \mathcal{N} (m_{SK,i}(\mathbf{x}_o), s^2_{SK,i}(\mathbf{x}_o)) ; \, i = 1, \cdots, Q 
\label{eq:conditional_w}
\end{equation}
where $m_{SK,i}$ and $s^2_{SK,i}$ are the simple Kriging mean and variance, respectively (Eq.~(\ref{eq:mean_sk}) and Eq.~(\ref{eq:variance_sk})),
associated to the $i$-th standardized PC score.
The simple Kriging formulation is used here as the assumed process is already centered (zero-mean).

% The unretained eigenvectors
$\boldsymbol{\Phi}_{>Q}$, a $P \times (P-Q)$ matrix, is the unretained columns of the PC loadings,
\begin{equation}
		\boldsymbol{\Phi}_{>Q} = (\boldsymbol{\phi}_{Q+1}, \boldsymbol{\phi}_{Q+2}, \cdots, \boldsymbol{\phi}_N)
\label{eq:unretained_pc}
\end{equation}
where $\boldsymbol{\phi}_{Q+i}$ is the $P$-dimensional column vector of the unretained $i$-th PC loading.

Finally, following \cite{Wilkinson2010}, $\mathbf{e}$ is the $(P-Q)$-dimensional vector of independent identically distributed normal random variable with mean $0$ and variance $1$.
In other words, truncation error in Eq.~(\ref{eq:lmc_pca}) due to the unretained PC loadings are modeled as independent normal random variable with the variance given the PC loadings.

% The Full Probability Model
The multivariate output at evaluated $\mathbf{x}_o$ conditioned by the training samples is thus distributed as $P$-variate Gaussian random variable,
\begin{equation}
	\begin{split}
		& \bm{Y} (\mathbf{x}_o) | \bm{Y}(DM)=\mathbf{Y} \sim \mathcal{N}_P (\boldsymbol{\mu}_P (\mathbf{x}_o), \Sigma_{P \times P} (\mathbf{x}_o)) \\
		& \boldsymbol{\mu}_P  = \bar{\mathbf{Y}} + \text{diag}(\boldsymbol{\sigma}_{\mathbf{y}}) \boldsymbol{\Phi} \mathbf{m}_{SK}(\mathbf{x}_o) \\
		& \Sigma_{P \times P} = \text{diag}(\boldsymbol{\sigma}_{\mathbf{y}}) (\boldsymbol{\Phi} \text{diag}(\mathbf{s}^2_{SK}(\mathbf{x}_o)) \boldsymbol{\Phi}^T + \boldsymbol{\Phi}_{>Q} \mathbf{I}\boldsymbol{\Phi}^T_{>Q}) \text{diag}(\boldsymbol{\sigma}_{\mathbf{y}}) \\
		& \mathbf{m}_{SK} = [m_{SK,1}(\mathbf{x}_o), m_{SK,2}(\mathbf{x}_o), \cdots, m_{SK,Q}(\mathbf{x}_o)] \\
		& \mathbf{s}^2_{SK} = [s^2_{SK,1}(\mathbf{x}_o), s^2_{SK,2}(\mathbf{x}_o), \cdots, s^2_{SK,Q}(\mathbf{x}_o)]
	\end{split}
\label{eq:p_variate_metamodel}
\end{equation} 