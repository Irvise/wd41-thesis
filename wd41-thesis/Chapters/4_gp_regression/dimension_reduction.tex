%*************************************************************************** 
\section{Dealing with Multivariate Output}\label{sec:gp_dimension_reduction}
%***************************************************************************

% Introductory Paragraph, Multiple Output
The previous discussion on \gls[hyper=false]{gp} metamodel dealt with a single output (univariate) case.
Many computer simulations produce multivariate outputs\footnote{in this thesis the number of outputs are referred to as the \emph{dimension of the output parameter space}.}.
\marginpar{multivariate outputs}
A typical \gls[hyper=false]{trace} simulation, for example, produces flow variables as functions of time and space as its \emph{raw} outputs.
This is indeed the case for the reflood simulation problem presented in Chapter~\ref{ch:trace_reflood}.
As outlined in Chapter~\ref{ch:sensitivity_analysis}, some techniques can be used to transform the raw outputs into quantities of interest (the maximum, etc.) that are useful to answer the questions at hand.
However, in the calibration setting, some of these outputs have corresponding measurement data and need to be represented by the metamodel in their original form for a direct comparison.

% Introductory Paragraph, one suggested approach
An approach proposed in \cite{Kleijnen2000} is to represent the multiple outputs by metamodels separately.
\marginpar{separate univariate metamodel}
That is, one metamodel is developed to represent each one of the multiple outputs individually.
Yet, for a very high-dimensional output (from tens to thousands), this approach is impractical as the numbers of metamodel to train becomes too numerous.
Furthermore, the outputs produced by the computer simulation are often highly correlated to each other.
As such, developing individual metamodels to represent the correlated outputs separately, especially when they are numerous, is wasteful. 

% The approach in this thesis
To cope with the problem of high-dimensionality of the outputs,
\marginpar{extension to multivariate case} 
this thesis adopted \gls[hyper=false]{lmc} \cite{Goulard1992,Paulo2012} coupled with \gls[hyper=false]{pca} technique \cite{Jolliffe2002,Higdon2008} to construct a tractable, multivariate version of \gls[hyper=false]{gp} metamodel.
The original \gls[hyper=false]{lmc} was formulated to model multivariate data in geostatistics that covary together over a region in a linear fashion, 
while \gls[hyper=false]{pca} is used here as a data-driven dimensional reduction tool.
The resulting model consists of few \emph{independent, univariate} \gls[hyper=false]{gp} metamodels, each of which is the one presented in the previous section.

%---------------------------------------------------------------------
\subsection{Linear Model of Coregionalization (LMC)}\label{sub:gp_lmc}
%---------------------------------------------------------------------

% Linear Model of Coregionalization
The function that represents the computer code simulation $f$ is now cast in its multivariate version,
$\mathbf{f}:\mathbf{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}^P$ where $P$ is the dimension of the output parameter space.
\marginpar{Linear model of coregionalization}
The \gls[hyper=false]{lmc} of the $P$-dimensional \gls[hyper=false]{gp} metamodel $\bm{\mathcal{Y}}$ can be written as,
\begin{equation}
	\bm{\mathcal{Y}}(\mathbf{x}) = \boldsymbol{\mu}(\mathbf{x}) + \boldsymbol{\Phi} \bm{\mathit{w}} (\mathbf{x}) + \boldsymbol{\epsilon}
\label{eq:lmc}
\end{equation}
where $\boldsymbol{\mu}$ is the $P$-dimensional mean vector of the multivariate process;
$\boldsymbol{\Phi}$ is a $P \times Q$ matrix, with $Q \leq P$;
$\boldsymbol{\epsilon}$ is a $P$-dimensional vector of li\-nearization error;
and $\bm{\mathit{w}} (\mathbf{x}) = (\mathit{w}_i(\mathbf{x}))$ is a $Q$-dimensional vector with univariate \glspl[hyper=false]{gp} as its elements,
\begin{equation}
	w_i(\mathbf{x}) \sim \mathcal{GP} (0, \sigma^2_i R_i(\mathbf{x}, \mathbf{x}^*))
\label{eq:lmc_weight}
\end{equation}
where $\sigma^2_i$ and $R_i$ are the process variance and correlation function associated with each element of the vector, respectively.
The term $\boldsymbol{\Phi} \mathbf{w}(\mathbf{x})$ describes the covariation between the multivariate outputs as function of model parameters.

%----------------------------------------------------------
\subsection{Principal Component Analysis}\label{sub:gp_pca}
%----------------------------------------------------------

% Principal Component Analysis
\gls[hyper=false]{pca} is then used as a data-driven approach to obtain the components of the \gls[hyper=false]{lmc} in Eq.~(\ref{eq:lmc}).
The term data-driven is used as the components are derived directly from the training data.
The raw outputs of the training runs are first concatenated row-wise resulting in an $N \times P$ matrix $\mathbf{Y}(\mathbf{DM})$,
\begin{equation}
	\begin{split}
		\mathbf{Y}(\mathbf{DM}) & = 
			\begin{pmatrix}
																	& \vdots	& \\
				\rule[.5ex]{2.5em}{0.4pt}	& \mathbf{y}_n	&	\rule[.5ex]{2.5em}{0.4pt} \\
																	& \vdots	&
			\end{pmatrix} \\
		\mathbf{y}_n & = (y_{n1}, \cdots, y_{np}, \cdots, y_{nP}) \\
		             & = (y(\mathbf{x}_n)_1, \cdots, y(\mathbf{x}_n)_p, \cdots, y(\mathbf{x}_n)_P)
	\end{split}
\label{eq:raw_output}
\end{equation}
where $y(\mathbf{x}_n)_p$ is the $p$-th output dimension, evaluated using the $n$-th training sample.
Note that the notation above is similar to Eq.~(\ref{eq:discrete_time}) but now the dimension of the output $y_p$ is not only restricted to time, 
nor they have to be of the same (physical) dimension.
In the formulation below, the raw training outputs is always assumed to be dependent on the training samples and thus the notation $\mathbf{DM}$ is suppressed.

% The mean
The sample mean of the raw outputs is used to substitute the mean in the \gls[hyper=false]{lmc} formulation,
\begin{equation}
	\boldsymbol{\mu}(\mathbf{x}) = \bar{\mathbf{y}}^T
\label{eq:lmc_mean}
\end{equation}
The sample mean is obtained by taking the column-wise average of Eq.~(\ref{eq:raw_output}),
\begin{equation}
	\begin{split}
		\bar{\mathbf{y}} & = [\bar{y}_1, \cdots, \bar{y}_p, \cdots, \bar{y}_P] \\
		\bar{y}_p & = \frac{1}{N} \sum_{n=1}^{N} y(\mathbf{x}_n)_{p} \\
	\end{split}
\label{eq:sample_mean}
\end{equation}
Note that by the above, it implies the mean of the \gls[hyper=false]{lmc} is a constant vector.

% Centering of the output
As the \gls[hyper=false]{pca} deals with the data covariance matrix,
the raw outputs in Eq.~(\ref{eq:raw_output}) should first be centered,
\begin{equation}
	\mathbf{Y}^* = (\mathbf{Y} - \mathbf{j}_N \bar{\mathbf{y}})
\label{eq:centered_raw_output}
\end{equation}
where $\mathbf{j}_N$ is the $N$-dimensional vector of ones.

% Singular Value Decomposition
The centered raw outputs $\mathbf{Y}^*$ is then decomposed by means of \gls[hyper=false]{svd} yielding,
\begin{equation}
	\mathbf{Y}^* = \mathbf{U} \mathbf{S} \mathbf{V}^T
\label{eq:svd_raw_outputs}
\end{equation}
where $\mathbf{U}$ is the $N \times N$ orthonormal, left singular matrix;
$\mathbf{S}$ is the $N \times P$ diagonal matrix of singular values;
and $\mathbf{V}$ is the $P \times P$ orthogonal, right singular matrix.

% Principal Component and Principal Component Scores
The column vector elements of $\mathbf{V}$ are the principal components of the dataset which describe the main directions of the dataset, along which the variance of the dataset is the largest.
Principal components are sorted in descending order such that the first principal component (leftmost in $\mathbf{V}$) contains the largest variance (Fig.~\ref{fig:pca_original}).
% Partial Variance of principal component
The singular values are related to the explained variance of the eigenvectors (i.e., their respective eigenvalues) by the following,
\begin{equation}
	\boldsymbol{\lambda} = \text{diag}\left(\frac{\mathbf{S}^2}{N-1}\right)
\label{eq:singular_values_variance}
\end{equation}
\bigdoublefigure[pos=tbhp,
                 mainlabel={fig:pca_illustrate},
								 mainshortcaption={Principal Component Analysis of a bivariate dataset},
                 maincaption={\gls[hyper=false]{pca} of a bivariate dataset. A highly correlated bivariate dataset can be transformed into a new orthogonal coordinate system according to the principal direction of the dataset. The principal directions redistribute the total variance such that the partial variance is preserved in the transformed coordinate. Illustrated above, $5$ selected points in the dataset in both coordinates.},
                 leftopt={width=0.475\textwidth},
                 leftlabel={fig:pca_original},
                 leftcaption={Original data},
                 rightopt={width=0.475\textwidth},
                 rightlabel={fig:pca_transformed},
                 rightcaption={Transformed data},
                 ]
{../figures/chapter4/figures/plotPCA_1}
{../figures/chapter4/figures/plotPCA_2}

% Projection of data
Projection of the data into the principal components results in \emph{principal component scores} (PC scores),
\begin{equation}
	\mathbf{W} = \mathbf{Y}^* \mathbf{V} = \mathbf{U} \mathbf{S}
\label{eq:pc_scores}
\end{equation}
where $W$ is the $N \times P$ matrix of principal component scores.
A unique set of $P$ principal component scores are associated with each points in the multivariate dataset.
The scores describe the locations of the multivariate data points in the new coordinate system as defined by the principal components (Fig.~\ref{fig:pca_transformed}).

%The partial variance obtained from the diagonal is automatically sorted in descending order with the top-left element being the largest.
%As such, the first principal component is always associated with the largest partial variance.
%Futhermore, the partial variance associated with a principal component also quantifies the strength of the component relative to the others.
% 
Often the results of \gls[hyper=false]{pca} are reformulated in terms of principal component loadings and standardized scores,
\begin{equation}
	\mathbf{V}^* = \frac{1}{\sqrt{N-1}} \mathbf{V} \mathbf{S} 
\label{eq:pc_loadings}
\end{equation}
\begin{equation}
	\mathbf{W}^* = \sqrt{N-1} \mathbf{U}
\label{eq:standardized_pc_scores}
\end{equation}
This reformulation gives a magnitude to the unit-norm principal components according to their respective standard deviation (square-root of variance Eq.~(\ref{eq:singular_values_variance})), it also results in standardized PC scores with variance of $1.0$.
In the context of regression of PC scores used later in metamodeling, the latter property improves the numerical stability of the maximum likelihood estimation of the hyper-parameters. 

% Dimension Reduction
Dimension reduction takes place when only a small numbers of \glspl[hyper=false]{pc} are kept.
\marginpar{Dimension Reduction}
That is, only the first $Q$ columns of $\mathbf{V}^*$ is retained with $Q << P$.
Such selection is justified by a certain amount of partial variance explained by those few first $Q$ principal components.
In the illustration of Fig.~\ref{fig:pca_illustrate},
the dimension reduction is carried out by simply using the first principal component (horizontal axis of Fig.~\ref{fig:pca_transformed}) to describe the dataset.

Back to the formulation of \gls[hyper=false]{lmc},
the matrix $\boldsymbol{\Phi}$ in Eq.~(\ref{eq:lmc}) is substituted by the set of empirical orthogonal basis functions obtained from the first $Q$ \gls[hyper=false]{pc} loadings of the dataset, which is defined as
\begin{equation}
	\boldsymbol{\Phi} = [\mathbf{v}^*_1, \cdots, \mathbf{v}^*_q, \cdots, \mathbf{v}^*_Q]
\label{eq:pc_reduction}
\end{equation}
where $\mathbf{v}_q$ is the $P$-dimensional column-vector taken from the $q$-th column of matrix $V$;
and $Q << P$.
This empirical orthogonal basis functions expansion is obviously related to the ones presented in Chapter 3.
The analysis done here, however, used the point-wise data formulation as opposed to functional formulation.

%------------------------------------------------------------------------------
\subsection{Multivariate Gaussian Process Metamodel}\label{sub:gp_multivariate}
%------------------------------------------------------------------------------

% LMC - PCA
The multivariate output formulation of a \gls[hyper=false]{gp} metamodel based on the previous discussion is summarized as the following equation, 
where a prediction at an arbitrary input $\mathbf{x}_o \in \mathbf{X}$ is made,
\begin{equation}
		\bm{\mathcal{Y}}(\mathbf{x}_o) = \bar{\mathbf{Y}} + \boldsymbol{\Phi}_Q \bm{\mathit{w}}^* (\mathbf{x}_o) + \boldsymbol{\Phi}_{>Q} \mathbf{e}
\label{eq:lmc_pca}
\end{equation}
where $\bar{\bm{Y}}$ is the $P$-dimensional vector of sample mean (Eq.~(\ref{eq:sample_mean})).
The other elements in the equation are described below.

% The Retained Eigenvectors
$\boldsymbol{\Phi}_Q$, a $P \times Q$ matrix, is the first $Q$ columns of the PC loadings retained to reconstruct the multivariate output.
Specifically, $\boldsymbol{\Phi}_Q$ is,
\begin{equation}
		\boldsymbol{\Phi}_Q = (\boldsymbol{\Phi}^*_1, \boldsymbol{\Phi}^*_2, \cdots, \boldsymbol{\Phi}^*_Q) = (\mathbf{v}^*_1, \mathbf{v}^*_2, \cdots, \mathbf{v}^*_Q) ; \, Q << P
\label{eq:retained_pc}
\end{equation}
where $\boldsymbol{v}^*_i$ is the $P$-dimensional column vector of the $i$-th PC loading taken from Eq.(~\ref{eq:pc_reduction}).

% The Standardized PC Scores, a Gaussian process
$\bm{\mathit{w}}^*$ is the $Q$-dimensional vector of standardized PC scores for each of the retained PC loading,
modeled as a set of univariate, independent, zero-mean \gls[hyper=false]{gp},
\begin{equation}
		\begin{split}
			\bm{\mathit{w}}^* & = [\mathit{w}^*_1, \mathit{w}^*_2, \cdots, \mathit{w}^*_Q] ; \, Q < P \\
			 \mathit{w}^*_i (\circ) & \sim \mathcal{GP}(0, \sigma_i^2 R_i(\circ, \circ)) 
		\end{split}
\label{eq:retained_pc_scores}
\end{equation}
where $\mathit{w}^*_i$ is the standardized PC scores of the $i$-th PC loading;
$\sigma_i^2$ and $R_i(\circ, \circ)$ are the process variance and the correlation function 
associated with \gls[hyper=false]{gp} of $\mathit{w}^*_i$, respectively.

% Conditioning the process by observed data
The observed data from the $N$ training samples is related to each of the $\mathit{w}_i$ by,
\begin{equation}
	\bm{w}^*_i(\mathbf{DM}) \equiv \mathbf{w}^*_i = \sqrt{N - 1} \mathbf{Y}^* \mathbf{S}^{-1} \mathbf{V}_i ; \, i = 1, \cdots, Q
\label{eq:data_w}
\end{equation}
That is, the observed data for the $i$-th standardized PC score is the projection of the standardized data on the $i$-th PC loading.
Conditioning the \gls[hyper=false]{gp} of $\mathit{w}_i$ by the observed data yields,
\begin{equation}
	\bm{w}_i (\mathbf{x}_o) | \mathbf{w}_i \sim \mathcal{N} (m_{SK,i}(\mathbf{x}_o), s^2_{SK,i}(\mathbf{x}_o)) ; \, i = 1, \cdots, Q 
\label{eq:conditional_w}
\end{equation}
where $m_{SK,i}$ and $s^2_{SK,i}$ are the simple Kriging mean and variance, respectively (Eq.~(\ref{eq:mean_sk}) and Eq.~(\ref{eq:variance_sk})),
associated to the $i$-th standardized PC score.
The simple Kriging formulation is used here as the assumed process is already centered (zero-mean).

% The unretained eigenvectors
$\boldsymbol{\Phi}_{>Q}$, a $P \times (P-Q)$ matrix, is the unretained columns of the PC loadings,
\begin{equation}
		\boldsymbol{\Phi}_{>Q} = (\boldsymbol{\phi}_{Q+1}, \boldsymbol{\phi}_{Q+2}, \cdots, \boldsymbol{\phi}_P) = (\mathbf{v}_{Q+1}, \mathbf{v}_{Q+2}, \cdots, \mathbf{v}_P)
\label{eq:unretained_pc}
\end{equation}
where $\boldsymbol{\phi}_{Q+i}$ is the $P$-dimensional column vector of the unretained $i$-th PC loading taken from Eq.(~\ref{eq:pc_loadings}).

Finally, following \cite{Wilkinson2010},
$\mathbf{e}$ is the $(P-Q)$-dimensional vector of independent identically distributed normal random variable with mean $0$ and variance $1$.
In other words, truncation error in Eq.~(\ref{eq:lmc_pca}) due to the unretained PC loadings are modeled as a set of independent normal random variables with the variance given by the PC loadings.

% The Full Probability Model
The multivariate output at evaluated $\mathbf{x}_o$ conditioned by the training data is thus distributed as $P$-variate Gaussian random variable,
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}} (\mathbf{x}_o) | \bm{\mathcal{Y}} (DM) = \mathbf{Y} \sim \mathcal{N}_P (\boldsymbol{\mu}_P (\mathbf{x}_o), \Sigma_{P \times P} (\mathbf{x}_o)) \\
		& \boldsymbol{\mu}_P  = \bar{\mathbf{Y}} + \boldsymbol{\Phi}_Q \mathbf{m}_{SK}(\mathbf{x}_o) \\
		& \Sigma_{P \times P} = \boldsymbol{\Phi}_Q \text{diag}(\mathbf{s}^2_{SK}(\mathbf{x}_o)) \boldsymbol{\Phi}^T_Q + \boldsymbol{\Phi}_{>Q} \mathbf{I}\boldsymbol{\Phi}^T_{>Q}) \\
		& \mathbf{m}_{SK} = [m_{SK,1}(\mathbf{x}_o), m_{SK,2}(\mathbf{x}_o), \cdots, m_{SK,Q}(\mathbf{x}_o)] \\
		& \mathbf{s}^2_{SK} = [s^2_{SK,1}(\mathbf{x}_o), s^2_{SK,2}(\mathbf{x}_o), \cdots, s^2_{SK,Q}(\mathbf{x}_o)]
	\end{split}
\label{eq:p_variate_metamodel}
\end{equation}
The model above assumed that the hyper-parameters associated with a selected correlation function are known.
In practice they are not and thus estimated from the data itself using \gls[hyper=false]{mle} as outlined in Section~\ref{sub:gp_fitting}.
Additionally, the underestimation of the Kriging variance explained in that section is also the case here when the \gls[hyper=false]{ml} estimates are plugged-in to the formulation.