\section{Dealing with Multivariate Output}\label{sec:gp_dimension_reduction}

% Introductory Paragraph, Multiple Output
The previous discussion on \gls[hyper=false]{gp} metamodel dealt with a single output (univariate) case.
On the other hand, many computer simulations produce multiple (multivariate) outputs\footnote{in this thesis the number of outputs are referred to as the \emph{dimension of the output parameter space}.}.
\marginpar{multivariate outputs}
A typical \gls[hyper=false]{trace} simulation, for example, produces flow variables as functions of time and space as its \emph{raw} outputs.
This is indeed the case for the reflood simulation problem presented in Chapter~\ref{ch:trace_reflood}.
As outlined in Chapter~\ref{ch:sensitivity_analysis}, some techniques can be used to transform the raw outputs into quantities of interest (the maximum, etc.) that are useful to answer the questions at hand.
However, in the calibration setting, some of these outputs have corresponding measurement data and need to be represented by the metamodel in their original form for a direct comparison.

% Introductory Paragraph, one suggested approach
An approach proposed in \cite{Kleijnen2000} is to represent the multiple outputs by metamodels separately.
\marginpar{separate univariate metamodel}
In other words, one metamodel is developed to represent each one of the multiple outputs individually.
Yet, for a very high-dimensional output (from tens to thousands), this approach is impractical as the numbers of metamodel to train becomes too numerous.
In addition to that, the outputs produced by the computer simulation are often highly correlated to each other.
As such, developing individual metamodels to represent the correlated outputs separately, especially when they are numerous, are wasteful. 

% The approach in this thesis
To cope with the problem of high-dimensionality of the outputs,
\marginpar{extension to multivariate case} 
this thesis adopted \gls[hyper=false]{lmc} (cite Raul) coupled with \gls[hyper=false]{pca} technique (cite Jolife and Higdon) to construct a tractable, multivariate version of \gls[hyper=false]{gp} metamodel.
The original \gls[hyper=false]{lmc} was formulated to model multivariate data in geostatistics that covary together over a region in a linear fashion, 
while \gls[hyper=false]{pca} is used here a data-driven dimensional reduction tool.
The resulting model consists of few \emph{independent, univariate} \gls[hyper=false]{gp} metamodels, each of which is the one presented in the previous section.

% Linear Model of Coregionalization
The function that represents the computer code simulation $f$ is now cast in its multivariate version, $\mathbf{f}:\mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}^P$ where $P$ is the dimension of the output parameter space.
\marginpar{Linear model of coregionalization}
The \gls[hyper=false]{lmc} of the $P$-dimensional \gls[hyper=false]{gp} metamodel $\tilde{\mathbf{f}}$ can be written as,
\begin{equation}
	\tilde{\mathbf{f}}(\mathbf{x}) = \boldsymbol{\mu}(\mathbf{x}) + \boldsymbol{\Phi} \mathbf{w}(\mathbf{x}) + \boldsymbol{\epsilon}
\label{eq:lmc}
\end{equation}
where $\boldsymbol{\mu}$ is the $P$-dimensional mean vector of the multivariate process;
$\boldsymbol{\Phi}$ is a $P \times Q$ matrix, with $R \leq Q$;
$\boldsymbol{\epsilon}$ is a $P$-dimensional vector of li\-nearization error;
and $\mathbf{w}(\mathbf{x}) = (w_i(\mathbf{x}))$ is a $Q$-dimensional vector with univariate \glspl[hyper=false]{gp} as its elements,
\begin{equation}
	w_i(\mathbf{x}) \sim \mathcal{GP} (0, \sigma^2_i R_i(\mathbf{x}, \mathbf{x}^*))
\label{eq:lmc_weight}
\end{equation}
where $\sigma^2_i$ and $R_i$ are the process variance and correlation function associated with each element of the vector, respectively.
The term $\boldsymbol{\Phi} \mathbf{w}(\mathbf{x})$ describes the covariation between the multivariate outputs as function of model parameters.

% Principal Component Analysis
\gls[hyper=false]{pca} is then used as a data-driven approach to obtain the components of the \gls[hyper=false]{lmc} in Eq.~(\ref{eq:lmc}).
The term data-driven is used as the components are derived directly from the training samples.
The raw outputs of the training runs are first concatenated row-wise resulting in an $N \times P$ matrix $\mathbf{Y}(\mathbf{DM})$,
\begin{equation}
	\begin{split}
		\mathbf{Y}(\mathbf{DM}) & = 
			\begin{pmatrix}
																	& \vdots	& \\
				\rule[.5ex]{2.5em}{0.4pt}	& \mathbf{y}_n	&	\rule[.5ex]{2.5em}{0.4pt} \\
																	& \vdots	&
			\end{pmatrix} \\
		\mathbf{y}_n & = (y_{n1}, \cdots, y_{np}, \cdots, y_{nP}) \\
		             & = (y(\mathbf{x}_n)_1, \cdots, y(\mathbf{x}_n)_p, \cdots, y(\mathbf{x}_n)_P)
	\end{split}
\label{eq:raw_output}
\end{equation}
where $y(\mathbf{x}_n)_p$ is the $p$-th output dimension, evaluated using the $n$-th training sample.
Note that the notation above is similar to Eq.~(\ref{eq:discrete_time}) but now the dimension of the output $y_p$ is not only restricted to time, 
nor they have to be of the same (physical) dimension.
In the formulation below, the raw training outputs is always assumed to be dependent on the training sample and thus the notation $\mathbf{DM}$ is suppressed.

% The mean
The sample mean of the raw outputs is used to substitute the mean in the \gls[hyper=false]{lmc} formulation,
\begin{equation}
	\boldsymbol{\mu}(\mathbf{x}) = \bar{\mathbf{y}}^T
\label{eq:lmc_mean}
\end{equation}
The sample mean is obtained by taking the column-wise average of Eq.~(\ref{eq:raw_output}),
\begin{equation}
	\begin{split}
		\bar{\mathbf{y}} & = [\bar{y}_1, \cdots, \bar{y}_p, \cdots, \bar{y}_P] \\
		\bar{y}_p & = \frac{1}{N} \sum_{n=1}^{N} y(\mathbf{x}_n)_{p} \\
	\end{split}
\label{eq:sample_mean}
\end{equation}
Note that by the above, it implies the mean of the \gls[hyper=false]{lmc} is a constant vector.

% Standardization of output
As the output dimensions might be of different physical dimensions or measurement units, the raw outputs in Eq.~(\ref{eq:raw_output}) is centered and standardized to a unit norm (or equivalently, unit variance),
\begin{equation}
	\mathbf{Y}^* = (\mathbf{Y} - \mathbf{j}_N \bar{\mathbf{y}}) \, \text{diag}^{-1}(\boldsymbol{\sigma}_{\mathbf{y}})
\label{eq:standardization_raw_output}
\end{equation}
where $\mathbf{j}_N$ is the $N$-dimensional vector of ones;
$\text{diag}^{-1} (\circ)$ is the inverse of diagonal matrix, of which the vector argument is its diagonal elements;
and $\sigma_{\mathbf{y}}$ is the $P$-dimensional vector of column-wise standard deviation of $\mathbf{Y}$,
\begin{equation}
	\begin{split}
		\boldsymbol{\sigma}_{\mathbf{y}} & = [\sigma_{\mathbf{y}1}, \cdots, \sigma_{\mathbf{y}p}, \cdots, \sigma_{\mathbf{y}P}] \\
		\sigma_{\mathbf{y}p} & = \sqrt{\frac{1}{N-1} \sum_{n=1}^{N} (y(\mathbf{x}_n)_{p} - \bar{y}_p)^2}
	\end{split}
\label{eq:sample_standard_deviation}
\end{equation}

% Singular Value Decomposition
The standardized raw outputs $\mathbf{Y}^*$ is then decomposed by means of \gls[hyper=false]{svd} yielding,
\begin{equation}
	\mathbf{Y}^* = \mathbf{U} \mathbf{S} \mathbf{V}^T
\label{eq:svd_raw_outputs}
\end{equation}
where $\mathbf{U}$ is the $N \times N$ orthonormal, left singular matrix;
$\mathbf{S}$ is the $N \times N$ diagonal matrix of singular values;
and $\mathbf{V}$ is the $N \times P$ orthogonal, right singular matrix.

% Principal Component and Principal Component Scores
The matrix $\boldsymbol{\Phi}$ in Eq.~(\ref{eq:lmc}) is substituted by a set of empirical orthogonal basis functions obtained from the first $Q$ \emph{principal components} (eigenvectors) of the dataset, which is defined as
\begin{equation}
	\boldsymbol{\Phi} = [\mathbf{v_1}, \cdots, \mathbf{v}_q, \cdots, \mathbf{v}_Q]
\label{eq:pc_direction}
\end{equation}
where $\mathbf{v}_q$ is the $P$-dimensional column-vector taken from the $q$-th column of matrix $V$;
and $Q \leq P$.
The principal components of the dataset describe the main direction of the dataset.
The main direction, in turn, is defined such that the transformation of the data into the new coordinate system will maximize its variance.
The partial (explained) variance of the principal components can be obtained from diagonal elements of the matrix $\mathbf{S}^2/(N-1)$.

The partial variance obtained from the diagonal is automatically sorted in descending order with the top-left element being the largest.
As such, the first principal component is always associated with the largest partial variance.
Futhermore, the partial variance associated with a principal component also quantifies the strength of the component relative to the others.

Projection of the data into the principal components results in \emph{principal component scores} (PC scores),
\begin{equation}
	\mathbf{W} = \mathbf{Y}^* \mathbf{V} = \mathbf{U} \mathbf{S}
\label{eq:pc_scores}
\end{equation}
where $W$ is the $N \times Q$ matrix of principal component scores.
A unique set of $Q$ principal component scores are associated with each points in the multivariate dataset.
The scores describe the locations of the multivariate data points in the new coordinate system as defined by the principal components.

% Illustration here

% Dimension Reduction
The dimension reduction takes place when the $Q$ is chosen such that $Q << P$.
Such selection is justified by a certain amount of partial variance explained by the first $Q$ principal components.


% Truncation Error

% Full Probabilistic Model