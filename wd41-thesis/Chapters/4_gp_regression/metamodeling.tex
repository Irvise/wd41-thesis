\section{Gaussian Process Metamodel and its Construction}\label{sec:gp_metamodeling}

% Kriging Model, Drift and Bias
To formalize the use of in the meta-modeling of computer simulation, 
consider once again a computer code represented as 

The output of code at an arbitrary inputs, $\mathbf{x_o}$, is then modeled using a predictor of the following form
\begin{equation}
	Y (\mathbf{x_o}) = \mu (\mathbf{x_o}) + Z (\mathbf{x_o})
\label{eq:kriging_model}
\end{equation}
The equation above, also known as the \emph{Kriging} model (cite Owen, Simpson, Dupuy)\footnote{in fact, the term Kriging model will be used interchangeably as Gaussian process metamodel in this thesis} , consists of two components:
\begin{itemize}
	\item The \emph{mean/drift/trend} term, a deterministic function
	\begin{equation}
		\mu: \mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}
	\label{eq:trend_function_mapping}
	\end{equation}
	The trend term, in turn, usually consists of general linear model of polynomials (cite Owen and Dupuy, Marrel).
	\begin{equation}
		\mu(\mathbf{x}) = \sum_{j=0}^{k} \beta_j h_j(\mathbf{x})
	\label{eq:trend_function_definition}
	\end{equation}
	The trend term captures the global input output space which has an equivalent interpretation to that of a linear regression model\footnote{$Y() = \mu(\mathbf{x} + \epsilon$ with $\epsilon$ is Normally independent and identically distributed measurement error term}.
	\item The \emph{bias} term, a stochastic process. 
	This term, in turn, usually modeled using zero mean, stationary Gaussian stochastic process (such as presented in Section~\ref{sub:gp_covariance}).
	\begin{equation}
		\mathcal{Z}(\mathbf{x}) \sim \mathcal{GP}(0, R(\mathbf{x},\mathbf{x}^*))
	\label{eq:stationary_gp}
	\end{equation}
	The bias term captures the local variation of the output space such that it pulls the predictor.
	The term local here is referring to the neighborhood of training data.
	In linear regression model the error between observation and prediction is due to experimental error.
	This, in essence, what distinguishes kriging model to ordinary least square.
	Furthermore, by using stationary Gaussian process it is also assumed that the bias term will be a function of distance between design points and test point. 
\end{itemize}

% Hyper-parameters
Additionaly, following the above formulation, a \gls[hyper=false]{gp} metamodel contains several parameters called the \emph{hyper-parameters}.
\marginpar{hyper-parameters}
This term is used to distinguish them from the parameter associated with the original simulation model which is referred to as the model parameter or often simply as the parameter.
The hyper-parameters of a \gls[hyper=false]{gp} metamodel are the ones associated with the trend function (Eq.~\ref{eq:trend_function_definition});
the ones associated with select correlation functions (Section~\ref{sub:gp_covariance}); and the one associated with the process variance $\sigma^2$.
The total number of hyper-parameters depends on the number of simulation model parameters as well as the select structure of mean and correlation functions.
For instance, for a $D$-parameter simulation model represented using \gls[hyper=false]{gp} metamodel with a constant mean and Gaussian correlation functions (Eq.~(\ref{eq:gaussian_kernel})), 
the total number of hyper-parameters $\boldsymbol{\Psi} = (\mu, \sigma^2, \boldsymbol{\theta})$ is $D + 2$.
On the other hand, the same model represented using a \gls[hyper=false]{gp} metamodel with linear first-order mean and power-exponential functions (Eq.~(\ref{eq:powexp_kernel})),
the total number of the hyper-parameters $\boldsymbol{\Psi} = (\boldsymbol{\beta}, \sigma^2, \boldsymbol{\theta}, \mathbf{p})$ is $3D + 2$.

\subsection{Simple, Ordinary, and Universal Kriging Models}\label{sub:gp_sk_ok_uk}

% Three classes of Kriging Model
Three classes of Kriging models can be distinguished depending on what is known about the hyper-parameters 
and what is assumed regarding the structure of the deterministic mean function $\mu(\mathbf{x})$: 
\emph{Simple Kriging}, \emph{Ordinary Kriging}, and \emph{Universal Kriging}.

% Simple Kriging
Simple Kriging is the simplest of the three where all the hyper-parameters involved are assumed to be known.
\marginpar{simple Kriging}
In that case the estimation of code output at an arbitrary input location becomes straightforward.
Given a deterministic function for the mean term and a stationary Gaussian stochastic process for the bias term following the formulation above, 
a \gls[hyper=false]{gp} metamodel, then by definition implies that the computer code outputs at every input locations are jointly Gaussian.
As such, the code outputs at the training inputs $\mathbf{DM} = \{\mathbf{x}_i\}_{i=1}^N, Y(\mathbf{DM}) = \{Y(\mathbf{x}_i)\}_{i=1}^N$ 
and the output at an arbitrary input location $\mathbf{x}_o$ are distributed jointly as a $N+1$-dimensional Gaussian,
\begin{equation}
	\begin{bmatrix}
			Y(\mathbf{DM}) \\
			Y(\mathbf{x}_o)
		\end{bmatrix} \sim \mathcal{N} \left(
			\begin{bmatrix}
				\mu(\mathbf{DM}) \\
				\mu(\mathbf{x}_o)
			\end{bmatrix}, \sigma^2
			\begin{bmatrix}
				R(\mathbf{DM}, \mathbf{DM})  & R(\mathbf{DM}, \mathbf{x}_o) \\
				R(\mathbf{x}_o, \mathbf{DM}) & R(\mathbf{x}_o, \mathbf{x}_o)
			\end{bmatrix} \right)
\label{eq:joint_training_test}
\end{equation}
where $\mu(\mathbf{DM}) = \{\mu(\mathbf{x}_i)\}_{i=1}^N$ is the vector of mean at the training inputs;
$\mu(\mathbf{x}_o)$ is the mean at an arbitrary input location;
$R(\mathbf{DM}, \mathbf{DM})$ is the $N \times N$ correlation matrix between point in the training set;
$R(\mathbf{DM}, \mathbf{x}_o) = R(\mathbf{x}_o, \mathbf{DM})$ is the $N \times 1$ vector of correlation between point in the training set and the test point;
and $R(\mathbf{x}_o, \mathbf{x}_o) = 1$ is the correlation.

Given the outputs at the training inputs are observed (i.e., the code is actually run at the inputs), $Y(\mathbf{DM}) = {y_i}_{i=1}^N$,
then the prediction at given the observation can be obtained through the properties of Gaussian distribution,
\begin{equation}
	Y(\mathbf{x}_o) | Y(\mathbf{DM}) = \{y_i\}_{i=1}^N \sim \mathcal{N} \left( \mu_{SK}(\mathbf{x}_o), s^2_{SK}(\mathbf{x})\right)
\label{eq:joint_training_test}
\end{equation}

% Kriging Predictor, the mean
The \emph{Kriging predictor} and the \emph{Kriging variance} are obtained through conditioning operation of a Gaussian random vector (see appendix).

% Kriging Variance

% Interesting Observation

% Ordinary Kriging and Universal Kriging

\subsection{Learning Hyperparameters}\label{sub:gp_learning_hyperparameters}

In the discussion above the,
In practice, however, these hyper-parameter values are not known and must be estimated by the data.

% Likelihood

% Profile/Concentrated Likelihood

% Maximum Likelihood Estimation / Empirical Bayes

% Fully Bayesian Approach

\subsection{Selection of Design Points}\label{sub:gp_design}

\subsection{Validation}\label{sub:gp_validation}

\subsection{Summary}\label{sub:gp_summary}