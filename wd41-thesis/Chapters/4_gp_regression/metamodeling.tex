\newpage
%**************************************************************
\section{Gaussian Process Metamodel}\label{sec:gp_metamodeling}
%**************************************************************

% Kriging Model, Drift and Bias
To formalize the use of \gls[hyper=false]{gp} in the metamodeling of a simulator, 
consider once again regression problem of predicting the output at an arbitrary input $f(\mathbf{x}_o) ; \, \mathbf{x}_o \notin \mathbf{DM}$ given $\{(\mathbf{DM}, \mathbf{y})\}$;
where $f$, $\mathbf{DM}$, $\mathbf{y}$ are the function representing the simulator, the design matrix, and training output, respectively.
A \gls[hyper=false]{gp} metamodel makes the prediction as
\begin{equation}
	\mathcal{Y} (\mathbf{x_o}) = \mu (\mathbf{x_o}) + \mathcal{Z} (\mathbf{x_o})
\label{eq:kriging_model}
\end{equation}
The equation above, the \emph{Kriging} model, consists of two components:
\begin{itemize}
	\item The \emph{mean/drift/trend} term, $\mu: \mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}$,
	is a deterministic function.
	The choice of the trend term distinguishes different classes of Kriging model.
	\emph{Simple Kriging} (SK), refers to a class of Kriging whose arbitrary trend function is fully specified up to its coefficients (if any).
	\emph{Universal Kriging} (UK), on the other hand, is a class of Kriging where a general polynomial model is assumed, but its coefficients are unknown \cite{Koehler1996,Ginsbourger2009,Marrel2008},
	\begin{equation}
		\mu(\mathbf{x}) = \sum_{j=0}^{J} \beta_j h_j(\mathbf{x})
	\label{eq:trend_polynomial}
	\end{equation}
	where $h_j$ is a set of polynomials basis function; and $\beta_j$ is the associated coefficients.
	\emph{Ordinary Kriging} (OK) is a special case of UK where the trend is set as an unknown constant ($h_0(\mathbf{x}) = 1;\,J = 0$). 
	\item The \emph{bias} or \emph{residual} term, a stochastic process. 
		In particular, it is modeled using a zero-mean, stationary Gaussian stochastic process,
		\begin{equation}
			\mathcal{Z}(\mathbf{x}) \sim \mathcal{GP}(0, \sigma^2 R(\mathbf{x},\mathbf{x}^*))
		\label{eq:stationary_gp}
		\end{equation}
		where $\sigma^2$ and $R$ are the process variance and a stationary correlation function (such as the ones presented in Section~\ref{sub:gp_covariance}), respectively.
		The residuals, being modeled as a \gls[hyper=false]{gp}, are correlated and this correlation is a function of inputs.
		As such, a Kriging model can be thought of as a generalized linear model whose elements of the correlation matrix is specified explicitly by a parametric function \cite{Martin2005}.
		Note that the predictor in Eq.~(\ref{eq:kriging_model}) becomes a stochastic process due to this bias term.
\end{itemize}

% Hyper-parameters
According to the above, a \gls[hyper=false]{gp} metamodel thus contains several parameters called the \emph{hyper-parameters}.
\marginpar{hyper-parameters}
This term is used to distinguish them from the parameter associated with the original simulation model which is referred to as the model parameter or simply as the parameter.
The hyper-parameters of a \gls[hyper=false]{gp} metamodel are the ones associated with the chosen trend function (Eq.~\ref{eq:trend_polynomial});
the ones associated with select correlation functions (Section~\ref{sub:gp_covariance}); and the process variance $\sigma^2$.
The total number of hyper-parameters depends on the number of model parameters as well as the select structure of mean and correlation functions.
For instance, for a $D$-parameter simulation model represented by a \gls[hyper=false]{gp} metamodel with linear first-order mean and power-exponential correlation function (Eq.~(\ref{eq:powexp_kernel})), the total number of the hyper-para\-meters $\boldsymbol{\Psi} = (\boldsymbol{\beta}, \sigma^2, \boldsymbol{\theta}, \mathbf{p})$ is $3D + 2$; while for the same model represented by a \gls[hyper=false]{gp} metamodel with a constant mean and Gaussian correlation functions (Eq.~(\ref{eq:gaussian_kernel})), the total number of hyper-parameters $\boldsymbol{\Psi} = (\mu, \sigma^2, \boldsymbol{\theta})$ is $D + 2$.
%while the same model 

% Two classes of Kriging Model
As mentioned, two classes of Kriging models can be distinguished depending on what is specified on the trend term: 
\emph{Simple Kriging} and \emph{Universal Kriging}. 
Simple Kriging is the simpler case where all the hyper-parameters involved are known.
In that case the prediction of output at an arbitrary input location is straightforward.

Following the formulation above, 
\marginpar{simple Kriging}
a \gls[hyper=false]{gp} metamodel, implies that the computer code outputs at every input locations are jointly Gaussian.
As such, the code outputs at the training inputs $\mathbf{DM} = \{\mathbf{x}_i\}_{i=1}^N, \mathcal{Y}(\mathbf{DM}) = (\mathcal{Y}(\mathbf{x}_1), \mathcal{Y}(\mathbf{x}_2), \cdots, \mathcal{Y}(\mathbf{x}_N))$
and the output at an arbitrary input $\mathbf{x}_o$, $\mathcal{Y}(\mathbf{x}_o)$ are distributed jointly as an $(N+1)$-dimensional Gaussian,
\begin{equation}
	\begin{bmatrix}
			\mathcal{Y}(\mathbf{DM}) \\
			\mathcal{Y}(\mathbf{x}_o)
		\end{bmatrix} \sim \mathcal{N} \left(
			\begin{bmatrix}
				\mu(\mathbf{DM}) \\
				\mu(\mathbf{x}_o)
			\end{bmatrix}, \sigma^2
			\begin{bmatrix}
				R(\mathbf{DM}, \mathbf{DM})  & R(\mathbf{DM}, \mathbf{x}_o) \\
				R(\mathbf{x}_o, \mathbf{DM}) & R(\mathbf{x}_o, \mathbf{x}_o)
			\end{bmatrix} \right)
\label{eq:joint_training_test}
\end{equation}
where
\begin{itemize}
	\item $\mu(\mathbf{DM})$ is the vector of mean at the training points,
		\begin{equation}
			\mu(\mathbf{DM}) = [\mu(\mathbf{x}_1), \cdots, \mu(\mathbf{x}_N)]^T 
		\label{eq:training_mean_vector}
		\end{equation}
	\item $\mu(\mathbf{x}_o)$ is the mean at an arbitrary test location.
	\item $R(\mathbf{DM}, \mathbf{DM})$ is the $N \times N$ correlation matrix between outputs at the training points,
		\begin{equation}
			R(\mathbf{DM}, \mathbf{DM}) = 
				\begin{bmatrix}
					R(\mathbf{x}_1, \mathbf{x}_1) & \cdots												& R(\mathbf{x}_1, \mathbf{x}_N) \\
					\vdots												& \ddots												&	\vdots \\
					R(\mathbf{x}_N, \mathbf{x}_1)	&	\cdots                        & R(\mathbf{x}_N, \mathbf{x}_N)
				\end{bmatrix}
		\label{eq:training_correlation_matrix}
		\end{equation}
	\item $R(\mathbf{DM}, \mathbf{x}_o) = R(\mathbf{x}_o, \mathbf{DM})$ is the $N \times 1$ vector of correlation between outputs at the training points and the output at the test point,
			\begin{equation}
				R(\mathbf{x}_o, \mathbf{DM}) = R(\mathbf{DM}, \mathbf{x}_o) =  [R(\mathbf{x}_o, \mathbf{x}_1), \cdots, R(\mathbf{x}_o, \mathbf{x}_N)]^T
			\label{eq:training_test_correlation}
			\end{equation}
		\item $R(\mathbf{x}_o, \mathbf{x}_o)$ is the correlation of the output at the test input with itself. By definition this correlation is equal to $1$.
\end{itemize}

% Conditional Distribution, Simple Kriging
Provided that the outputs at the training inputs are fully observed (i.e., the code is actually run at those inputs),
then the output at the test input $Y(\mathbf{x}_o)$ \emph{given	} the observed outputs $Y(\mathbf{DM}) = \mathbf{y} = (y_1, y_2, \cdots,$ 
$y_N)^T$ is a conditional Gaussian random variable,
\begin{equation}
	Y(\mathbf{x}_o) | Y(\mathbf{DM}) = \{y_i\}_{i=1}^N \sim \mathcal{N} \left( m_{SK}(\mathbf{x}_o), s^2_{SK}(\mathbf{x})\right)
\label{eq:joint_training_test}
\end{equation}
where $\mu_{SK}$ and $s^2_{SK}$ are the mean and the variance of the distribution, respectively.
They are also often referred to as the \emph{simple Kriging mean} and \emph{simple Kriging variance}, respectively.

% Kriging Predictor, the mean
The simple Kriging mean (or the \emph{Kriging predictor}) is expressed as follow
\marginpar{(simple) Kriging mean}
\begin{equation}
	m_{SK} (\mathbf{x}_o) = \mu (\mathbf{x}_o) + R^T(\mathbf{x}_o, \mathbf{DM}) R^{-1}(\mathbf{DM}, \mathbf{DM}) (\mathbf{y} - \mu(\mathbf{DM}))
\label{eq:mean_sk}
\end{equation}
% Kriging Variance
The simple Kriging variance, on the other hand, is expressed as
\marginpar{(simple) Kriging variance}
\begin{equation}
	s^2_{SK} (\mathbf{x}_o) = \sigma^2 (1 - R^T(\mathbf{x}_o, \mathbf{DM}) R^T(\mathbf{DM}, \mathbf{DM}) R(\mathbf{x}_o, \mathbf{DM}))
\label{eq:variance_sk}
\end{equation}
The expressions for the mean and the variance above are obtained through the conditioning operation of the Gaussian random vector in Eq.(~\ref{eq:joint_training_test}) (See Appendix). 
In practice, the Kriging mean are used as a predictor of the code output at an arbitrary input location, 
while the variance is used as a measure of error of that prediction.

% Interesting Observation
The simple Kriging model has several interesting features:
\begin{itemize}
	% Linearity
	\item The Kriging predictor given by the mean in Eq.(\ref{eq:mean_sk}) is a \emph{linear predictor}. 
				\marginpar{linear predictor}
	      In other words, the centered predictor ($m_{SK}(\mathbf{x}_o) - \mu (\mathbf{x}_o)$)  is a weighted linear combination of the centered data 
	      ($\mathbf{y} - \mu(\mathbf{DM})$).
				The weights depends on the correlation function $R(\circ,\circ)$, the design of training points $\mathbf{DM}$, and the distance between the test point and the training points.
	% Interpolant
	\item The variance collapses at the training points, that is plugging-in $\mathbf{x}_i \in DM$ into Eq.(\ref{eq:variance_sk}) will yield $s^2_{SK}(\mathbf{x}_i) = 0$.
				\marginpar{Kriging as an interpolant}
	      As such, the Kriging predictor is also an \emph{interpolant}, which exactly fits the observed data (i.e., deterministic code output at the training inputs).
				See Fig.~\ref{fig:plot_bayesian_perspective_3}.
	% Dependence of the variance
	\item The variance on a given test point does not depend on the observed data.
				\marginpar{Variance as function of distance between test and training points}
	      Strictly speaking, it is only dependent on the process variance $\sigma^2$ and the correlation function $R(\circ,\circ)$.
				Furthermore, the variance on a given test point is also equal or less than the process variance, 
				the difference of which depends on the distance between $\mathbf{x}_o$ and the training points $\mathbf{DM}$.
				The closer $\mathbf{x}_o$ is to the training points, the smaller the variance at that point.
				See the difference between two red points in Fig.~\ref{fig:plot_bayesian_perspective_3} in relation to their relative position to the data.
	% Epistemic Uncertainty interpretation
	\item Being the variance of a conditional Gaussian distribution, the Kriging variance can be intuitively interpreted as the posterior \emph{uncertainty} of the prediction given the observed data.
				\marginpar{Variance as measure of epistemic uncertainty}
	      The nature of this uncertainty is epistemic as, in the case of this thesis, the computer code that underlies the observed data is deterministic.
				In other words, the uncertainty associated with the prediction at an arbitrary input is due to the lack of knowledge because the code itself is not run at that point.
				However, the prediction at that point is informed by the observed data as contained in the training data.
\end{itemize}

% Ordinary and Universal Kriging
Adding a general polynomial trend term with unknown coefficients results in a Universal Kriging model.

% Some thought

% Model selection and model fitting
Both Simple and Universal Kriging models assume that the correlation function has been selected and its hyper-parameters are fully known.
\marginpar{model selection, model fitting}
In most practical situations, there are different choices of correlation functions (such as the ones discussed before) to choose from.
Its hyper-parameters are also not known a priori and have to be estimated from a set of observations.
These two problems, \emph{model selection} and \emph{model fitting}, will be discussed in the next section. 
