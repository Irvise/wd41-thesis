\section{Gaussian Process Metamodel and its Construction}\label{sec:gp_metamodeling}

% Kriging Model, Drift and Bias
To formalize the use of in the meta-modeling of computer simulation, 
consider once again a computer code represented as 

The output of code at an arbitrary inputs, $\mathbf{x_o}$, is then modeled using a predictor of the following form
\begin{equation}
	Y (\mathbf{x_o}) = \mu (\mathbf{x_o}) + Z (\mathbf{x_o})
\label{eq:kriging_model}
\end{equation}
The equation above, also known as the \emph{Kriging} model (cite Owen, Simpson, Dupuy)\footnote{in fact, the term Kriging model will be used interchangeably as Gaussian process metamodel in this thesis} , consists of two components:
\begin{itemize}
	\item The \emph{mean/drift/trend} term, a deterministic function
	\begin{equation}
		\mu: \mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}
	\label{eq:trend_function_mapping}
	\end{equation}
	The trend term, in turn, usually consists of general linear model of polynomials (cite Owen and Dupuy, Marrel).
	\begin{equation}
		\mu(\mathbf{x}) = \sum_{j=0}^{k} \beta_j h_j(\mathbf{x})
	\label{eq:trend_function_definition}
	\end{equation}
	The trend term captures the global input output space which has an equivalent interpretation to that of a linear regression model\footnote{$Y() = \mu(\mathbf{x} + \epsilon$ with $\epsilon$ is Normally independent and identically distributed measurement error term}.
	\item The \emph{bias} term, a stochastic process. 
	This term, in turn, usually modeled using zero mean, stationary Gaussian stochastic process (such as presented in Section~\ref{sub:gp_covariance}).
	\begin{equation}
		\mathcal{Z}(\mathbf{x}) \sim \mathcal{GP}(0, R(\mathbf{x},\mathbf{x}^*))
	\label{eq:stationary_gp}
	\end{equation}
	The bias term captures the local variation of the output space such that it pulls the predictor.
	The term local here is referring to the neighborhood of training data.
	In linear regression model the error between observation and prediction is due to experimental error.
	This, in essence, what distinguishes kriging model to ordinary least square.
	Furthermore, by using stationary Gaussian process it is also assumed that the bias term will be a function of distance between design points and test point. 
\end{itemize}

% Hyper-parameters
Additionally, according to the above formulation, a \gls[hyper=false]{gp} metamodel contains several parameters called the \emph{hyper-parameters}.
\marginpar{hyper-parameters}
This term is used to distinguish them from the parameter associated with the original simulation model which is referred to as the model parameter or often simply as the parameter.
The hyper-parameters of a \gls[hyper=false]{gp} metamodel are the ones associated with the chosen trend function (Eq.~\ref{eq:trend_function_definition});
the ones associated with select correlation functions (Section~\ref{sub:gp_covariance}); and the one associated with the process variance $\sigma^2$.
The total number of hyper-parameters depends on the number of simulation model parameters as well as the select structure of mean and correlation functions.
For instance, for a $D$-parameter simulation model represented using \gls[hyper=false]{gp} metamodel with a constant mean and Gaussian correlation functions (Eq.~(\ref{eq:gaussian_kernel})), 
the total number of hyper-parameters $\boldsymbol{\Psi} = (\mu, \sigma^2, \boldsymbol{\theta})$ is $D + 2$.
On the other hand, the same model represented using a \gls[hyper=false]{gp} metamodel with linear first-order mean and power-exponential functions (Eq.~(\ref{eq:powexp_kernel})),
the total number of the hyper-para\-meters $\boldsymbol{\Psi} = (\boldsymbol{\beta}, \sigma^2, \boldsymbol{\theta}, \mathbf{p})$ is $3D + 2$.

% Three classes of Kriging Model
Two classes of Kriging models can be distinguished depending on what is known about the hyper-parameters: 
\emph{Simple Kriging} and \emph{Universal Kriging}.

\subsection{Simple Kriging}\label{sub:gp_sk}

% Simple Kriging
Simple Kriging is the simplest case of the Kriging models where all the hyper-parameters involved are assumed to be known.
\marginpar{simple Kriging}
In that case the estimation of code output at an arbitrary input location becomes straightforward.
Following the formulation above, 
a \gls[hyper=false]{gp} metamodel, then by definition implies that the computer code outputs at every input locations are jointly Gaussian.
As such, the code outputs at the training inputs $\mathbf{DM} = \{\mathbf{x}_i\}_{i=1}^N, Y(\mathbf{DM}) = (Y(\mathbf{x}_1), Y(\mathbf{x}_2), \cdots, Y(\mathbf{x}_N))$
and the output at an arbitrary input $\mathbf{x}_o$, $Y(\mathbf{x}_o)$ are distributed jointly as an $N+1$-dimensional Gaussian,
\begin{equation}
	\begin{bmatrix}
			Y(\mathbf{DM}) \\
			Y(\mathbf{x}_o)
		\end{bmatrix} \sim \mathcal{N} \left(
			\begin{bmatrix}
				\mu(\mathbf{DM}) \\
				\mu(\mathbf{x}_o)
			\end{bmatrix}, \sigma^2
			\begin{bmatrix}
				R(\mathbf{DM}, \mathbf{DM})  & R(\mathbf{DM}, \mathbf{x}_o) \\
				R(\mathbf{x}_o, \mathbf{DM}) & R(\mathbf{x}_o, \mathbf{x}_o)
			\end{bmatrix} \right)
\label{eq:joint_training_test}
\end{equation}
where
\begin{itemize}
	\item $\mu(\mathbf{DM})$ is the vector of mean at the training points;
		\begin{equation}
			\mu(\mathbf{DM}) = (\mu(\mathbf{x}_1), \mu(\mathbf{x}_2), \cdots, \mu(\mathbf{x}_N)) 
		\label{eq:training_mean_vector}
		\end{equation}
	\item $\mu(\mathbf{x}_o)$ is the mean at an arbitrary test location;
	\item $R(\mathbf{DM}, \mathbf{DM})$ is the $N \times N$ correlation matrix between output at the training points
		\begin{equation}
			R(\mathbf{DM}, \mathbf{DM}) = 
				\begin{bmatrix}
					R(\mathbf{x}_1, \mathbf{x}_1) & \cdots												& R(\mathbf{x}_1, \mathbf{x}_N) \\
					\vdots												& \ddots												&	\vdots \\
					R(\mathbf{x}_N, \mathbf{x}_1)	&	R(\mathbf{x}_N, \mathbf{x}_2) & R(\mathbf{x}_N, \mathbf{x}_N)
				\end{bmatrix}
		\label{eq:training_correlation_matrix}
		\end{equation}
	\item $R(\mathbf{DM}, \mathbf{x}_o) = R(\mathbf{x}_o, \mathbf{DM})$ is the $N \times 1$ vector of correlation between outputs at the training points and the output at the test point
			\begin{equation}
				R(\mathbf{x}_o, \mathbf{DM}) = R(\mathbf{DM}, \mathbf{x}_o) =  
					\begin{bmatrix}
						R(\mathbf{x}_o, \mathbf{x}_1) \\
						\vdots												\\
						R(\mathbf{x}_o, \mathbf{x}_N)	
					\end{bmatrix}
			\label{eq:training_test_correlation}
			\end{equation}
		\item and $R(\mathbf{x}_o, \mathbf{x}_o)$ is the correlation of the output at the test input with itself. By definition this correlation is equal to $1$.
\end{itemize}

% Conditional Distribution, Simple Kriging
Provided that the outputs at the training inputs are fully observed (i.e., the code is actually run at those inputs),
then the output at the test input $Y(\mathbf{x}_o)$ \emph{given	} the observed outputs $Y(\mathbf{DM}) = \mathbf{y} = (y_1, y_2, \cdots,$ 
$y_N)^T$ is a conditional Gaussian random variable,
\begin{equation}
	Y(\mathbf{x}_o) | Y(\mathbf{DM}) = \{y_i\}_{i=1}^N \sim \mathcal{N} \left( m_{SK}(\mathbf{x}_o), s^2_{SK}(\mathbf{x})\right)
\label{eq:joint_training_test}
\end{equation}
where $\mu_{SK}$ and $s^2_{SK}$ are the mean and the variance of the distribution, respectively.
They are also often referred to as the \emph{simple Kriging mean} and \emph{simple Kriging variance}, respectively.

% Kriging Predictor, the mean
The simple Kriging mean (or the \emph{Kriging predictor}) is expressed as follow
\marginpar{(simple) Kriging mean, \\ Kriging predictor}
\begin{equation}
	m_{SK} (\mathbf{x}_o) = \mu (\mathbf{x}_o) + R^T(\mathbf{x}_o, \mathbf{DM}) R^{-1}(\mathbf{DM}, \mathbf{DM}) (\mathbf{y} - \mu(\mathbf{DM}))
\label{eq:mean_sk}
\end{equation}
% Kriging Variance
The simple Kriging variance, on the other hand, is expressed as
\marginpar{(simple) Kriging variance}
\begin{equation}
	s^2_{SK} (\mathbf{x}_o) = \sigma^2 (1 - R^T(\mathbf{x}_o, \mathbf{DM}) R^T(\mathbf{DM}, \mathbf{DM}) R(\mathbf{x}_o, \mathbf{DM}))
\label{eq:variance_sk}
\end{equation}
The expressions for the mean and the variance above are obtained through the conditioning operation of the Gaussian random vector in Eq.(~\ref{eq:joint_training_test}) (See Appendix). 
In practice, the Kriging mean are used as a predictor of the code output at an arbitrary input location, 
while the variance is used as a measure of error of that prediction.

% Interesting Observation
The simple Kriging model has several interesting features:
\begin{itemize}
	% Linearity
	\item The Kriging predictor given by the mean in Eq.(\ref{eq:mean_sk}) is a \emph{linear predictor}. 
	      In other words, the centered predictor $m_{SK}(\mathbf{x}_o) - \mu (\mathbf{x}_o))$)  is a linear combination of the centered data 
	      ($\mathbf{y} - \mu(\mathbf{DM})$).
	\item The variance collapses at the training points, that is plugging-in $\mathbf{x}_i \in DM$ into Eq.(\ref{eq:variance_sk}) will yield $s^2_{SK}(\mathbf{x}_i) = 0$.
	      As such Kriging predictor is also an \emph{interpolant}, which exactly fits the observed data (i.e., deterministic code output at the training inputs).
	\item The variance on a given test point does not depend on the observed data. 
	      Strictly speaking, it is only dependent on the process variance $\sigma^2$ and the correlation function $R(\circ,\circ)$.
\end{itemize}


% Epistemic Uncertainty interpretation

\subsection{Universal Kriging and Hyper-Parameter Estimation}\label{sub:gp_uk}

However, in most practical situation, the values of the hyper-parameters are not known a priori.
This is indeed the case for \emph{Universal Kriging} model where hyper-parameters values associated both with the mean and covariance functions are to be estimated simultaneously from the training data.

% Likelihood

% Profile/Concentrated Likelihood

% Maximum Likelihood Estimation / Empirical Bayes

% Fully Bayesian Approach

\subsection{Selection of Design Points}\label{sub:gp_design}

\subsection{Validation}\label{sub:gp_validation}

\subsection{Summary}\label{sub:gp_summary}