\newpage
%**************************************************************
\section{Gaussian Process Metamodel}\label{sec:gp_metamodeling}
%**************************************************************

% Kriging Model, Drift and Bias
To formalize the use of \gls[hyper=false]{gp} in the metamodeling of computer simulation, 
consider once again a represented as 

The output of code at an arbitrary inputs, $\mathbf{x_o}$, is then modeled using a predictor of the following form:
\begin{equation}
	\mathcal{Y} (\mathbf{x_o}) = \mu (\mathbf{x_o}) + \mathcal{Z} (\mathbf{x_o})
\label{eq:kriging_model}
\end{equation}
The equation above, the \emph{Kriging} model (cite Owen, Simpson, Dupuy), consists of two components:
\begin{itemize}
	\item The \emph{mean/drift/trend} term, a deterministic function.
	\begin{equation}
		\mu: \mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}
	\label{eq:trend_function_mapping}
	\end{equation}
	The choice of the trend term distinguishes different classes of Kriging model.
	\emph{Simple Kriging} (SK), refers to a class of Kriging whose arbitrary trend function is fully specified.
	\emph{Universal Kriging} (UK), on the other hand, refers to class of Kriging where where general polynomial model is assumed, but its coefficients are unknown \cite{Koehler1996,Ginsbourger2009,Marrel2008},
	\begin{equation}
		\mu(\mathbf{x}) = \sum_{j=0}^{k} \beta_j h_j(\mathbf{x})
	\label{eq:trend_function_definition}
	\end{equation}
	where $h_j$ is a set of basis function; and $\beta_j$ is the set of the associated coefficients.
	\emph{Ordinary Kriging} is a special case of UK where the trend is set as an unknown constant. 
	\item The \emph{bias} or \emph{residual} term, a stochastic process. 
		This term, in turn, usually modeled using zero mean, stationary Gaussian stochastic process (such as presented in Section~\ref{sub:gp_covariance}).
		\begin{equation}
			\mathcal{Z}(\mathbf{x}) \sim \mathcal{GP}(0, \sigma^2 R(\mathbf{x},\mathbf{x}^*))
		\label{eq:stationary_gp}
		\end{equation}
	The residuals, being modelled as a \gls[hyper=false]{gp}, are correlated and the correlation is a function of inputs.
	As such, a Kriging model can be thought of as a generalized linear model whose elements of the correlation matrix is specified explicitly by a parametric correlation function \cite{Martin2005}.
\end{itemize}

% Hyper-parameters
Additionally, according to the above formulation, a \gls[hyper=false]{gp} metamodel contains several parameters called the \emph{hyper-parameters}.
\marginpar{hyper-parameters}
This term is used to distinguish them from the parameter associated with the original simulation model which is referred to as the model parameter or often simply as the parameter.
The hyper-parameters of a \gls[hyper=false]{gp} metamodel are the ones associated with the chosen trend function (Eq.~\ref{eq:trend_function_definition});
the ones associated with select correlation functions (Section~\ref{sub:gp_covariance}); and the one associated with the process variance $\sigma^2$.
The total number of hyper-parameters depends on the number of simulation model parameters as well as the select structure of mean and correlation functions.
For instance, for a $D$-parameter simulation model represented using \gls[hyper=false]{gp} metamodel with a constant mean and Gaussian correlation functions (Eq.~(\ref{eq:gaussian_kernel})), 
the total number of hyper-parameters $\boldsymbol{\Psi} = (\mu, \sigma^2, \boldsymbol{\theta})$ is $D + 2$.
On the other hand, the same model represented using a \gls[hyper=false]{gp} metamodel with linear first-order mean and power-exponential functions (Eq.~(\ref{eq:powexp_kernel})),
the total number of the hyper-para\-meters $\boldsymbol{\Psi} = (\boldsymbol{\beta}, \sigma^2, \boldsymbol{\theta}, \mathbf{p})$ is $3D + 2$.

% Three classes of Kriging Model
As mentioned, two classes of Kriging models can be distinguished depending on what is specified on the trend term: 
\emph{Simple Kriging} and \emph{Universal Kriging}.

% Simple Kriging
Simple Kriging is the simpler case of the Kriging models where all the hyper-parameters involved are assumed to be known.
\marginpar{simple Kriging}
In that case the estimation of code output at an arbitrary input location becomes straightforward.
Following the formulation above, 
a \gls[hyper=false]{gp} metamodel, then by definition implies that the computer code outputs at every input locations are jointly Gaussian.
As such, the code outputs at the training inputs $\mathbf{DM} = \{\mathbf{x}_i\}_{i=1}^N, Y(\mathbf{DM}) = (Y(\mathbf{x}_1), Y(\mathbf{x}_2), \cdots, Y(\mathbf{x}_N))$
and the output at an arbitrary input $\mathbf{x}_o$, $Y(\mathbf{x}_o)$ are distributed jointly as an $N+1$-dimensional Gaussian,
\begin{equation}
	\begin{bmatrix}
			Y(\mathbf{DM}) \\
			Y(\mathbf{x}_o)
		\end{bmatrix} \sim \mathcal{N} \left(
			\begin{bmatrix}
				\mu(\mathbf{DM}) \\
				\mu(\mathbf{x}_o)
			\end{bmatrix}, \sigma^2
			\begin{bmatrix}
				R(\mathbf{DM}, \mathbf{DM})  & R(\mathbf{DM}, \mathbf{x}_o) \\
				R(\mathbf{x}_o, \mathbf{DM}) & R(\mathbf{x}_o, \mathbf{x}_o)
			\end{bmatrix} \right)
\label{eq:joint_training_test}
\end{equation}
where
\begin{itemize}
	\item $\mu(\mathbf{DM})$ is the vector of mean at the training points;
		\begin{equation}
			\mu(\mathbf{DM}) = (\mu(\mathbf{x}_1), \mu(\mathbf{x}_2), \cdots, \mu(\mathbf{x}_N)) 
		\label{eq:training_mean_vector}
		\end{equation}
	\item $\mu(\mathbf{x}_o)$ is the mean at an arbitrary test location;
	\item $R(\mathbf{DM}, \mathbf{DM})$ is the $N \times N$ correlation matrix between outputs at the training points
		\begin{equation}
			R(\mathbf{DM}, \mathbf{DM}) = 
				\begin{bmatrix}
					R(\mathbf{x}_1, \mathbf{x}_1) & \cdots												& R(\mathbf{x}_1, \mathbf{x}_N) \\
					\vdots												& \ddots												&	\vdots \\
					R(\mathbf{x}_N, \mathbf{x}_1)	&	R(\mathbf{x}_N, \mathbf{x}_2) & R(\mathbf{x}_N, \mathbf{x}_N)
				\end{bmatrix}
		\label{eq:training_correlation_matrix}
		\end{equation}
	\item $R(\mathbf{DM}, \mathbf{x}_o) = R(\mathbf{x}_o, \mathbf{DM})$ is the $N \times 1$ vector of correlation between outputs at the training points and the output at the test point
			\begin{equation}
				R(\mathbf{x}_o, \mathbf{DM}) = R(\mathbf{DM}, \mathbf{x}_o) =  
					\begin{bmatrix}
						R(\mathbf{x}_o, \mathbf{x}_1) \\
						\vdots												\\
						R(\mathbf{x}_o, \mathbf{x}_N)	
					\end{bmatrix}
			\label{eq:training_test_correlation}
			\end{equation}
		\item and $R(\mathbf{x}_o, \mathbf{x}_o)$ is the correlation of the output at the test input with itself. By definition this correlation is equal to $1$.
\end{itemize}

% Conditional Distribution, Simple Kriging
Provided that the outputs at the training inputs are fully observed (i.e., the code is actually run at those inputs),
then the output at the test input $Y(\mathbf{x}_o)$ \emph{given	} the observed outputs $Y(\mathbf{DM}) = \mathbf{y} = (y_1, y_2, \cdots,$ 
$y_N)^T$ is a conditional Gaussian random variable,
\begin{equation}
	Y(\mathbf{x}_o) | Y(\mathbf{DM}) = \{y_i\}_{i=1}^N \sim \mathcal{N} \left( m_{SK}(\mathbf{x}_o), s^2_{SK}(\mathbf{x})\right)
\label{eq:joint_training_test}
\end{equation}
where $\mu_{SK}$ and $s^2_{SK}$ are the mean and the variance of the distribution, respectively.
They are also often referred to as the \emph{simple Kriging mean} and \emph{simple Kriging variance}, respectively.

% Kriging Predictor, the mean
The simple Kriging mean (or the \emph{Kriging predictor}) is expressed as follow
\marginpar{(simple) Kriging mean}
\begin{equation}
	m_{SK} (\mathbf{x}_o) = \mu (\mathbf{x}_o) + R^T(\mathbf{x}_o, \mathbf{DM}) R^{-1}(\mathbf{DM}, \mathbf{DM}) (\mathbf{y} - \mu(\mathbf{DM}))
\label{eq:mean_sk}
\end{equation}
% Kriging Variance
The simple Kriging variance, on the other hand, is expressed as
\marginpar{(simple) Kriging variance}
\begin{equation}
	s^2_{SK} (\mathbf{x}_o) = \sigma^2 (1 - R^T(\mathbf{x}_o, \mathbf{DM}) R^T(\mathbf{DM}, \mathbf{DM}) R(\mathbf{x}_o, \mathbf{DM}))
\label{eq:variance_sk}
\end{equation}
The expressions for the mean and the variance above are obtained through the conditioning operation of the Gaussian random vector in Eq.(~\ref{eq:joint_training_test}) (See Appendix). 
In practice, the Kriging mean are used as a predictor of the code output at an arbitrary input location, 
while the variance is used as a measure of error of that prediction.

% Interesting Observation
The simple Kriging model has several interesting features:
\begin{itemize}
	% Linearity
	\item The Kriging predictor given by the mean in Eq.(\ref{eq:mean_sk}) is a \emph{linear predictor}. 
				\marginpar{linear predictor}
	      In other words, the centered predictor ($m_{SK}(\mathbf{x}_o) - \mu (\mathbf{x}_o)$)  is a weighted linear combination of the centered data 
	      ($\mathbf{y} - \mu(\mathbf{DM})$).
				The weights depends on the correlation function $R(\circ,\circ)$, the design of training points $\mathbf{DM}$, and the distance between the test point and the training points.
	% Interpolant
	\item The variance collapses at the training points, that is plugging-in $\mathbf{x}_i \in DM$ into Eq.(\ref{eq:variance_sk}) will yield $s^2_{SK}(\mathbf{x}_i) = 0$.
				\marginpar{Kriging as an interpolant}
	      As such, the Kriging predictor is also an \emph{interpolant}, which exactly fits the observed data (i.e., deterministic code output at the training inputs).
				See Fig.~\ref{fig:plot_bayesian_perspective_3}.
	% Dependence of the variance
	\item The variance on a given test point does not depend on the observed data.
				\marginpar{Variance as function of distance between test and training points}
	      Strictly speaking, it is only dependent on the process variance $\sigma^2$ and the correlation function $R(\circ,\circ)$.
				Furthermore, the variance on a given test point is also equal or less than the process variance, 
				the difference of which depends on the distance between $\mathbf{x}_o$ and the training points $\mathbf{DM}$.
				The closer $\mathbf{x}_o$ is to the training points, the smaller the variance at that point.
				See the difference between two red points in Fig.~\ref{fig:plot_bayesian_perspective_3} in relation to their relative position to the data.
	% Epistemic Uncertainty interpretation
	\item Being the variance of a conditional Gaussian distribution, the Kriging variance can be intuitively interpreted as the posterior \emph{uncertainty} of the prediction given the observed data.
				\marginpar{Variance as measure of epistemic uncertainty}
	      The nature of this uncertainty is epistemic as, in the case of this thesis, the computer code that underlies the observed data is deterministic.
				In other words, the uncertainty associated with the prediction at an arbitrary input is due to the lack of knowledge because the code itself is not run at that point.
				However, the prediction at that point is informed by the observed data as contained in the training data.
\end{itemize}

% Ordinary and Universal Kriging
In most practical situation, however, the values of the hyper-parameters are not known a priori.
This is indeed the case for the Universal Kriging model where all the hyper-parameters values associated with both a linear mean and covariance functions are to be estimated simultaneously from the training data.

% Some thought

% Model selection and model fitting
Both Simple and Universal Kriging models assume that the correlation function has been selected and its hyper-parameters are fully known.
\marginpar{model selection, model fitting}
In most practical situations, there are different choices of correlation functions (such as the ones discussed before) to choose.
Additionally, its hyper-parameters are not known a priori and have to be estimated from a set of observations.
These two problems, \emph{model selection} and \emph{model fitting}, will be discussed in the next section. 
