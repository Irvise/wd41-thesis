%******************************************************
\section{Chapter Summary}\label{sec:gp_chapter_summary}
%******************************************************

% Introductory paragraph
The functional approximation part of the proposed statistical framework has been presented in this chapter.
The goal of such an approximation was to evaluate the output of a computer simulation code for an arbitrary input (much) faster.
The approximation is based on Gaussian stochastic process resulting in a statistical metamodel.
As the dimensionality of the output is large, in the order of tens of thousands, a dimension reduction step is adopted by means of \gls[hyper=false]{pca} (an approach similar to what was adopted in Chapter 3).

% Error in Context
The results obtained on the \gls[hyper=false]{trace} model \gls[hyper=false]{feba} is reasonable.
Though the prediction error can at times be large, the metamodel gives an overall good performance on average for the three types of multivariate output (clad temperature, pressure drop, and liquid carryover).
The metamodels for both pressure drop and liquid carryover outputs have less than $0.9\%$ prediction error (\gls[hyper=false]{rmse}), while the metamodel for the clad temperature output has less than $9\%$ prediction error (\gls[hyper=false]{rmse}); these errors are relative to the standard deviation of the respective outputs in the testing data set.
The larger error for predicting the clad temperature output highlights the limitation of the approach for outputs that exhibit strong non-linearity and discontinuity (such as the quenching in the clad temperature transient).
This, in turn, is due to the use of \gls[hyper=false]{pca} as the (linear) dimension reduction tool.
As such, a first step of improvement in this regard can be aimed toward replacing \gls[hyper=false]{pca} with another, more advanced dimension reduction tool.

% Computational Advantages (and disadvantages)
Using the \gls[hyper=false]{gp} \gls[hyper=false]{pc} metamodel as the surrogate for \gls[hyper=false]{trace} run, 
the prediction for arbitrary model parameters values can be made much faster ($< 5 [s]$ per metamodel evaluation vs. $6-15 \, [min]$ per \gls[hyper=false]{trace} run).
As such the metamodel constructed in this chapter can be used as the basis for Bayesian model calibration which requires tens if not hundreds of thousands function evaluations. 
However, it is also important to note that the time required for the construction of the metamodel as well as for its convergence study has to be taken into account.
The training, validation, and testing data have to be gene\-rated from actual code runs.
Additionally, the model fitting step to estimate \gls[hyper=false]{gp} metamodel hyper-parameters is an optimization problem that can easily become expensive for large training sample of large dimensions (large number of input parameters).

% Recommendation
The study confirms that the size of the training data is the main factor in determining the predictive performance of the metamodel.
As a result, the size of the training data should be as large as the computational budget allowed.
At the same time, the choice of covariance function has some impact especially in relation to the stability of the performance.
Regarding this, the power-exponential and Mat\'ern covariance kernel functions are preferred,
while the Gaussian kernel should be avoided.
Finally,
the choice of experimental design has a negligible impact on the predictive performance of the metamodel.
