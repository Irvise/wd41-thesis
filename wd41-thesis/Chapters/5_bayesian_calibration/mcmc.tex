\newpage
%************************************************************************************
\section[MCMC Simulation]{\glsfirst[hyper=false]{mcmc} Simulation}\label{sec:bc_mcmc}
%************************************************************************************

% Introductory Paragraph
The formulation of Bayesian calibration of a computer model presented above results in a joint posterior \gls[hyper=false]{pdf} for all the parameters involved in the model.
\marginpar{Posterior uncertainty of the model parameters}
This density contains all the information (and consequently, the uncertainties) regarding the model parameters conditioned on the observed data and the assumed data-generating process.
The uncertainties associated with the model parameters can then be represented using different summary statistics, many of which involve integration.

For example,
the uncertainties associated with a model parameter $x_d$ can be represented by its variance,
which is defined as
\begin{equation*}
	\begin{split}
	\mathbb{V}[\mathcal{X}_d] & = \mathbb{E}[\mathcal{X}_d^2] - \mathbb{E}^2[\mathcal{X}_d]\\
	                          & = \int_{-\infty}^{\infty} x_d^2 p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} x_d p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
	\end{split}
%\label{eq:variance_marginalization}
\end{equation*}
where the integrations are carried out over the support of $p(\bm{x}|\mathbf{y})$, assumed to be on the entire real line.
An alternative way to summarize the uncertainties of a model parameter is through its $\theta$-quantile $Q^{\theta}_d$,
which for parameter $x_d$ is defined as
\begin{equation*}
	Q^{\theta}_d \, : \, \mathbb{P} (\mathcal{X}_d \leq Q^{\theta}_d) = \int^{Q^{\theta}_d}_{-\infty} \int\limits_{\mathbf{X}_{\sim d}} p(x_d, \bm{x}_{\sim d} | \mathbf{y}) d\bm{x}_{\sim d} d x_d = \theta
%	\label{eq:quantile_integral}
\end{equation*}
Assuming that the support of $p(\bm{x}|\mathbf{y})$ is on the entire real line.
In this manner,
the $95\%$ confidence interval of the parameter is written as $Q_d^{0.025} \leq \mathcal{X}_d \leq Q_d^{0.975}$.

Though these summaries might be of interest themselves,
\marginpar{Posterior uncertainty of the model prediction}
in an application setting, the model parameters uncertainties are often propagated through the simulation model to obtain the uncertainty in the output (prediction).
Hence, the output from a simulation model $y = f(\bm{x})$ is expressed as random variable $\mathcal{Y}$ from a transformation of random variable $\bm{\mathcal{X}}|\mathbf{y}$ by the function $f$
\begin{equation*}
	\mathcal{Y} = f(\bm{\mathcal{X}} | \mathbf{y})  \, ; \, p_{\bm{\mathcal{X}} | \mathbf{y}} (\bm{x}) = p(\bm{x}| \mathbf{y})  
\end{equation*}
where the \gls[hyper=false]{pdf} of $\bm{\mathcal{X}} | \mathbf{y}$ is the posterior density $p(\bm{x}| \mathbf{y})$.
The actual \gls[hyper=false]{pdf} of $\mathcal{Y}$ follows the rule of transformation of random variable
and it represents the uncertainty in the output due to the uncertainty in the input parameter conditioned on the data.
This uncertainty can also be summarized with various statistics and, as before, many of which involve integration operation.
For instance, the variance of the output, is written as
\begin{equation*}
	\mathbb{V}[\mathcal{Y}] = \int_{-\infty}^{\infty} f^2(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} f(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
\end{equation*}

The posterior density $p(\bm{x}|\mathbf{y})$ and the function $f(\bm{x})$, however, are in practice highly multidimensional functions and 
the performance of numerical integration is typically worsened with an increasing number of dimensions of the input parameter space.
\marginpar{Challenges in dealing with posterior density}
At the same time,
conducting \gls[hyper=false]{mc} simulation for estimating the integrals (such was done in Chapter~\ref{ch:gsa}) is not straightforward in this case.
The multiplication of likelihood and prior density will, in general, yield an arbitrary posterior density not available in a closed-form expression.
As a result, generating independent samples from the posterior density required for the \gls[hyper=false]{mc} estimation becomes a difficult task.

This section presents a simulation approach, the so-called \gls[hyper=false]{mcmc} simulation,
to directly generate samples from an arbitrary \gls[hyper=false]{pdf}. 
These samples, in turn, are useful for estimating various quantities given as examples above independent on the dimension of the input parameter space.

Although in the context of Bayesian data analysis the \gls[hyper=false]{pdf} of interest is the posterior \gls[hyper=false]{pdf} \cite{Tierney1994},
the problem of generating samples from an arbitrary \gls[hyper=false]{pdf} is general.
As such, in the following discussion, a generic notation for an arbitrary \gls[hyper=false]{pdf} $p(\bm{x})$ is used instead of $p(\bm{x}|\mathbf{y})$.

%----------------------------------------------------
\subsection{Motivation}\label{sub:bc_mcmc_motivation}
%----------------------------------------------------

% Introductory Paragraph
Consider the following problem: Given a random variable $\bm{\mathcal{X}}$ equipped with the \gls[hyper=false]{pdf} $p:\mathbf{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}_{\geq 0}$,
generate a set of samples $\{\bm{x}_n\}_{n=1}^N$ from the \gls[hyper=false]{pdf}.
\marginpar{Problem statement}
It is assumed that the \gls[hyper=false]{pdf} can be evaluated at any given $\bm{x} \in \mathbf{X}$, at least up to a proportionality constant:
\begin{equation}
	p(\bm{x}) = \frac{p^*(\bm{x})}{C} \propto p^*(\bm{x})  
\label{eq:bc_prop_post}
\end{equation}
The proportionality constant in the above equation is the normalizing constant such that $p$ is a valid \gls[hyper=false]{pdf},
\begin{equation}
	C = \int p^*(\bm{x}) d\bm{x}  \Rightarrow \int p(\bm{x}) d\bm{x} = 1.0
\label{eq:bc_prop_const}
\end{equation}
Such samples can then be used, among other things,
to evaluate different summary statistics (such as expectation, variance, etc.) of $\bm{x}$ itself or
of any function under the \gls[hyper=false]{pdf}.
Note that in the rest of the section, the term model input parameter space $\mathbf{X}$ is replaced by the term \emph{state space}, the range of possible values of the random variable $\bm{\mathcal{X}}$;
A more appropriate term in the context of generic problem of generating samples from a distribution.

% Why it it difficult to samples
Generating samples from an arbitrary multidimensional density function is generally a difficult task.
\marginpar{A correct sampling}
Intuitively, for a given sample size, correctly generating samples from a density means
that the sample values have to be distributed proportional to its \gls[hyper=false]{pdf}.
There should be more samples in the region where the \gls[hyper=false]{pdf} value is high,
and less in the the region where the \gls[hyper=false]{pdf} value is low.
For a complex multidimensional density function,
these locations are not known a priori and might have to be identified exhaustively \cite{Mackay2005}.

% Inverse Transform Sampling and Its Problem
In one dimension,
the most common way of generating sample from a given density is
by inverse transform sampling coupled with a random number generator.
\marginpar{Inverse transform sampling}
The approach requires the quantile function of the \gls[hyper=false]{pdf}.
To obtain the quantile function,
the density has to be integrated and its normalizing constant has to be computed.
Appendix~\ref{app:its} provides a more detail account on the topic.
Many univariate random variables are widely studied and the analytical solutions to their quantile functions are available \cite{Lange2010}.
However, the method is not extendable to distributions of higher dimension.
Additionally, though sampling algorithms exist for several multivariate densities (notably, the multivariate normal density in Appendix~\ref{app:mvn_sampling}),
this will not be the case for an arbitrary density function of higher dimension.

% Illustration
To illustrate this point,
\marginpar{Illustration}
consider the following bivariate (unnormalized) density parameterized by location parameters $\mu_1, \mu_2$ and scale parameters $\sigma_1, \sigma_2$ \cite{Balakrishnan2014}:
\begin{equation}
	\begin{split}
	& p^*(x_1, x_2) = \frac{\exp{(-(x_1 - \mu_1)/\sigma_1)} \exp{(-(x_2 - \mu_2)/\sigma_2)}}{(1 + \exp{(-(x_1 - \mu_1)/\sigma_1)} + \exp{(-(x_2 - \mu_2)/\sigma_2)})^3} \, \\ 
	& x_1, x_2 \in \mathbb{R}; \mu_1, \mu_2 \in \mathbb{R};\, \text{and} \, \sigma_1, \sigma_2 \in \mathbb{R}^+
	\end{split}
\label{eq:bc_unnormalized_gumbel}
\end{equation}
Fig.~\ref{fig:ch5_plot_gumbel_illustration} shows the contour plot of the joint density as well as the marginal density for each of the variate.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_gumbel_illustration},
                  maincaption={Joint and marginal densities plots of the unnormalized \gls[hyper=false]{pdf} in the example. The parameters used in the example are: $\mu_1 = 5, \mu_2 = 2, \sigma_1 = 1.25$, and $\sigma_2 = 3$.},%
									mainshortcaption={Joint and marginal densities plots for the unnormalized \gls[hyper=false]{pdf} in the example.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_gumbel_illustration_1},
                  leftcaption={Joint density},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_gumbel_illustration_2},
                  rightcaption={Marginal density},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotGumbelIllustration_1}
{../figures/chapter5/figures/plotGumbelIllustration_2}

% Grid Approach
A straightforward approach to generate samples from a given multivariate density is done by first 
\marginpar{Discretized grid approach}
discretizing the state space of the density function and evaluate the density at the discretized points.
Supposed the domain of the density has been discretized uniformly in each dimension with a level $\Delta$ resulting in $\{\bm{x}_i\}_{i=1}^{I}$ with $I$ the number of discretized points.
At the discretized levels, the probability for each value of $\bm{x}_i$ is approximated by $p(\bm{x}_i) = p^*(\bm{x}_i) / \sum_i p^*(\bm{x}_i)$\footnote{strictly speaking, each density value has to be multiplied by the hypervolume of the grid to obtain the probability mass, but the term cancels out in computing $p$.)}.
The pairs constitute a complete discrete probability distributions.
Generating samples from such a probability distribution is straightforward in a modern computing environment \cite{Mackay2005}.

% The example
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid} illustrates this procedure for the example given above.
First, the state is windowed in $\mathbf{X}\in[-25,25]^2$ before being discretized in $\Delta = 50$ levels.
\marginpar{Discretized grid approach illustrated}
This results in $2'601$ discretized points at which the density is evaluated (Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1}).
Next, the density values are taken to be the probability for each of the $2'601$ discretized points.
Together they make up a complete discrete probability distribution from which samples can be readily generated.

% The figure explained
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1} shows $5'000$ samples generated from the discrete distribution.
Darker points indicate that the values have been sampled multiple times following the actual underlying \gls[hyper=false]{pdf} (the contour of the analytical joint density is overlaid). 
Figs.~\ref{fig:ch5_plot_gumbel_sample_grid_2} and~\ref{fig:ch5_plot_gumbel_sample_grid_3} show the histograms for each of the marginals.
The figure shows that the generated samples are indeed approximately distributed as the given \gls[hyper=false]{pdf}.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_gumbel_sample_grid},
			           maincaption={Sampling from a multivariate density by discretizing the state space in grids. The state space is discretized into $\Delta = 50$ levels. The density is then evaluated at the discretized points. (Left) $5'000$ samples are generated following the resulting discrete probability distribution; (Center and Right) The histograms of the marginals approximately follow the shape of the respective analytical marginal density. The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Sampling from a multivariate density by discretizing the state space in grids.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_gumbel_sample_grid_1},
			           leftcaption={Joint samples},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_gumbel_sample_grid_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_gumbel_sample_grid_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotGumbelSampleGrid_1}
{../figures/chapter5/figures/plotGumbelSampleGrid_2}
{../figures/chapter5/figures/plotGumbelSampleGrid_3}

% Problem with Discretized Grid, curse of dimensionality
The main issue with the discretized grid approach, conceptually simple as it is,
is the curse of dimensionality similar to the one mentioned in the previous chapters.
\marginpar{curse of dimensionality}
The number of density evaluations grows exponentially with the number of dimension.
As a rule, for a given discretization level $\Delta$ and dimension $D$,
the number of density evaluations is $(\Delta+1)^D$.

% Problem with Discretized Grid, typical set
Moreover, many of the evaluations on the grid exemplified above are potentially wasteful for carrying out an integration over the density.
Assuming a well-behaved function of interest inside the integral, some regions of the state space will contribute more to the integration than the others.
\marginpar{Integration over a density, typical set}
In fact, this ``region of space where it matters'' is related to the mathematical notion of the \emph{typical set} of a distribution \cite{Mackay2005}.
Loosely speaking, it can be thought of as the region of state space where the probability mass (density times volume) is concentrated.
Consequently, any integration over the whole state space of a density can be approximated by an integration over this typical set \cite{Mackay2005}.
Although there is pretty good idea where this typical set is located for the example above (the region around the center of the density),
in an arbitrary high-dimensional \gls[hyper=false]{pdf} (such as a posterior density) there will not be\footnote{And in fact, the region around the mode becomes less typical in high-dimension. Moving away from the mode of the density reduce the density value while increasing the amount of volume.
In a high-dimensional case, the region of space around the mode contributes less and less mass in comparison to other part of the state space and thus becoming less typical. This is yet another example of the curse of dimensionality previously mentioned.}.
Having samples that is representative of a typical set is a particularly challenging task in conducting \gls[hyper=false]{mc} integration over an arbitrary high-dimensional \gls[hyper=false]{pdf}\footnote{Recall that in Chapter~\ref{ch:gsa} the \gls[hyper=false]{mc} integrations for Sobol' indices estimation were conducted over a uniform density thus it was only the property of the function of interest that mattered.}. 

% In relation to Bayesian data analysis
%In the context of Bayesian data analysis,
%the multidimensional likelihood function inside the posterior generally can be a complex function.
%In consequence, a very fine discretization level might be required to appropriately capture the function %behavior at important regions which are unknown a priori.
%On top of that, the computational cost of evaluating the (complex) posterior density becomes nonnegligible.
%As such, except for a very simple likelihood function and/or in a very low dimension,
%grid approach is further deemed inapplicable for generating samples from an arbitrary multidimensional %density.

% Entry to Markov Chain
To circumvent these issues,
a sampling technique based on the theory of stochastic process is widely adopted.
\marginpar{Markov Chain Monte Carlo}
Specifically, by constructing a Markov chain of the input parameters values,
the resulting process will eventually converge to a stationary distribution which coincides with the distribution according to the given density (i.e., \emph{target density}).
Instead of blindly evaluating the density at every corner of the state space, such a Markov chain will be directed to explore the typical set of the distribution.
Generating such samples for the purpose of \glsfirst[hyper=false]{mc} simulation by simulating a Markov chain is termed \glsfirst[hyper=false]{mcmc}.
%Theoretically, this family of techniques would have less severe dependence to the dimension of the state space and its convergence is guaranteed.

The following briefly presents the basics of Markov chain and its importance in solving the the problem of generating samples from an arbitrary density.
Markov chain (in continuous state space) as well as some important related concepts and theorems are first presented, without proof and in somewhat lax manner.
A more precise statements of these concepts and theorems are difficult without measure theory which for an application purpose of this thesis is irrelevant. 
Appendix~\ref{app:markov_chain} provides the definitions and illustrations for some of these concepts for a discrete state Markov chain where more intuitive matrix notation and graphical representation are applicable.
Finally,
two methods to construct a Markov chain for the purpose of \gls[hyper=false]{mc} simulation are introduced and illustrated.

%----------------------------------------------
\subsection{Markov Chain}\label{sub:bc_mcmc_mc}
%----------------------------------------------
%once more the posterior \gls[hyper=false]{pdf} of the model parameters conditioned on the observed data.
%Strictly speaking such a \gls[hyper=false]{pdf} will also be conditioned on a selected data-generating process model $\mathcal{M}$, i.e., $p(\bm{x}|\mathbf{y},\mathcal{M})$.
%However, in the following discussion, this conditioning is implicitly assumed and removed from the notation yielding
%\begin{equation}
%	p(\bm{x} | \mathbf{y}) = \frac{p(\bm{y} = \mathbf{y} | \bm{x}) p(\bm{x})}{\int p(\mathbf{y} | \bm{x}) p(\bm{x}) d\bm{x}}
%\label{eq:pdf_posterior}
%\end{equation}

% Introductory paragraph (Why Markov Chain)
Markov chain is an example of a \emph{discrete-time} stochastic process.
Recall that from Chapter~\ref{ch:gp_metamodel}, a stochastic process is a collection of random variables $\{\mathcal{X}^{(i)}; i \in I\}$ where $I$ is an index set.
\marginpar{Discrete-time stochastic process}
The term \emph{discrete-time} refers to the fact that the possible values of the index set $I$ is restricted to being discrete.
Moreover, the term \emph{time} is used by convention but by no means it is exclusively referred to the physical time.
In this thesis, a more fitting alternative term would be \emph{step} or \emph{iteration}.

% Markov Chain Definition
Specifically, a \emph{continuous-state} Markov chain on state space $\mathbf{X} \subseteq \mathbb{R}^D$, $D$ being the dimension of the state space, is defined as a sequence of random variables $\{\bm{\mathcal{X}}^{(i)}; i \geq 0\}$ where the indices represents successive time, steps, or iterations,
\emph{such that the conditional probability of $\bm{\mathcal{X}}^{(i)}$ given the previous iterations follows the Markov assumption}.
\marginpar{Markov chain}
That is,
\begin{equation}
  \begin{split}
  \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} \in \mathbf{A} & | \bm{\mathcal{X}}^{(i)} = \bm{x}^{(i)}, \ldots, \bm{\mathcal{X}}^{(0)} = \bm{x}^{(0)}) = \\
  & \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} \in \mathbf{A} | \bm{\mathcal{X}}^{(i)} = \bm{x}^{(i)}), \,\,\,\, \mathbf{A} \subseteq \mathbf{X}
  \end{split}
\label{eq:markov_property}
\end{equation}
Put differently, the present value depends on the preceding values only through the present \cite{Geyer2011,Sokal1997}.

% Specification and Ingredients, Initial Distribution and Transition Kernel
A Markov chain is fully specified by three components:
\begin{itemize}
  \item The \emph{state space} $\mathbf{X} \subseteq \mathbb{R}^D$, the set of values in which the random variables $\bm{\mathcal{X}}^{(i)}$ take on.
	
  \item The \emph{initial distribution} of $\bm{\mathcal{X}}^{(0)}$, given by the density $\pi^{(0)} = \pi^{(0)}(\bm{x})$.
	
	\item The \emph{transition probability kernel (density)} $T(\bm{x}, \bm{x}')$ a function $T: \mathbf{X} \times \mathbf{X} \mapsto \mathbb{R}$.
    For any given $\bm{x}$, $T(\bm{x},\circ)$ defines the conditional probability density of $\bm{\mathcal{X}}^{(i)}$ given $\bm{\mathcal{X}}^{(i-1)} = \bm{x}$ with the following properties:
  \begin{equation}
    \begin{split}
      & T(\bm{x}, \bm{x}') \geq 0, \,\,\, \forall x,y \in \mathbf{X} \\
      & \int_{\mathbf{X}} T(\bm{x}, \bm{x}') \, d\bm{x} = 1, \,\,\, \forall \bm{x} \in \mathbf{X}\\
    \end{split}
  \label{eq:ch5_markov_chain_transition_kernel}
  \end{equation}
	As $T(\bm{x}, \circ)$ is a \gls[hyper=false]{pdf}, then for any given $\bm{x}$ it also follows that,
	\begin{equation}
    \mathbb{P}(\bm{\mathcal{X}}^{(i)} \in \mathbf{A} | \bm{\mathcal{X}}^{(i-1)} = \bm{x}) = \int_\mathbf{A} T(\bm{x}, \bm{x}') \, d\bm{x}' \, ; \, \mathbf{A} \subset \mathbf{X}
  \label{eq:markov_chain_transition_probability}
  \end{equation}

\end{itemize}

% Transition Between Iterations
The distribution of $\bm{\mathcal{X}}^{(i)}$ due to the the transition from the previous iteration $\bm{\mathcal{X}}^{(i-1)}$, are given by the transition probability kernel $T$ operated on the density function of the previous iteration $\pi^{(i-1)}$ as follows,
\begin{equation}
  \pi^{(i)}(\bm{x}') = \int_\mathbf{X} \pi^{(i-1)}(\bm{x}) \, T(\bm{x}, \bm{x}') \, d\bm{x} \equiv (\pi^{(i-1)}T)(\bm{x}')
\label{eq:ch5_markov_chain_transition}
\end{equation}
where $\pi^{(i)}(\bm{x}')$ is the \gls[hyper=false]{pdf} of $\bm{\mathcal{X}}^{(i)}$.
The rightmost definition signifies that the integration of the density function with the transition kernel is taken as an operator on the density function $\pi^{(i-1)}$ resulting in $\pi^{(i)}$ \cite{Stachurski2009,Sargent2017}.
As such, given the initial distribution of the chain $\bm{\mathcal{X}}^{(0)}$ and the transition kernel $T$,
the distributions of all the other Markov chain iterations are determined by repeating the integration for each successive iterations.
This, in terms of the transition operator,
\begin{equation}
	\begin{split}
		\pi^{(i)}(\bm{x}) & = (\pi^{(0)}T^i)(\bm{x}) \\
		                  & \equiv \idotsint\limits_\mathbf{X} \pi^{(0)}(\bm{x}^{(0)}) \, T(\bm{x}^{(0)}, \bm{x}^{(1)}) \ldots T(\bm{x}^{(i-1)}, \bm{x}) \, d\bm{x}^{(0)} \ldots d\bm{x}^{(i-1)}
	\end{split}
\label{eq:ch5_markov_chain_multiple_transitions}
\end{equation}

% Stationary Distribution
A density $\pi^*$ is said to be \emph{stationary} with respect to a transition kernel $T$ if the density is invariant under transition.
\marginpar{Stationary density}
Specifically,
\begin{equation}
	(\pi^*T)(\bm{x}) = \pi^*(\bm{x})
\label{eq:ch5_markov_chain_stationary_density}
\end{equation} 
In other words, once the chain reaches the stationary density, it will stay there and the chain itself becomes stationary.

% Convergence (Existence, Uniqueness, and Convergence)
The notion of stationary density of a Markov chain is central to the application of Markov chain for generating samples from an arbitrary probability density.
\marginpar{Fundamental theorem of Markov chain}
Under certain conditions\footnote{Specifically: irreducible, aperiodic, and Harris recurrent (see Appendix~\ref{app:markov_chain}).} for the transition kernel $T$, a stationary density exists, it is unique and the \emph{limiting density} of the stochastic process,
\begin{equation}
	\lim_{i \rightarrow \infty} \,\, |(\pi T^i) - \pi^*| = 0, \,\,\,\, \forall \pi \in \mathbf{D}
\label{eq:ch5_markov_chain_limiting_density}
\end{equation}
where $\mathbf{D}$ is the set of all possible \glspl[hyper=false]{pdf} on $\mathbf{X}$.
It implies that regardless of the starting density, some transition kernels will converge to a unique stationary density.
This in turn, is of practical importance when the transition kernel in an \gls[hyper=false]{mcmc} algorithm is designed such that the given target density is the stationary density.
This statement of the existence, the uniqueness, and the convergence of a stationary density is the \emph{fundamental theorem} of Markov chain \cite{Robert2004,Stachurski2009}.

% Ergodicity (Rate of Convergence)
Another important result in relation to the application of Markov chain for \gls[hyper=false]{mc}
\marginpar{Ergodic theorem}
simulation is the \emph{ergodic theorem} which states that for a given realization $\{\bm{x}^{(i)}\}_{i=1}^I$ of a Markov chain $\{\bm{\mathcal{X}}^{(i)}\}$ satisfying the conditions before\footnote{\emph{Ibid.}}, with a stationary density $\pi^*$, and for any integrable function $f: \mathbf{X} \mapsto \mathbb{R}$, it holds that
\begin{equation}
  \begin{split}
	& \lim_{I \rightarrow \infty} \frac{1}{I} \sum_{i = 1}^{I} f(\bm{x}^{(i)}) = \mathbb{E}_{\pi^*}[f] \\
  & \mathbb{E}_{\pi^*}[f] = \int_\mathbf{X} f(\bm{x}) \, \pi^*(\bm{x}) \, d\bm{x}
  \end{split}
\label{eq:ch5_markov_chain_ergodic_theorem}
\end{equation}
The ergodic theorem is the equivalence of the Law of Large Number for \gls[hyper=false]{mcmc} \cite{Robert2004,Sargent2017}.
This asymptotic result provides a justification for using samples generated from a Markov chain (of certain conditions) for estimating integral in a more conventional \gls[hyper=false]{mc} simulation.

% Central Limit Theorem (Limiting Behavior)
It is argued in \cite{Robert2004} that although the previous two theorems provide the mathematical foundation for constructing an \gls[hyper=false]{mcmc} algorithm,
\marginpar{Central Limit Theorem}
it is the property of the sample path\footnote{Recall that from Chapter~\ref{ch:gsa}, a sample path is a realization of a stochastic process and in this case, a realization of a Markov chain.} of a finite size, $\{\bm{x}^{(i)}\}_{i=1}^{I}$, that matters when conducting an actual \gls[hyper=false]{mcmc} simulation.
And indeed, yet another important theorem exist and it is instrumental in such a simulation setting.
The theorem states that for a given realization $\{\bm{x}^{(i)}\}_{i=1}^I$ of a Markov chain with stationary density $\pi^*$ under certain conditions\footnote{Namely, irreducible, aperiodic, Harris recurrent, \emph{and} geometrically ergodic. Thus the chain is called (geometrically) ergodic.\label{foot:bc_ergodicity}.}
and for function $f: \mathbf{X} \mapsto \mathbb{R}$ the following holds,
\begin{equation}
  \lim_{I \rightarrow \infty} \,\, \frac{1}{I} \sum_{i=1}^I f(\bm{x}^{(i)}) - \mathbb{E}_{\pi^*}[f] \thicksim \mathcal{N} \left(0, \frac{\sigma^2}{I}\right)
\label{eq:ch5_markov_chain_clt}
\end{equation}
That is, the difference between the sample mean and the expected value converges in distribution to the normal distribution with variance $\sigma^2/I$.
$\sigma^2$ is the variance of the function evaluated under the stationary density of the chain.
The theorem provides a basis for estimating the error of an estimate computed by \gls[hyper=false]{mcmc} samples of a finite size (assuming that stationarity has been attained).

Though similar to that of \gls[hyper=false]{mc} standard error \cite{Owen2013}, it is important to note that successive realizations of the Markov chain are not, by construction, independent and identically distributed.
The consequence of this will be revisited when the topic of analyzing samples of a Markov chain is discussed in Section~\ref{sec:bc_mcmc_practical_aspects}.

% Entry to MCMC, Detailed Balance Condition
In generating samples from a target density, 
\marginpar{Detailed balance condition}
the engineering is done somewhat in reverse: ``Given a density $\pi$, construct $T$ such that its stationary density $\pi^*$ converges to $\pi$''.
Thus it is worthwhile to note the \emph{detailed balance} condition which is a central condition for an \gls[hyper=false]{mcmc} algorithm.
A Markov chain with transition kernel density $T(\circ,\circ)$ satisfies the detailed balance condition if there exists a probability density $\pi$ such that,
\begin{equation}
  \pi(\bm{x}) \, T(\bm{x}, \bm{x}')  = \pi(\bm{x}') \, T(\bm{x}', \bm{x}) \,\,\,\, \forall \, \bm{x},\bm{x}' \in \mathbf{X}
\label{eq:ch5_markov_chain_detailed_balance}
\end{equation}
\marginpar{Reversible Markov chain}
As a result, the chain is said to be \emph{reversible}.
Formally, for $\mathbf{A} \subset \mathbf{X}$,
\begin{equation}
  \mathbb{P}(\bm{\mathcal{X}}^{(i)} \in \mathbf{A} | \bm{\mathcal{X}}^{(i-1)} = \bm{x}) = \mathbb{P}(\bm{\mathcal{X}}^{(i)} \in \mathbf{A} | \bm{\mathcal{X}}^{(i+1)} = \bm{x}) \,\,\,\, \forall \bm{x} \in \mathbf{X}
\label{eq:ch5_markov_chain_reversibility}
\end{equation}
A reversible chain is a stationary chain \cite{Robert2004}.
Consequently, in an \gls[hyper=false]{mcmc} algorithm,
if the transition probability satisfies the detailed balance condition with respect to the target distribution, 
it ensures the reversibility of the process and ultimately the stationarity of the chain.
Finally, by imposing additional condition\footnote{Namely, ergodicity (see Footnote~\ref{foot:bc_ergodicity}).}, the stationary distribution of the chain converges to the target distribution.

%------------------------------------------------------------
\subsection{Markov Chain Monte Carlo}\label{sub:bc_mcmc_mcmc}
%------------------------------------------------------------

% Introductory Paragraph
Consider once more the problem set up at the start of the section:
\marginpar{The objective revisited}
Generate samples from a target probability distribution with a density $p(\bm{x})$,
known at least up to a proportionality constant, $p(\bm{x}) \propto p^*(\bm{x})$.

% Markov Chain Monte Carlo
By acknowledging the theorems above,
the task is then to construct a Markov transition kernel such that the target density $p$ becomes the
stationary distribution of the Markov chain.
\marginpar{MCMC algorithms}
Thereafter, based on such kernel, generate a realization of the chain (i.e., \emph{simulate}) long enough for the limiting distribution of the chain is reached and it converges to the target density $p$.
As a result, the samples generated from the realization of the chain converges, in distribution, to the target density.
This, in essence, is the objective of \gls[hyper=false]{mcmc} algorithms as defined in \cite{Robert2004}.

% Metropolis-Hastings Algorithm, Origin
One might think that the task of constructing a Markov transition kernel would be difficult,
especially considering wide range of possible target distribution which might call for different classes of transition kernels.
\marginpar{Metropolis-Hastings algorithms, origin}
However, there exists a class of algorithms for generating Markov chain that guarantees its convergence (in distribution) to \emph{any} target distribution as its stationary distribution\footnote{see \cite{Robert2010,Geyer2011} for more rigorous treatment on the convergence properties.}.
The Metropolis-Hastings algorithm and its various extensions remains the most universal class of algorithms to generate such Markov chains \cite{Robert2010}. 
The method was first applied for a statistical mechanics problem by Metropolis et al. \cite{Metropolis1953}\footnote{There is apparently a controversy surrounding the attribution of the algorithm solely to Metropolis, especially when his role was claimed to be nothing more ``other than providing computer time'' \cite{Gubernatis2005}.}
and later generalized by Hastings \cite{Hastings1970}\footnote{There is, to the best of the author's knowledge, no controversy here.}.

% Metropolis-Hastings Algorithm, In essence
The \gls[hyper=false]{mh} algorithm prescribes two main components for constructing transition kernel of a Markov chain that converges to the target distribution: a \emph{proposal probability density} $q(\bm{x}^*|\bm{x})$ and an \emph{acceptance probability} $\alpha$.
\marginpar{Metropolis-Hastings algorithm, proposal density}
The proposal probability density is responsible for generating a proposal transition or candidate move $\bm{x}^*$ for the Markov chain at each iteration and it is in general a density conditional on the previous state.
This density is chosen such that it is easier to sample and indeed it is often selected from well-known densities such the Gaussian or uniform densities.
This proposal move, in turn, will be accepted with a probability,
\begin{equation}
	\alpha = \text{min} \left(\frac{p(\bm{x}^*)}{p(\bm{x}^{(i-1)})} \times \frac{q(\bm{x}^{(i-1)} | \bm{x}^*)}{q(\bm{x}^* | \bm{x}^{(i-1)})}, 1.0\right)
\label{eq:ch5_acceptance_probability}
\end{equation}
\marginpar{Metropolis-Hastings algorithm, acceptance probability}
Where $p(\bm{x}^*)$ and $p(\bm{x})$ are the values of the target density at the proposed state and the previous state, respectively;
and $q(\bm{x}^{(i-1)} | \bm{x}^*)$ ($q(\bm{x}^* | \bm{x}^{(i-1)})$) is the value of the proposal density at the previous (proposed) state conditional on the value of the proposed (previous) state.
Notice from the ratio, the proportionality constant in Eq.~(\ref{eq:bc_prop_const}) cancels out and only the density up to that constant $p^*$ is required. 
The acceptance probability is formulated to satisfy the \emph{detailed balance condition} (Eq.~(\ref{eq:ch5_markov_chain_detailed_balance})) for any valid proposal probability distribution. This, in turn, guarantees the stationarity of the process \cite{Chib1995}.

If the proposal move is accepted it becomes the current state of the chain,
otherwise the chain remains at its current state for the given iteration.
To generate a Markov chain of certain length (i.e., certain number of samples),
the steps are repeated multiple times until the required length of the chain is met.
Algorithm~\ref{alg:metropolis_hastings} summarizes the steps for constructing a Markov chain by \gls[hyper=false]{mh} algorithm.
\begin{algorithm}
\caption[Metropolis-Hastings Algorithm]{Metropolis-Hastings Algorithm \\ Generate samples from $p(\bm{x}) \propto p^*(\bm{x})$ given proposal density $q (\bm{x}^* | \bm{x})$ in $I$ iterations}
\label{alg:metropolis_hastings}
\begin{algorithmic}
  \REQUIRE $I > 0$, $p^*(\bm{x})$, and $q(\bm{x}^* | \bm{x}^{(i-1)})$
  \STATE $\bm{x}^{(0)} \leftarrow \bm{x}_0 \, ; \, \forall \bm{x}_0 \in \mathbf{X}$
  \FOR{$i = 1$ to $I$}
    \STATE sample $\bm{x}^*$ from $q(\bm{x} | \bm{x}^{(i-1)})$
    \STATE $\alpha \leftarrow \text{min} \left(\frac{p^*(\bm{x}^*)}{p^*(\bm{x}^{(i-1)})} \times \frac{q(\bm{x}^{(i-1)} | \bm{x}^*)}{q(\bm{x}^* | \bm{x}^{(i-1)})}, 1.0\right)$
    \STATE sample $u$ from $\mathcal{U}[0,1]$
    \IF{$u < \alpha$}
      \STATE $\bm{x}^{(i)} \leftarrow \bm{x}^*$
    \ELSE
      \STATE $\bm{x}^{(i)} \leftarrow \bm{x}^{(i-1)}$
    \ENDIF
  \ENDFOR
\end{algorithmic}
\end{algorithm}

% Random Walk Markov Chain
In the original paper of Metropolis et al.,
the proposal distribution was chosen to be a symmetric distribution such that $q(\bm{x}^* | \bm{x}^{(i-1)}) = q(\bm{x}^{(i-1)} | \bm{x}^*)$.
\marginpar{Random walk \gls[hyper=false]{mh} algorithm}
As a result the terms associated with the proposal density in Eq.~(\ref{eq:ch5_acceptance_probability}) cancel each other.
Consequently, any proposal move that yields an ``improvement'' on the target density evaluation will be accepted, otherwise it will only be accepted according to its acceptance probability. 
This particularly simple \gls[hyper=false]{mh} algorithm results in a \emph{random walk} Markov chain
and it is termed the \emph{random walk} \gls[hyper=false]{mh} \cite{Robert2010}.  

% Application on the Simple Running Example
To illustrate the application of the \gls[hyper=false]{mh}, particularly the random walk \gls[hyper=false]{mh},
\marginpar{Random walk \gls[hyper=false]{mh} algorithm, illustrated}
for generating samples from an arbitrary target distribution,
consider again the example of generating samples from the density given in Eq.~(\ref{eq:bc_unnormalized_gumbel}).
For this example, the proposal distribution is chosen to be a bivariate normal with a variance (the scale parameter) of $2.0$ equal in both dimensions and without correlation.
The initial state of the chain $\bm{x}^{(0)}$ is set to be at the origin.

% Proposing Move
The first three iterations of the random walk \gls[hyper=false]{mh} algorithm is illustrated in Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration}.
At the first iteration (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_1}),
a proposal move is drawn from the bivariate normal distribution (centered at the origin).
The proposal move brings the state closer to the center of the target density, thus it is accepted (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_2}).
A new proposal move is generated from the bivariate normal centered at the newly accepted move.
This time, because the proposal move moves farther away from the center of the target density, it is rejected.
The chain remains at the current state and a new proposal move is drawn (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_3}).
Note that this kind of proposal move will not always be rejected outright but is subject to chance based on the acceptance probability.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain_iteration},
			           maincaption={Illustration of the first three iterations in Markov Chain simulation by random walk \gls[hyper=false]{mh} algorithm to sample the density given in Eq.~(\ref{eq:bc_unnormalized_gumbel}) whose contours showed in solid lines. The proposal density is an independent bivariate normal distribution with $\sigma^2 = 2.0$ whose contours showed in dashed lines, centered at current state.},
			           mainshortcaption={Illustration of iterations in Markov Chain simulation by random walk Metropolis-Hastings algorithm.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_iteration_1},
			           leftcaption={Iteration $1$},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_iteration_2},
			           midcaption={Iteration $2$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_iteration_3},
			           rightcaption={Iteration $3$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_2}
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_3}

% Traversing the Target Density and Trace Plot
By repeating those steps multiple time, the chain traverses the state space according to the target distribution.
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_1} illustrates the chain traversing the $2$-dimensional state space of the density of Eq.~(\ref{eq:bc_unnormalized_gumbel}) for the first $250$ iterations.
In the long run, the chain will spend more time in the regions where the density are high and less time where the density low\footnote{The goal of an \gls[hyper=false]{mcmc} algorithm is not, on the other hand, to obtain the parameter value which maximizes the target distribution, at least not only. It seeks to explore the state space in proportion to value of the density function \cite{Tierney1994}.}.
Therefore, the resulting samples generated by the chain will be distributed according to the target distribution.
Figs.~\ref{fig:ch5_plot_illustrate_markov_chain_1} and~\ref{fig:ch5_plot_illustrate_markov_chain_2} are the \emph{trace} plots for the chain after $50'000$ iterations.
\marginpar{trace plot}
A trace plot shows the evolution of the chain during the iterations and it is often the first graphical diagnostic tool to spot any possible issue of convergence of a Markov chain \cite{Robert2010}.
In this particular case, the plots show that the chain seemingly converges to particular region of the input state space and within this region (i.e., the so-called \emph{typical set} \cite{Mackay2005}) the chain randomly moves from one state to another. 
It also indicates that $x_1$ are centered differently than $x_2$,
and $x_2$ has a relatively larger dispersion than $x_1$ (as can be seen also from the contour plot).
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain},
			           maincaption={Illustration of Markov Chain simulation to generate samples from a target density of Eq.~(\ref{eq:bc_unnormalized_gumbel}). (Left) The chain traverses the state space. At each iteration, a move is proposed and accepted in probabilistic manner. (Center and Right) The trace plots.},
			           mainshortcaption={Illustration of Markov Chain simulation to generate samples from a target density given in the example.},%
			           leftopt={width=0.28\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_1},
			           leftcaption={Trace plots in $2$-dimensional parameter plane (the first $250$ iterations)},
			           midopt={width=0.28\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_2},
			           midcaption={Trace plot for $x_1$},
			           rightopt={width=0.28\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_3},
			           rightcaption={Trace plot for $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChain_1}
{../figures/chapter5/figures/plotIllustrateMarkovChain_2}
{../figures/chapter5/figures/plotIllustrateMarkovChain_3}
%This also means that the goal of \gls[hyper=false]{mcmc} algorithm is not to obtain the values at which the target density is maximized, 

% Marginal Distribution
After the iterations are completed, the resulting samples should be distributed according to the target distribution.
Indeed this is the case for the \gls[hyper=false]{mcmc} simulation.
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_sample} shows that the resulting samples are distributed according to the target distribution both as a joint as well as its marginal.
The joint distribution, in particular, shows that the samples generated by \gls[hyper=false]{mcmc} simulation are correlated according to the correlation contain in the density of Eq.~(\ref{eq:bc_unnormalized_gumbel}).
Note that, in practice, the correct distribution of the resulting samples cannot simply be verified by comparing it to the analytical formula.
The whole point of generating samples via \gls[hyper=false]{mcmc} simulation is exactly because such  arbitrary high-dimensional distributions are hard to characterize.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain_sample},
			           maincaption={Results of samples generated by Markov chain simulation for the target density given in the example. After $50'000$ iterations, the samples resembles the actual shape of the distribution both as a joint (Left) and as marginal distributions (Center and Right). The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Results of samples generated by Markov chain simulation for the target density given in the example.},%
			           leftopt={width=0.28\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_sample_1},
			           leftcaption={Joint samples},
			           midopt={width=0.28\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_sample_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.28\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_sample_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_2}
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_3}

% The Importance of Proposal Distribution in Metropolis-Hastings Algorithm
Although the theorems that underlie the application of \gls[hyper=false]{mcmc} algorithm guarantee the convergence of the chain to the target distribution,
its rate of convergence is problem dependent.
For many \gls[hyper=false]{mh} algorithms, the choice of proposal distribution is particularly important in determining the convergence rate of the algorithm in reaching the target distribution as its stationary distribution.
For instance, Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_large_step} illustrates the case where the scale parameter of the proposal distribution is set to be much larger than the actual scale of the target distribution.
\marginpar{Over-dispersed proposal distribution.}
As shown, because the proposal moves can jump from one side of the state space to another,
they are rarely accepted and the chain stucks at the same values for a long period.
For the same length of the chain as before ($50'000$), the resulting distribution of the samples (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_large_step_2}) hardly resembles the target distribution.
% Over-dispersed step
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_markov_chain_large_step},
                  maincaption={Convergence issue due to an over-dispersed proposal distribution ($\sigma^2 =100.0$).},%
									mainshortcaption={Convergence issue due to an over-dispersed proposal distribution.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_illustrate_markov_chain_large_step_1},
                  leftcaption={Trace plot of $x_1$},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_illustrate_markov_chain_large_step_2},
                  rightcaption={Marginal of $x_1$},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotIllustrateMarkovChainLargeStep_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainLargeStep_2}

% Under-dispersed step
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step} shows the behavior of the chain in the case of too small variance parameter of the proposal distribution, in comparison to the scale of the target density.
\marginpar{Under-dispersed proposal distribution}
In this case, any proposal move around the previously accepted state would almost always be accepted and the chain traverses the state space very slowly Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step_1}.
Consequently, the resulting samples from the chain (with the same length of $50'000$ samples in this example) would not be representative of the target distribution as illustrated in Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step_2}.
It is important to note that, in both cases, the chain would eventually converge in distribution for both parameters.
But this convergence would not be attained for a Markov chain of a practical length.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_markov_chain_small_step},
                  maincaption={Convergence issue due to an under-dispersed proposal distribution ($\sigma^2 = 0.01$).},%
									mainshortcaption={Convergence issue due to an under-dispersed proposal distribution.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_illustrate_markov_chain_small_step_1},
                  leftcaption={Trace plot of $x_2$},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_illustrate_markov_chain_small_step_2},
                  rightcaption={Marginal of $x_2$},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotIllustrateMarkovChainSmallStep_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainSmallStep_2}

% Adaptive Sampler
In fact, the optimal choice of the scale parameters for the proposal distribution is closely related to the characteristic length scale of the target distribution \cite{Mackay2005}.
\marginpar{Tuning and adaptive \gls[hyper=false]{mh} algorithms}
Unfortunately, in practice, there would be little information, if any, on the characteristic length scale for each parameter of a given density.
As such, some sort of tuning is required regarding the proposal distribution for it to be optimal with respect to some chosen measures.
This is the main motivation for the development of various \emph{adaptive} \gls[hyper=false]{mh} algorithms.
In such algorithms, the transition kernels are adapted during iteration to optimize the performance of the algorithm \cite{Robert2010}.

% Entry to AIES
Instead of delving into any of the particular improvements on the \gls[hyper=false]{mh} algorithm (such as through the adaptive schemes)\footnote{Ref.~\cite{Andrieu2008} provides an overview of adaptive \gls[hyper=false]{mc} algorithms, while Refs.~\cite{Haario2001,Haario2006,Griffin2011} are examples of adaptive algorithms.},
this thesis adopted a relatively new \gls[hyper=false]{mcmc} algorithm based on an \emph{ensemble} Markov chain.
The algorithm has the potential of requiring minimal tuning to any given particular problem.
The main ideas of the above (i.e., proposal move and its acceptance probability), however,
remain central in an ensemble \gls[hyper=false]{mcmc} algorithm.

%----------------------------------------------------------------------------
\subsection{Affine-Invariant Ensemble Sampler (AIES)}\label{sub:bc_mcmc_aies}
%----------------------------------------------------------------------------

% Introductory Paragraph and main motivation
\Glsfirst[hyper=false]{aies} is an \gls[hyper=false]{mcmc} algorithm proposed by Goodman and Weare \cite{Goodman2010}.
\marginpar{AIES, motivation}
Its main motivation is exactly where the previous section left off: the difficulty in tuning or adapting \gls[hyper=false]{mh} algorithm applicable to wide class of target distribution.
The situation is typically worsened for a highly correlated distribution of high dimension where some of the length scales of the target distribution are very small forcing an adaptive algorithm to spend majority of its time tuning the scale of the proposal distribution,
eventually resulting in high overhead computational cost of adaptation \cite{Foreman-Mackey2013}.

Through some affine transformations,
a target distribution with a highly skewed aspect ratio can be made less skewed and thus less awkward to be sampled from \cite{Huijser2015}.
By ensuring the algorithm to be affine-invariance, the performance of the algorithm would then be equal under all affine transformations of the target distribution.
Finally, implementing such a transformation to an ensemble sampler\footnote{\emph{sampler of many particles}, as opposed to a sampler of \emph{single particle} in the conventional \gls[hyper=false]{mcmc} algorithm, such as the previous \gls[hyper=false]{mh} algorithm.} results in an algorithm that requires minimal tuning with respect to each of the state space dimensions (explained below).

% Ensemble Sampler
\gls[hyper=false]{aies} belongs to a class of \gls[hyper=false]{mcmc} algorithms that generates a Markov chain on the state space of \emph{ensembles} (i.e., \emph{ensemble samplers}) \cite{Goodman2010}.
\marginpar{Ensemble sampler}
An ensemble $\vec{\bm{\mathcal{X}}}$ is a collection of $L$ random variables $\{\bm{\mathcal{X}}_{l}\}_{l = 1}^{L}$ called \emph{walkers}, each of which is in $\mathbb{R}^D$.
That is, $\bm{\mathcal{X}}_{l} = [\mathcal{X}_{l,1}, \mathcal{X}_{l,2}, \ldots, \mathcal{X}_{l, D}]$.
The ensemble of $L$ walkers are independent to each other with respect to the target distribution $p (\circ)$.
Specifically,
\begin{equation}
	p(\vec{\bm{x}}) = p(\bm{x}_1) \cdot p(\bm{x}_2) \cdot \ldots \cdot p(\bm{x}_L) 
\label{eq:ch5_ensemble_independence}
\end{equation}
Eq.~\ref{eq:ch5_ensemble_independence} implies that the target distribution is being independently sampled by $L$ walkers.

A Markov chain of an ensemble, in turn, is a sequence of ensembles, $\{\vec{\bm{\mathcal{X}}}^{(i)}\}$ for $i \geq 0$ that follows the Markov property while preserving the condition in Eq.~(\ref{eq:ch5_ensemble_independence}).
Consequently, the Markov property lies on the state space of the ensemble\footnote{if each walker is in $\mathbb{R}^D$ then an ensemble of $L$ walkers can be thought of to be in $\mathbb{R}^{DL}$.}
and the sequence of each walker $\{\bm{\mathcal{X}}_l^{(i)}\}$ itself needs not be Markovian \cite{Goodman2010}.

% Affine Transformation
In \gls[hyper=false]{aies}, the transition between states of an ensemble is conducted by carrying out an affine transformation to the ensemble.
\marginpar{Affine Transformation}
The transition (thus the transformation) is carried out at the level of individual walkers one at a time.
In other words, an affine transformation $f_a$ is defined such that, 
\begin{equation}
	f_a: \mathcal{X} \mapsto \mathcal{Y} = M \mathcal{X} + b
\label{eq:ch5_affine_transformation_single}
\end{equation}
where $M$ and $b$ are scalars.
When applied to an ensemble of multivariate random variables $\bm{\mathcal{X}}$,
\begin{equation}
	\begin{split}
		 & f_a: \vec{\bm{\mathcal{X}}} \mapsto \vec{\bm{\mathcal{Y}}} = \mathbf{M} \vec{\bm{\mathcal{X}}} + \bm{b} \\
		 & \vec{\bm{\mathcal{Y}}} = [\mathbf{M} \bm{\mathcal{X}}_1 + \bm{b}, \mathbf{M} \bm{\mathcal{X}}_2 + \bm{b}, \ldots, \mathbf{M} \bm{\mathcal{X}}_L + \bm{b}]
	\end{split}
\label{eq:ch5_affine_transformation}
\end{equation}
where $\mathbf{M}$ is now a $D \times D$ invertible matrix;
and $\bm{b}$ is a $D$-dimensional vector.
In the context of \gls[hyper=false]{mcmc} simulation,
$\vec{\bm{\mathcal{Y}}}$ would represent the proposal move of the chain transition whose acceptance is subject to chance as will be discussed further below.

% Affine Invariance
Suppose $\bm{\mathcal{X}}$ is a multivariate random variable with a state space $\mathbf{X} \subseteq \mathbb{R}^D$ and has a multivariate \gls[hyper=false]{pdf} $p$.
Let $\bm{\mathcal{Y}} = \mathbf{M} \bm{\mathcal{X}} + \bm{b}$,
then the \gls[hyper=false]{pdf} of $\bm{\mathcal{Y}}$, according to the change of variables rule \cite{Siegrist2017}, is given by
\begin{equation}
	g(\bm{y}) = \frac{1}{|\text{det}\,\mathbf{M}|} p(\mathbf{M}^{-1} (\bm{y} - \bm{b})) = \frac{1}{|\text{det} \,\mathbf{M}|} p(\bm{x}) \propto p(\bm{x})
\label{eq:ch5_random_variable_linear_transformation}
\end{equation}
In other words, barring a proportionality constant, $\bm{\mathcal{Y}}$ is the distributed the same way as $\bm{\mathcal{X}}$.
An ensemble \gls[hyper=false]{mcmc} algorithm is called \emph{affine-invariant} if the transition kernel of the algorithm follows the same transformation:
\marginpar{Affine-invariant algorithm}
\begin{equation}
	T(\vec{\bm{y}}, \vec{\bm{y}}^*) = C_{\mathbf{M},\bm{b}} T(\vec{\bm{x}}, \vec{\bm{x}}^*)
\label{eq:ch5_affine_invariance}
\end{equation}
where $T(\vec{\bm{y}}, \vec{\bm{y}}^*)$ is the transition kernel of the transformed variable;
$T(\vec{\bm{x}}, \vec{\bm{x}}^*)$ is the transition kernel of the original variable;
and $C_{\mathbf{M},\bm{b}}$ is a normalizing constant of the transition kernel, independent of the variable. 
This implies that the algorithm sees no different between sampling the transformed variables or the original variables.
An affine-invariant algorithm thus requires no modification under any affine transformation of the variables \cite{Goodman2010,Hou2012,Foreman-Mackey2013}.

% AIES Algorithm (Stretch Move)
One particular implementation of an \gls[hyper=false]{aies} \gls[hyper=false]{mcmc} algorithm is the so-called \emph{stretch-move} \cite{Goodman2010,Foreman-Mackey2013}.
\marginpar{AIES, stretch-move}
As mentioned, the transition between iterations of an ensemble starts at the level of individual walkers.
That is, the update is carried out one walker at a time and for stretch-move it proceeds as follows.

Let $\bm{\mathcal{X}}_l^{(i-1)}$ be the walker $l$ at iteration $(i-1)$, and is to be updated.
\marginpar{Complementary ensemble}
Let $\vec{\bm{\mathcal{X}}}_{\sim l}^{(i-1)}$, called a \emph{complementary ensemble},
be the ensemble of walkers at iteration $i$,
complementary to the walker $\bm{\mathcal{X}}_l^{(i-1)}$.
Specifically,
\begin{equation}
	\vec{\bm{\mathcal{X}}}_{\sim l}^{i-1} = [\bm{\mathcal{X}}_{1}^{(i)}, \ldots, \bm{\mathcal{X}}_{l-1}^{(i)}, \bm{\mathcal{X}}_{l+1}^{(i-1)}, \ldots, \bm{\mathcal{X}}_{L}^{(i-1)}]
\label{eq:ch5_aies_complementary_ensemble}
\end{equation}
where all the walkers $k < l$ have been updated to their respective new states.
Finally, let $\vec{\bm{x}}^{(0)} = [\bm{x}^{(0)}_1, \ldots, \bm{x}^{(0)}_L]$ be the initial state of the chain, arbitrarily chosen within the support of $\bm{\mathcal{X}}$.

% Proposal move
In transitioning the ensemble to $\vec{\bm{x}}^{(i)}$,
a proposal move is made on one walker at a time and it follows an affine transformation:
\marginpar{Proposal move}
\begin{equation}
	\bm{x}_l^* = \bm{x}_j + z (\bm{x}_l^{(i-1)} - \bm{x}_j)
\label{eq:ch5_proposal_move_stretch_move}
\end{equation}
where $\bm{x}_l^*$ is the proposal move for the walker $l$ at the current iteration;
$\bm{x}_l^{(i-1)}$ is the walker to be updated (i.e., walker $l$ at the previous state $(i-1)$);
$\bm{x}_j$ is the \emph{complementary walker}, randomly selected from the complementary ensembles of $\vec{\bm{x}}^{(i-1)}_{\sim l}$;
and $z$ is the scaler of the transformation (i.e., the \emph{stretcher}), randomly generated from,
\begin{equation}
	g(z) \propto z^{-0.5}, \,\,\,\, z \in [a^{-1}, a]
\label{eq:ch5_scaler_distribution}
\end{equation}
where $a$ is a free positive parameter,
and where the value of $2.0$ is widely used as default for many applications \cite{Goodman2010,Hou2012,Allison2013,Foreman-Mackey2013,Akeret2013}.

% Stretch-Move, Illustrated
Fig.~\ref{fig:ch5_aies_stretch} illustrates how a move is proposed in stretch-move for a single walker in an ensemble of $10$ walkers, in a $2$-dimensional state space.
\marginpar{Stretch-move, illustrated}
First, a walker in the ensemble will be updated while the rest of the walkers becomes its complementary walkers (Fig.~\ref{fig:ch5_aies_stretch_1}).
Secondly, a complementary walker is randomly selected among the ensemble of complementary walkers (Fig.~\ref{fig:ch5_aies_stretch_2}).
Thirdly and finally,
a proposal scaler is randomly generated according to $q$ and a proposed move is made according to Eq. (Fig.~\ref{fig:ch5_aies_stretch_3}).
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_aies_stretch},
			           maincaption={Illustration of a \emph{stretch move} update for a single walker in a $2$-dimensional state space.},
			           mainshortcaption={Illustration of a \emph{stretch move} update for a single walker in a $2$-dimensional state space.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_aies_stretch_1},
			           leftcaption={A walker in an ensemble of complementary walkers},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_aies_stretch_2},
			           midcaption={A complementary walker is randomly selected},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_aies_stretch_3},
			           rightcaption={A move is proposed},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/aies_stretch_1}
{../figures/chapter5/figures/aies_stretch_2}
{../figures/chapter5/figures/aies_stretch_3}

% Acceptance probability
The probability of accepting this proposal move is given by,
\marginpar{Acceptance probability}
\begin{equation}
	\alpha = \text{min} \left(\frac{p(\bm{x}_l^*)}{p(\bm{x}_l^{(i-1)})} \times z^{D-1}, 1.0\right)
\label{eq:ch5_acceptance_probability_stretch_move}
\end{equation}
where $p^*(\bm{x}_l^{*})$ and $p^*(\bm{x}_l^{(i-1)})$ are the values of the (unnormalized) target density at the proposed and previous states, respectively;
and $D$ is the dimension of the state space (or the dimension of the state space).
As with the \gls[hyper=false]{mh} algorithm,
if accepted the proposal move becomes the current state of the walker; otherwise, it remains in the previous state.
The steps are then repeated for the current iteration until all the walkers in the ensemble have been updated.
Algorithm~\ref{alg:aies} summarizes the steps in the stretch-move \gls[hyper=false]{aies} \gls[hyper=false]{mcmc} algorithm.
\begin{algorithm}
\caption[Affine-Invariance Ensemble Sampler (Stretch-Move)]{Affine-Invariant Ensemble Sampler (Stretch Move)\\ Generate samples from $p(\bm{x}) \propto p^*(\bm{x})$ using $L$ walkers in $I$ iterations.}
\label{alg:aies}
\begin{algorithmic}
  \REQUIRE $I > 0$, $L \geq I + 1$, $p^*(\bm{x})$, and $q (z)$
		\STATE $\vec{\bm{x}}^{(0)} = [\bm{x}^{(0)}_1, \ldots, \bm{x}^{(0)}_L] \leftarrow \vec{\bm{x}}_0 \, ; \, \forall \vec{\bm{x}}_0 \in \mathbf{X}^L$
		\FOR{$i = 1$ to $I$}
			\FOR{$l = 1$ to $L$}
				\STATE pick randomly $\bm{x}_j$ from $\vec{\bm{x}}^{(i-1)}_{\sim l}$
				\STATE sample $z$ from $q(z)$
				\STATE $\bm{x}^* \leftarrow \bm{x}_j + z (\bm{x}^{(i-1)}_l - \bm{x}_j)$
				\STATE $\alpha \leftarrow \text{min} \left(\frac{p^*(\bm{x}^*)}{p^*(\bm{x}^{(i-1)}_l)} \times z^{D-1}, 1.0\right)$
			\STATE sample $u$ from $\mathcal{U}[0,1]$
			\IF{$u < \alpha$}
				\STATE $\bm{x}^{(i)}_l \leftarrow \bm{x}^*$
			\ELSE
				\STATE $\bm{x}^{(i)}_l \leftarrow \bm{x}^{(i-1)}_l$
			\ENDIF
			\ENDFOR
		\ENDFOR
\end{algorithmic}
\end{algorithm}

% Running Example
To illustrate the application of the \gls[hyper=false]{aies} algorithm
\marginpar{\gls[hyper=false]{aies} algorithm, illustrated}
for generating samples from an arbitrary target distribution,
consider again the example of generating samples from the density given in Eq.~(\ref{eq:bc_unnormalized_gumbel}).
The number of walkers is set to be $100$ and the algorithm is run for $500$ iterations.
In other words, there is $50'000$ target density evaluations, $1/10$ the number of evaluations for the illustration of the \gls[hyper=false]{mh} algorithm before.
The initial state of the chain $\vec{\bm{x}}^{(0)}$ is set to be at the origin for all walkers.

% Traversing the Target Density and Trace Plot
Fig.~\ref{fig:ch5_plot_aies_walker_ensemble_1} presents the evolution of each individual walker traversing the state space of variable $x_1$.
\marginpar{ensemble samples, trace plot}
As mentioned, the Markov property of an ensemble is not guaranteed at individual.
Thus it is more appropriate to graphically diagnose the chain by plotting all the individual walkers.
It is shown here that after an obvious initial phase,
the ensemble seems to converge and stays around particular region of the state space (i.e., its typical set).
The width of the darker region, indicating region of the state space visited more often,
show the dispersion of the variable. 
Indeed this is confirmed by plotting the running empirical mean and standard deviation of the ensemble (Fig.~\ref{fig:ch5_plot_aies_walker_running_stats_1}).
The values $5.0$ and $2.3$ shown in the plot are the analytical mean and standard deviation of $x_1$, respectively.
Although not shown here, the behavior of the  $x_2$ is similar.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_aies_walker_ensemble},
                  maincaption={Trace plots of individual walkers and the running mean and standard deviation for $x_1$.},%
									mainshortcaption={.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_aies_walker_ensemble_1},
                  leftcaption={Trace plot of all walkers},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_aies_walker_running_stats_1},
                  rightcaption={Running empirical mean and standard deviation of the samples},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotAIESWalkerEnsemble_1}
{../figures/chapter5/figures/plotAIESWalkerRunningStats_1}

% Implementation
%\normdoublefigure[pos=tbhp,
%                  mainlabel={fig:ch5_plot_aies_walker_2},
%                  maincaption={Convergence issue due to an under-dispersed proposal density.},%
%									mainshortcaption={Convergence issue due to an under-dispersed proposal density. The marginal densities have been normalized to match the peak of the histogram.},
%                  leftopt={width=0.45\textwidth},
%                  leftlabel={fig:ch5_plot_aies_walker_ensemble_2},
%                  leftcaption={Trace plot of all walkers},
                  %leftshortcaption={},%
%                  rightopt={width=0.45\textwidth},
%                 rightlabel={fig:ch5_plot_aies_walker_running_stats_2},
%                  rightcaption={Running empirical mean and standard deviation of the samples},
                  %rightshortcaption={},
                  %spacing={\hfill}
%                 ]
%{../figures/chapter5/figures/plotAIESWalkerEnsemble_2}
%{../figures/chapter5/figures/plotAIESWalkerRunningStats_2}

% Marginal Distribution
After the iterations are completed, the resulting samples should be distributed according to the target distribution.
\marginpar{ensemble samples, marginal}
Fig.~\ref{fig:ch5_plot_aies_walker_samples} shows that the resulting samples are distributed according to the target distribution both as a joint as well as its marginal.
Shown here are the samples with the aforementioned ``initialization phase'' removed from the final tally.
It was estimated from the plot to last for about $100$ iterations.
Thus the final number of samples presented below is $(500-100)\times100=40'000$ samples. 
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_aies_walker_samples},
			           maincaption={Results of samples generated by \gls[hyper=false]{aies}. After $500$ iterations of an ensemble of $100$ walkers (for a total $50'000$ target density evaluations), the samples resembles the actual shape of the distribution both as a joint distribution (Left) and as marginal distributions (Center and Right). The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Results of samples generated by \gls[hyper=false]{aies} for the target density given in the example.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_aies_walker_samples_1},
			           leftcaption={Joint samples},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_aies_walker_samples_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_aies_walker_samples_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotAIESWalkerSamples_1}
{../figures/chapter5/figures/plotAIESWalkerSamples_2}
{../figures/chapter5/figures/plotAIESWalkerSamples_3}

% intuition Possible problem and Why not
The \gls[hyper=false]{aies} algorithm requires minimal tuning to generate samples from a arbitrary target distribution.
\marginpar{\gls[hyper=false]{aies} algorithm, intuition}
The intuition behind this is that instead of trying to adapt the proposal distribution during the iterations (possibly with several associated tuning parameters),
a proposal move relies on the information carried by the previous positions of the complementary ensemble.
With a large number of walkers, thus with larger computational cost per iteration, more information will automatically be available per iteration regarding the landscape of the distribution.
And although the parameter $a$ in Eq.~(\ref{eq:ch5_scaler_distribution}) is one potential tuning parameter,
most applications work well with the value of $2.0$.
As such, the algorithm required mainly the decision on the number of walkers $L$ and total number of iterations $I$ (these two yields the total number of target density evaluations $L \times I$).

% Possible problems
Depending on how the ensemble is distributed at the beginning, however,
might cause a relatively long ``initialization phase'' noted above (compared to the total length of the iterations).
\marginpar{\gls[hyper=false]{aies} algorithm, possible problems}
This is especially the case if the chain starts at very atypical value of the distribution.
Having a large ensemble implies a larger ``inertia'' for each of them to move and settle to a more typical region of the state space.
The algorithm is also recently reported to scale worse in the number of dimensions and will eventually fail in a high dimension with $D \geq 50$ either by a very slow convergence or, more importantly, by a biased convergence (i.e., it converges to a wrong value) \cite{Carpenter2017,Huijser2015}.
The number of dimensions concerned in this thesis, however, is still far below the above value. 

% Entry to practical aspects
There are two important issues not discussed in detail in either of the illustrations above, namely the convergence and the required length of the chain.
These issues are of practical importance for any \gls[hyper=false]{mcmc} algorithm.
For instance, the ``initialization phase'' obviously presents in the illustration of \gls[hyper=false]{aies} algorithm above is an indication of a particular lack of convergence to stationarity in the initial part of the transient.
It needs to be detected and removed lest they would bias the estimation using the samples.
Rigorous proof of stationarity is difficult to obtain in practice and instead, analysis is often based on empirical diagnostic that can only indicate lack of stationarity.
Afterward,
the main question will be on how long the chain needs to be generated.
In turn, these issues are closely related to the error of an \gls[hyper=false]{mcmc} estimate.
Analyzing a realization of a Markov chain for Monte Carlo application is the subject of the next section.