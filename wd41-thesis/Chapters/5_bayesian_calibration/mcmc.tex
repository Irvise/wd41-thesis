\newpage
%*******************************************************************
\section{\glsfirst[hyper=false]{mcmc} Simulation}\label{sec:bc_mcmc}
%*******************************************************************

% Introductory Paragraph
The formulation of Bayesian calibration of a computer model presented above results in a joint posterior \gls[hyper=false]{pdf} for all the parameters involved.
\marginpar{Posterior uncertainty of the model parameters}
This density contains all the information (and consequently, the uncertainties) regarding the model parameters conditioned on the observed data and the assumed data-generating process.
The uncertainties associated with the model parameters can then be represented using different summary statistics, many of which involve integration.

For example,
the uncertainties associated with a model parameter $x_d$ can be represented with its variance,
which is defined as
\begin{equation*}
	\begin{split}
	\mathbb{V}[\mathcal{X}_d] & = \mathbb{E}[\mathcal{X}_d^2] - \mathbb{E}^2[\mathcal{X}_d]\\
	                          & = \int_{-\infty}^{\infty} x_d^2 p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} x_d p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
	\end{split}
%\label{eq:variance_marginalization}
\end{equation*}
where the integrations are carried out over the support of $p(\bm{x}|\mathbf{y})$, assumed to be on the entire real line.
An alternative way to summarize the uncertainties of a model parameter is through its $\theta$-quantile $Q^{\theta}_d$,
which for parameter $x_d$ is defined as
\begin{equation*}
	Q^{\theta}_d \, : \, \mathbb{P} (\mathcal{X}_d \leq Q^{\theta}_d) = \int^{Q^{\theta}_d}_{-\infty} \int\limits_{\mathbf{X}_{\sim d}} p(x_d, \bm{x}_{\sim d} | \mathbf{y}) d\bm{x}_{\sim d} d x_d = \theta
%	\label{eq:quantile_integral}
\end{equation*}
Assuming that the support of $p(\bm{x}|\mathbf{y})$ is on the entire real line.
In this manner,
the $95\%$ confidence interval of the parameter is written as $Q_d^{0.025} \leq \mathcal{X}_d \leq Q_d^{0.975}$.

Though these summaries might be of interest themselves,
\marginpar{Posterior uncertainty of the model prediction}
in an application setting, the model parameters uncertainties are often propagated through the simulation model to obtain the uncertainty in the output (prediction).
Hence, the output from a simulation model $y = f(\bm{x})$ is expressed as random variable $\mathcal{Y}$ from a transformation of random variable $\bm{\mathcal{X}}|\mathbf{y}$ by the function $f$
\begin{equation*}
	\mathcal{Y} = f(\bm{\mathcal{X}} | \mathbf{y})  \, ; \, p_{\bm{\mathcal{X}} | \mathbf{y}} (\bm{x}) = p(\bm{x}| \mathbf{y})  
\end{equation*}
where the \gls[hyper=false]{pdf} of $\bm{\mathcal{X}} | \mathbf{y}$ is the posterior density $p(\bm{x}| \mathbf{y})$.
The actual \gls[hyper=false]{pdf} of $\mathcal{Y}$ follows the rule of transformation of random variable
and it represents the uncertainty in the output due to the uncertainty in the input parameter conditioned on the data.
This uncertainty can also be summarized with various statistics and, as before, many of which involve integration operation.
For instance, the variance of the output, is written as
\begin{equation*}
	\mathbb{V}[\mathcal{Y}] = \int_{-\infty}^{\infty} f^2(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} f(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
\end{equation*}

The posterior density $p(\bm{x}|\mathbf{y})$ and the function $f(\bm{x})$, however, are in practice highly multidimensional functions and 
the performance of numerical integration is typically worsened with an increasing number of dimensions of the input parameter space.
\marginpar{Challenges in dealing with posterior density}
At the same time,
conducting \gls[hyper=false]{mc} simulation for estimating the integrals (such was done in Chapter~\ref{ch:gsa}) is not straightforward in this case.
The multiplication of likelihood and prior density will, in general, yield an arbitrary posterior density not available in a closed-form expression.
As a result, generating independent samples from the posterior density required for the \gls[hyper=false]{mc} estimation becomes a difficult task.

This section presents a simulation approach, the so-called \gls[hyper=false]{mcmc} simulation,
to directly generate samples from an arbitrary \gls[hyper=false]{pdf}. 
These samples, in turn, are useful for estimating various quantities given as examples above independent on the dimension of the input parameter space.

Although in the context of Bayesian data analysis such an arbitrary \gls[hyper=false]{pdf} of interest is the posterior \gls[hyper=false]{pdf},
the problem of generating sample from an arbitrary \gls[hyper=false]{pdf} is quite general.
As such, in the following discussion, a generic notation for an arbitrary \gls[hyper=false]{pdf} $p(\bm{x})$ is used instead of $p(\bm{x}|\mathbf{y})$.

%----------------------------------------------------
\subsection{Motivation}\label{sub:bc_mcmc_motivation}
%----------------------------------------------------

% Introductory Paragraph
Consider the following problem: Given a \gls[hyper=false]{pdf} $p:\mathbf{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}^+$,
generate a set of samples $\{\bm{x}_n\}_{n=1}^N$ from the \gls[hyper=false]{pdf}.
\marginpar{Problem statement}
It is assumed that the \gls[hyper=false]{pdf} can be evaluated at any given $\bm{x} \in \mathbf{X}$, at least up to a proportionality constant:
\begin{equation}
	p(\bm{x}) = \frac{p^*(\bm{x})}{C} \propto p^*(\bm{x})  
\label{eq:bc_prop_post}
\end{equation}
The proportionality constant in the above equation is the normalizing constant such that $p$ is a valid \gls[hyper=false]{pdf},
\begin{equation}
	C = \int p^*(\bm{x}) d\bm{x}  \Rightarrow \int p(\bm{x}) d\bm{x} = 1.0
\label{eq:bc_prop_const}
\end{equation}
Such samples can then be used, among other things,
to evaluate different summary statistics (such as expectation, variance, etc.) of $\bm{x}$ itself or
of any function under the \gls[hyper=false]{pdf}.

% Why it it difficult to samples
Generating samples from an arbitrary multidimensional density function is generally a difficult task.
\marginpar{A correct sampling}
Intuitively, for a given sample size, correctly generating samples from a density means
that the sample values have to be distributed proportional to its \gls[hyper=false]{pdf}.
There should be more samples in the region where the \gls[hyper=false]{pdf} value is high,
and less in the the region where the \gls[hyper=false]{pdf} value is low.
For a complex multidimensional density function,
these locations are not known a priori and might have to be identified exhaustively \cite{Mackay2005}.

% Inverse Transform Sampling and Its Problem
In a relatively low dimension,
the most common way of generating sample from a given density is
by inverse transform sampling coupled with a random number generator.
\marginpar{Inverse transform sampling}
The approach requires the quantile function of the \gls[hyper=false]{pdf}.
To obtain the quantile function,
the density has to be integrated and its normalizing constant has to be computed.
Appendix~\ref{app:its} provides a more detail account on the topic.
Many univariate (and some multivariate) \glspl[hyper=false]{pdf} are widely studied and the analytical solutions to their quantiles are available \cite{Lange2010}.
Additionally, sampling algorithms readily exist for several multivariate densities due to their special properties (e.g., generating sample from a multivariate normal density in Appendix~\ref{app:mvn_sampling}).
However, this will not be the case for an arbitrary density function of higher dimension.
In the absence of an analytical solution,
the cost of numerically integrating such function might itself be very costly due to the size of the dimension.

% Illustration
To illustrate this point,
\marginpar{Illustration}
consider the following bivariate (unnormalized) density parameterized by location parameters $\mu_1, \mu_2$and scale parameters $\sigma_1, \sigma_2$:
\begin{equation}
	\begin{split}
	& p^*(x_1, x_2) = \frac{\exp{(-(x_1 - \mu_1)/\sigma_1)} \exp{(-(x_2 - \mu_2)/\sigma_2)}}{(1 + \exp{(-(x_1 - \mu_1)/\sigma_1)} + \exp{(-(x_2 - \mu_2)/\sigma_2)})^3} \, \\ 
	& x_1, x_2 \in \mathbb{R}; \mu_1, \mu_2 \in \mathbb{R};\, \text{and} \, \sigma_1, \sigma_2 \in \mathbb{R}^+
	\end{split}
\label{eq:bc_unnormalized_gumbel}
\end{equation}
In this simple example,
the inverse transform sampling works well as the function is relatively easy to integrate in order to obtain the quantile function.
However, for the sake of illustration, it is assumed here that the quantile function of Eq.~(\ref{eq:bc_unnormalized_gumbel}) and its numerical approximation are not available.
Fig.~\ref{fig:ch5_plot_gumbel_illustration} shows the contour plot of the joint density as well as the marginal density for each of the variate.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_gumbel_illustration},
                  maincaption={Joint and marginal densities plots of the unnormalized \gls[hyper=false]{pdf} in the example. The parameters used in the example are: $\mu_1 = 5, \mu_2 = 2, \sigma_1 = 1.25$, and $\sigma_2 = 3$.},%
									mainshortcaption={Joint and marginal densities plots for the unnormalized \gls[hyper=false]{pdf} in the example.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_gumbel_illustration_1},
                  leftcaption={Joint density},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_gumbel_illustration_2},
                  rightcaption={Marginal density},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotGumbelIllustration_1}
{../figures/chapter5/figures/plotGumbelIllustration_2}

% Grid Approach
A straightforward approach to generate samples from a given multivariate density is done by first discretizing the input parameter space of the density function and evaluate the density at the discretized points.
\marginpar{Discretized grid approach}
Supposed the domain of the density has been discretized uniformly in each dimension with a level $\Delta$ resulting in $\{\bm{x}_i\}_{i=1}^{I}$ with $I$ the number of discretized points.
At the discretized levels, each value of $\bm{x}_i$ is associated with the probability $p(\bm{x}_i) = p^*(\bm{x}_i) / \sum_i p^*(\bm{x}_i)$.
These pairs constitute a complete discrete probability distributions.
Generating samples from such a discrete probability distribution is simple in modern computing environment \cite{Mackay2005}.

% The example
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid} illustrates this procedure for the example given above.
First, the input parameter space is windowed in $\mathbf{X}\in[-25,25]^2$ before being discretized in $\Delta = 50$ levels.
\marginpar{Discretized grid approach illustrated}
This results in $(\Delta + 1)^2 = 2'601$ discretized points at which the density is evaluated (Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1}).
Next, the density values are taken to be the probability for each of the $2'601$ discretized points.
Together they make up a complete discrete probability distribution from which samples can be readily generated.

% The figure explained
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1} shows $5'000$ samples generated from the discrete distribution.
Darker points indicate that the values have been sampled multiple times following the actual underlying \gls[hyper=false]{pdf}.
The contour of the analytical joint density is overlaid to serve as a guide. 
Figs.~\ref{fig:ch5_plot_gumbel_sample_grid_2} and~\ref{fig:ch5_plot_gumbel_sample_grid_3} show the histograms for each of the marginals.
The figure shows that the generated samples are indeed approximately distributed as the given \gls[hyper=false]{pdf}.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_gumbel_sample_grid},
			           maincaption={Sampling from a multivariate density by discretizing the input parameter space in grids. The input parameter space is discretized into $\Delta = 50$ levels. The density is then evaluated at the discretized points. (Left) $5'000$ samples are generated following the resulting discrete probability distribution; (Center and Right) The histograms of the marginals approximately follow the shape of the respective analytical marginal density. The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Sampling from a multivariate density by discretizing the input parameter space in grids.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_gumbel_sample_grid_1},
			           leftcaption={Joint samples},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_gumbel_sample_grid_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_gumbel_sample_grid_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotGumbelSampleGrid_1}
{../figures/chapter5/figures/plotGumbelSampleGrid_2}
{../figures/chapter5/figures/plotGumbelSampleGrid_3}

% Problem with Discretized Grid
The main issue with the discretized grid approach, conceptually simple as it is,
is the curse of dimensionality similar to the one mentioned in the previous chapters.
\marginpar{curse of dimensionality}
The number of density evaluations grows exponentially with the number of dimension.
As a rule, for a given discretization level $\Delta$ and dimension $D$,
the number of density evaluations is $(\Delta+1)^D$.

% In relation to Bayesian data analysis
In the context of Bayesian data analysis,
the multidimensional likelihood function inside the posterior generally can be a complex function.
In consequence, a very fine discretization level might be required to appropriately capture the function behavior at important regions which are unknown a priori.
On top of that, the computational cost of evaluating the (complex) posterior density becomes nonnegligible.
As such, except for a very simple likelihood function and/or in a very low dimension,
grid approach is deemed inapplicable for generating samples from an arbitrary multidimensional density.

% Entry to Markov Chain
To circumvent these issues,
a sampling technique based on the theory of stochastic process is widely adopted.
Specifically, by constructing a Markov chain\footnote{a realization of a Markov process} of the input parameters values,
the resulting process will eventually becomes stationary and simultaneously converge to any given \gls[hyper=false]{pdf} (i.e., \emph{target density}).
In other words, the samples generated from the stationary process are distributed according to the given density. 
Theoretically, this family of techniques would be independent of the dimension of the input parameter space and its convergence are guaranteed.

The next subsection briefly presents the basics of Markov chain and its importance in solving the the problem of generating samples from an arbitrary density.
Afterward,
a traditional and a more recent methods for constructing a Markov chain for the purpose of \gls[hyper=false]{mc} simulation are introduced. 

%----------------------------------------------
\subsection{Markov Chain}\label{sub:bc_mcmc_mc}
%----------------------------------------------
%once more the posterior \gls[hyper=false]{pdf} of the model parameters conditioned on the observed data.
%Strictly speaking such a \gls[hyper=false]{pdf} will also be conditioned on a selected data-generating process model $\mathcal{M}$, i.e., $p(\bm{x}|\mathbf{y},\mathcal{M})$.
%However, in the following discussion, this conditioning is implicitly assumed and removed from the notation yielding
%\begin{equation}
%	p(\bm{x} | \mathbf{y}) = \frac{p(\bm{y} = \mathbf{y} | \bm{x}) p(\bm{x})}{\int p(\mathbf{y} | \bm{x}) p(\bm{x}) d\bm{x}}
%\label{eq:pdf_posterior}
%\end{equation}
% Introductory paragraph (Why Markov Chain)

% Markov Chain Definition

% Theorem, Markov Chain and Stationary Distribution

%------------------------------------------------------------
\subsection{Markov Chain Monte Carlo}\label{sub:bc_mcmc_mcmc}
%------------------------------------------------------------

% Introductory Paragraph

% Metropolis-Hastings Algorithm

% Simple Example

% Different Samplers

%---------------------------------------------------------------------
\subsection{Affine-Invariant Ensemble Sampler}\label{sub:bc_mcmc_aies}
%---------------------------------------------------------------------

% Introductory Paragraph

% Affine-Invariant Ensemble Sampler, Motivation

% Affine-Invariant

% Ensemble Sampler

% AIES Algorithm

% Implementation

% Possible problem and Why not