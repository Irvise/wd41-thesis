\newpage
%************************************************************************************
\section[MCMC Simulation]{\glsfirst[hyper=false]{mcmc} Simulation}\label{sec:bc_mcmc}
%************************************************************************************

% Introductory Paragraph
The formulation of Bayesian calibration of a computer model presented above results in a joint posterior \gls[hyper=false]{pdf} for all the parameters involved in the model.
\marginpar{Posterior uncertainty of the model parameters}
This density contains all the information (and consequently, the uncertainties) regarding the model parameters conditioned on the observed data and the assumed data-generating process.
The uncertainties associated with the model parameters can then be represented using different summary statistics, many of which involve integration.

For example,
the uncertainties associated with a model parameter $x_d$ can be represented by its variance,
which is defined as
\begin{equation*}
	\begin{split}
	\mathbb{V}[\mathcal{X}_d] & = \mathbb{E}[\mathcal{X}_d^2] - \mathbb{E}^2[\mathcal{X}_d]\\
	                          & = \int_{-\infty}^{\infty} x_d^2 p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} x_d p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
	\end{split}
%\label{eq:variance_marginalization}
\end{equation*}
where the integrations are carried out over the support of $p(\bm{x}|\mathbf{y})$, assumed to be on the entire real line.
An alternative way to summarize the uncertainties of a model parameter is through its $\theta$-quantile $Q^{\theta}_d$,
which for parameter $x_d$ is defined as
\begin{equation*}
	Q^{\theta}_d \, : \, \mathbb{P} (\mathcal{X}_d \leq Q^{\theta}_d) = \int^{Q^{\theta}_d}_{-\infty} \int\limits_{\mathbf{X}_{\sim d}} p(x_d, \bm{x}_{\sim d} | \mathbf{y}) d\bm{x}_{\sim d} d x_d = \theta
%	\label{eq:quantile_integral}
\end{equation*}
Assuming that the support of $p(\bm{x}|\mathbf{y})$ is on the entire real line.
In this manner,
the $95\%$ confidence interval of the parameter is written as $Q_d^{0.025} \leq \mathcal{X}_d \leq Q_d^{0.975}$.

Though these summaries might be of interest themselves,
\marginpar{Posterior uncertainty of the model prediction}
in an application setting, the model parameters uncertainties are often propagated through the simulation model to obtain the uncertainty in the output (prediction).
Hence, the output from a simulation model $y = f(\bm{x})$ is expressed as random variable $\mathcal{Y}$ from a transformation of random variable $\bm{\mathcal{X}}|\mathbf{y}$ by the function $f$
\begin{equation*}
	\mathcal{Y} = f(\bm{\mathcal{X}} | \mathbf{y})  \, ; \, p_{\bm{\mathcal{X}} | \mathbf{y}} (\bm{x}) = p(\bm{x}| \mathbf{y})  
\end{equation*}
where the \gls[hyper=false]{pdf} of $\bm{\mathcal{X}} | \mathbf{y}$ is the posterior density $p(\bm{x}| \mathbf{y})$.
The actual \gls[hyper=false]{pdf} of $\mathcal{Y}$ follows the rule of transformation of random variable
and it represents the uncertainty in the output due to the uncertainty in the input parameter conditioned on the data.
This uncertainty can also be summarized with various statistics and, as before, many of which involve integration operation.
For instance, the variance of the output, is written as
\begin{equation*}
	\mathbb{V}[\mathcal{Y}] = \int_{-\infty}^{\infty} f^2(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} - \left( \int_{-\infty}^{\infty} f(\bm{x}) p(\bm{x} | \mathbf{y}) d\bm{x} \right)^2
\end{equation*}

The posterior density $p(\bm{x}|\mathbf{y})$ and the function $f(\bm{x})$, however, are in practice highly multidimensional functions and 
the performance of numerical integration is typically worsened with an increasing number of dimensions of the input parameter space.
\marginpar{Challenges in dealing with posterior density}
At the same time,
conducting \gls[hyper=false]{mc} simulation for estimating the integrals (such was done in Chapter~\ref{ch:gsa}) is not straightforward in this case.
The multiplication of likelihood and prior density will, in general, yield an arbitrary posterior density not available in a closed-form expression.
As a result, generating independent samples from the posterior density required for the \gls[hyper=false]{mc} estimation becomes a difficult task.

This section presents a simulation approach, the so-called \gls[hyper=false]{mcmc} simulation,
to directly generate samples from an arbitrary \gls[hyper=false]{pdf}. 
These samples, in turn, are useful for estimating various quantities given as examples above independent on the dimension of the input parameter space.

Although in the context of Bayesian data analysis the \gls[hyper=false]{pdf} of interest is the posterior \gls[hyper=false]{pdf} \cite{Tierney1994},
the problem of generating samples from an arbitrary \gls[hyper=false]{pdf} is general.
As such, in the following discussion, a generic notation for an arbitrary \gls[hyper=false]{pdf} $p(\bm{x})$ is used instead of $p(\bm{x}|\mathbf{y})$.

%----------------------------------------------------
\subsection{Motivation}\label{sub:bc_mcmc_motivation}
%----------------------------------------------------

% Introductory Paragraph
Consider the following problem: Given a \gls[hyper=false]{pdf} $p:\mathbf{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}^+$,
generate a set of samples $\{\bm{x}_n\}_{n=1}^N$ from the \gls[hyper=false]{pdf}.
\marginpar{Problem statement}
It is assumed that the \gls[hyper=false]{pdf} can be evaluated at any given $\bm{x} \in \mathbf{X}$, at least up to a proportionality constant:
\begin{equation}
	p(\bm{x}) = \frac{p^*(\bm{x})}{C} \propto p^*(\bm{x})  
\label{eq:bc_prop_post}
\end{equation}
The proportionality constant in the above equation is the normalizing constant such that $p$ is a valid \gls[hyper=false]{pdf},
\begin{equation}
	C = \int p^*(\bm{x}) d\bm{x}  \Rightarrow \int p(\bm{x}) d\bm{x} = 1.0
\label{eq:bc_prop_const}
\end{equation}
Such samples can then be used, among other things,
to evaluate different summary statistics (such as expectation, variance, etc.) of $\bm{x}$ itself or
of any function under the \gls[hyper=false]{pdf}.

% Why it it difficult to samples
Generating samples from an arbitrary multidimensional density function is generally a difficult task.
\marginpar{A correct sampling}
Intuitively, for a given sample size, correctly generating samples from a density means
that the sample values have to be distributed proportional to its \gls[hyper=false]{pdf}.
There should be more samples in the region where the \gls[hyper=false]{pdf} value is high,
and less in the the region where the \gls[hyper=false]{pdf} value is low.
For a complex multidimensional density function,
these locations are not known a priori and might have to be identified exhaustively \cite{Mackay2005}.

% Inverse Transform Sampling and Its Problem
In one dimension,
the most common way of generating sample from a given density is
by inverse transform sampling coupled with a random number generator.
\marginpar{Inverse transform sampling}
The approach requires the quantile function of the \gls[hyper=false]{pdf}.
To obtain the quantile function,
the density has to be integrated and its normalizing constant has to be computed.
Appendix~\ref{app:its} provides a more detail account on the topic.
Many univariate random variables are widely studied and the analytical solutions to their quantile functions are available \cite{Lange2010}.
However, the method is not extendable to distributions of higher dimension.
Additionally, though sampling algorithms exist for several multivariate densities (notably, the multivariate normal density in Appendix~\ref{app:mvn_sampling}),
this will not be the case for an arbitrary density function of higher dimension.

% Illustration
To illustrate this point,
\marginpar{Illustration}
consider the following bivariate (unnormalized) density parameterized by location parameters $\mu_1, \mu_2$ and scale parameters $\sigma_1, \sigma_2$ \cite{Balakrishnan2014}:
\begin{equation}
	\begin{split}
	& p^*(x_1, x_2) = \frac{\exp{(-(x_1 - \mu_1)/\sigma_1)} \exp{(-(x_2 - \mu_2)/\sigma_2)}}{(1 + \exp{(-(x_1 - \mu_1)/\sigma_1)} + \exp{(-(x_2 - \mu_2)/\sigma_2)})^3} \, \\ 
	& x_1, x_2 \in \mathbb{R}; \mu_1, \mu_2 \in \mathbb{R};\, \text{and} \, \sigma_1, \sigma_2 \in \mathbb{R}^+
	\end{split}
\label{eq:bc_unnormalized_gumbel}
\end{equation}
Fig.~\ref{fig:ch5_plot_gumbel_illustration} shows the contour plot of the joint density as well as the marginal density for each of the variate.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_gumbel_illustration},
                  maincaption={Joint and marginal densities plots of the unnormalized \gls[hyper=false]{pdf} in the example. The parameters used in the example are: $\mu_1 = 5, \mu_2 = 2, \sigma_1 = 1.25$, and $\sigma_2 = 3$.},%
									mainshortcaption={Joint and marginal densities plots for the unnormalized \gls[hyper=false]{pdf} in the example.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_gumbel_illustration_1},
                  leftcaption={Joint density},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_gumbel_illustration_2},
                  rightcaption={Marginal density},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotGumbelIllustration_1}
{../figures/chapter5/figures/plotGumbelIllustration_2}

% Grid Approach
A straightforward approach to generate samples from a given multivariate density is done by first 
\marginpar{Discretized grid approach}
discretizing the input parameter space of the density function and evaluate the density at the discretized points.
Supposed the domain of the density has been discretized uniformly in each dimension with a level $\Delta$ resulting in $\{\bm{x}_i\}_{i=1}^{I}$ with $I$ the number of discretized points.
At the discretized levels, the probability for each value of $\bm{x}_i$ is approximated by $p(\bm{x}_i) = p^*(\bm{x}_i) / \sum_i p^*(\bm{x}_i)$\footnote{strictly speaking, each density value has to be multiplied by the hypervolume of the grid to obtain the probability mass, but the term cancels out in computing $p$.)}.
The pairs constitute a complete discrete probability distributions.
Generating samples from such a probability distribution is straightforward in a modern computing environment \cite{Mackay2005}.

% The example
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid} illustrates this procedure for the example given above.
First, the input parameter space is windowed in $\mathbf{X}\in[-25,25]^2$ before being discretized in $\Delta = 50$ levels.
\marginpar{Discretized grid approach illustrated}
This results in $2'601$ discretized points at which the density is evaluated (Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1}).
Next, the density values are taken to be the probability for each of the $2'601$ discretized points.
Together they make up a complete discrete probability distribution from which samples can be readily generated.

% The figure explained
Fig.~\ref{fig:ch5_plot_gumbel_sample_grid_1} shows $5'000$ samples generated from the discrete distribution.
Darker points indicate that the values have been sampled multiple times following the actual underlying \gls[hyper=false]{pdf} (the contour of the analytical joint density is overlaid). 
Figs.~\ref{fig:ch5_plot_gumbel_sample_grid_2} and~\ref{fig:ch5_plot_gumbel_sample_grid_3} show the histograms for each of the marginals.
The figure shows that the generated samples are indeed approximately distributed as the given \gls[hyper=false]{pdf}.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_gumbel_sample_grid},
			           maincaption={Sampling from a multivariate density by discretizing the input parameter space in grids. The input parameter space is discretized into $\Delta = 50$ levels. The density is then evaluated at the discretized points. (Left) $5'000$ samples are generated following the resulting discrete probability distribution; (Center and Right) The histograms of the marginals approximately follow the shape of the respective analytical marginal density. The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Sampling from a multivariate density by discretizing the input parameter space in grids.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_gumbel_sample_grid_1},
			           leftcaption={Joint samples},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_gumbel_sample_grid_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_gumbel_sample_grid_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotGumbelSampleGrid_1}
{../figures/chapter5/figures/plotGumbelSampleGrid_2}
{../figures/chapter5/figures/plotGumbelSampleGrid_3}

% Problem with Discretized Grid
The main issue with the discretized grid approach, conceptually simple as it is,
is the curse of dimensionality similar to the one mentioned in the previous chapters.
\marginpar{curse of dimensionality}
The number of density evaluations grows exponentially with the number of dimension.
As a rule, for a given discretization level $\Delta$ and dimension $D$,
the number of density evaluations is $(\Delta+1)^D$.

% In relation to Bayesian data analysis
In the context of Bayesian data analysis,
the multidimensional likelihood function inside the posterior generally can be a complex function.
In consequence, a very fine discretization level might be required to appropriately capture the function behavior at important regions which are unknown a priori.
On top of that, the computational cost of evaluating the (complex) posterior density becomes nonnegligible.
As such, except for a very simple likelihood function and/or in a very low dimension,
grid approach is deemed inapplicable for generating samples from an arbitrary multidimensional density.

% Entry to Markov Chain
To circumvent these issues,
a sampling technique based on the theory of stochastic process is widely adopted.
\marginpar{Markov Chain Monte Carlo}
Specifically, by constructing a Markov chain of the input parameters values,
the resulting process will eventually converge to a stationary distribution which coincides with the given \gls[hyper=false]{pdf} (i.e., \emph{target density}).
In other words, the samples generated from the stationary distribution of the process are distributed according to the given density.
Generating samples for the purpose of \glsfirst[hyper=false]{mc} simulation by simulating a Markov chain is termed \glsfirst[hyper=false]{mcmc}.
Theoretically, this family of techniques would have less severe dependence to the dimension of the input parameter space and its convergence is guaranteed.

The following briefly presents the basics of Markov chain and its importance in solving the the problem of generating samples from an arbitrary density.
Then,
two methods to construct a Markov chain for the purpose of \gls[hyper=false]{mc} simulation are introduced. 

%----------------------------------------------
\subsection{Markov Chain}\label{sub:bc_mcmc_mc}
%----------------------------------------------
%once more the posterior \gls[hyper=false]{pdf} of the model parameters conditioned on the observed data.
%Strictly speaking such a \gls[hyper=false]{pdf} will also be conditioned on a selected data-generating process model $\mathcal{M}$, i.e., $p(\bm{x}|\mathbf{y},\mathcal{M})$.
%However, in the following discussion, this conditioning is implicitly assumed and removed from the notation yielding
%\begin{equation}
%	p(\bm{x} | \mathbf{y}) = \frac{p(\bm{y} = \mathbf{y} | \bm{x}) p(\bm{x})}{\int p(\mathbf{y} | \bm{x}) p(\bm{x}) d\bm{x}}
%\label{eq:pdf_posterior}
%\end{equation}

% Introductory paragraph (Why Markov Chain)
Markov chain is an example of a \emph{discrete-time} stochastic process.
Recall from Chapter~\ref{ch:gp_metamodel} that stochastic process is a collection of random variables $\{\mathcal{X}^{(i)}, i \in I\}$ where $I$ is an index set.
\marginpar{A discrete-time  stochastic process}
The term \emph{discrete-time} refers to the fact that the possible values of the index set $I$ is restricted to being discrete.
Moreover, the term \emph{time} is used by convention but by no means it is exclusively referred to the physical time.
In this thesis, a more fitting alternative term would be \emph{step} or \emph{iteration}. 

% Markov Chain Definition
Specifically, Markov chain with state space $\mathcal{S} \subseteq \mathbf{X} \subseteq \mathbb{R}^D$ is defined as a sequence of random variables $\{\bm{\mathcal{X}}^{(i)}, i \geq 0\}$ where the indices represents successive time, step, or iteration,
\emph{such that the conditional probability distribution $\bm{\mathcal{X}}^{(i+1)}$ follows the Markov assumption}.
\marginpar{Markov chain}
That is,
\begin{equation}
  \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} | \bm{\mathcal{X}}^{(i)}, \bm{\mathcal{X}}^{(i-1)}, \ldots, \bm{\mathcal{X}}^{(0)}) = \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} | \bm{\mathcal{X}}^{(i)})
\label{eq:markov_property}
\end{equation}
Put differently, the future value depends on the past only through the present value \cite{Geyer2011} and Sokal.

% Specification and Ingredients, Initial Distribution and Transition Kernel
A Markov chain is fully specified by its joint probability distribution,
\begin{equation}
  \mathbb{P}(\bm{\mathcal{X}}^{(i+1)}, \bm{\mathcal{X}}^{(i)}, \ldots, \bm{\mathcal{X}}^{(0)}) = \mathbb{P}(\bm{\mathcal{X}}^{(0)}) \cdot \mathbb{P}(\bm{\mathcal{X}}^{(1)} | \bm{\mathcal{X}}^{(0)}) \cdot \ldots \cdot \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} | \bm{\mathcal{X}}^{(i)}) \cdot
\label{eq:markov_chain_joint_probability}
\end{equation}
This joint probability, in turn, consists of two main components:
\begin{itemize}
	\item The \emph{initial distribution} of $\bm{\mathcal{X}}^{(0)}$. This distribution is the marginal probability distribution of $\bm{\mathcal{X}}^{(0)}$.
	
	\item The \emph{transition probability kernel} $T(\bm{\mathcal{X}}^{(i)}, \bm{\mathcal{X}}^{(i+1)})$.
	This kernel is equivalent to the conditional distribution of $\bm{\mathcal{X}}^{(i+1)}$ given $\bm{\mathcal{X}}^{(i)}$. That is,
\begin{equation}
  \bm{\mathcal{X}}^{(i+1)} | \bm{\mathcal{X}}^{(i)}, \ldots, \bm{\mathcal{X}}^{(0)} = \bm{\mathcal{X}}^{(i+1)} | \bm{\mathcal{X}}^{(i)} \sim T(\bm{\mathcal{X}}^{(i)}, \bm{\mathcal{X}}^{(i+1)})
\label{eq:markov_chain_transition_kernel}
\end{equation}
	such that,
	\begin{equation}
  \mathbb{P}(\bm{\mathcal{X}}^{(i+1)} \in B | \bm{\mathcal{X}}^{(i)} = \bm{x}^{(i)}) = \int_B T(\bm{x}^{(i)}, \bm{x}) d\bm{x} \, ; \, B \in \mathcal{S}
\label{eq:markov_chain_transition_probability}
\end{equation}
	In the continous-state Markov chain, this distribution is referred to as the \emph{transition kernel}.

\end{itemize}

% Stationary Distribution, Irreducibility
Central to the application of Markov chain for generating samples are the notion of stationary distribution of a Markov chain.
Stationary distribution is an important concept in the practical application.

% Convergence Theorem
	It is often assumed in practice that the transition probability is stationary such that it does not depend on $i$.

% Ergodic Theorem

% Limits and Ergodic Theorem, Markov Chain and Stationary Distribution
The theorem of Markov chain convergence states that if this exist.
The theorem is put in more detail in Appendix~\ref{app:markov_chain}.
It is suffice to say here that the goal of Markov chain monte carlo is to construct a chain through some transition probability such that its stationary distribution is the target density.

% Reference to the appendix

%------------------------------------------------------------
\subsection{Markov Chain Monte Carlo}\label{sub:bc_mcmc_mcmc}
%------------------------------------------------------------

% Introductory Paragraph
Consider once more the problem set up at the start of the section:
\marginpar{The objective revisited}
Generate samples from a target probability distribution with a density $p(\bm{x})$,
known at least up to a proportionality constant, $p(\bm{x}) \propto p^*(\bm{x})$.

% Markov Chain Monte Carlo
Acknowledging the fundamental theorem of Markov Chain (see Appendix~\ref{app:markov_chain}),
the task is then to construct a Markov transition kernel such that the target density $p$ becomes the
stationary distribution of the Markov chain.
\marginpar{MCMC algorithms}
Thereafter, based on such kernel, generate a realization of the chain (i.e., \emph{simulate}) long enough for the limiting distribution of the chain is reached and it converges to the target density $p$.
As a result, the samples generated from the realization of the chain converges, in distribution, to the target density.
This, in essence, is the objective of \gls[hyper=false]{mcmc} algorithms as defined in \cite{Robert2004}.

% Metropolis-Hastings Algorithm, Origin
One might think that the task of constructing a Markov transition kernel would be difficult,
especially considering wide range of possible target distribution which might call for different classes of transition kernels.
\marginpar{Metropolis-Hastings algorithms, origin}
However, there exists a class of algorithms for generating Markov chain that guarantees its convergence (in distribution) to \emph{any} target distribution as its stationary distribution\footnote{see \cite{Robert2010,Geyer2011} for more rigorous treatment on the convergence properties.}.
The Metropolis-Hastings algorithm and its various extensions remains the most universal class of algorithms to generate such Markov chain \cite{Robert2010}. 
The method was first applied for a statistical mechanics problem by Metropolis et al. \cite{Metropolis1953}\footnote{There is apparently a controversy surrounding the attribution of the algorithm solely to Metropolis, especially when his role was claimed to be nothing more ``other than providing computer time'' \cite{Gubernatis2005}.}
and later generalized by Hastings \cite{Hastings1970}\footnote{There is, to the best of the author's knowledge, no controversy here.}.

% Metropolis-Hastings Algorithm, In essence
The \gls[hyper=false]{mh} algorithm prescribes two main components for constructing transition kernel of a Markov chain that converges to the target distribution: a \emph{proposal probability density} $q(\bm{x}^*|\bm{x})$ and an \emph{acceptance probability} $\alpha$.
\marginpar{Metropolis-Hastings algorithm, proposal density}
The proposal probability density is responsible for generating a proposal transition or candidate move $\bm{x}^*$ for the Markov chain at each iteration and it is in general a density conditional on the previous state.
This density is chosen such that it is easier to sample and indeed it is often selected from well-known densities such the Gaussian or uniform densities.
This proposal move, in turn, will be accepted with a probability,
\begin{equation}
	\alpha = \text{min} \left(\frac{p(\bm{x}^*)}{p(\bm{x}^{(i-1)})} \times \frac{q(\bm{x}^{(i-1)} | \bm{x}^*)}{q(\bm{x}^* | \bm{x}^{(i-1)})}, 1.0\right)
\label{eq:ch5_acceptance_probability}
\end{equation}
\marginpar{Metropolis-Hastings algorithm, acceptance probability}
Where $p(\bm{x}^*)$ and $p(\bm{x})$ are the values of the target density at the proposed state and the previous state, respectively;
and $q(\bm{x}^{(i-1)} | \bm{x}^*)$ ($q(\bm{x}^* | \bm{x}^{(i-1)})$) is the value of the proposal density at the previous (proposed) state conditional on the value of the proposed (previous) state.
Notice from the ratio, the proportionality constant in Eq.~(\ref{eq:bc_prop_const}) cancels out and only the density up to that constant $p^*$ is required. 
The acceptance probability is formulated to satisfy the \emph{detailed balance condition} (Appendix~\ref{app:markov_chain}) for any valid proposal probability distribution. This, in turn, guarantees the stationarity of the process \cite{Chib1995}.

If the proposal move is accepted it becomes the current state of the chain,
otherwise the chain remains at its current state for the given iteration.
To generate a Markov chain of certain length (i.e., certain number of samples),
the steps are repeated multiple times until the required length of the chain is met.
Algorithm~\ref{alg:metropolis_hastings} summarizes the steps for constructing a Markov chain by \gls[hyper=false]{mh} algorithm.
\begin{algorithm}
\caption[Metropolis-Hastings Algorithm]{Metropolis-Hastings Algorithm \\ Generate samples from $p(\bm{x}) \propto p^*(\bm{x})$ given proposal density $q (\bm{x}^* | \bm{x})$ in $I$ iterations}
\label{alg:metropolis_hastings}
\begin{algorithmic}
  \REQUIRE $I > 0$, $p^*(\bm{x})$, and $q(\bm{x}^* | \bm{x}^{(i-1)})$
  \STATE $\bm{x}^{(0)} \leftarrow \bm{x}_0 \, ; \, \forall \bm{x}_0 \in \mathbf{X}$
  \FOR{$i = 1$ to $I$}
    \STATE sample $\bm{x}^*$ from $q(\bm{x} | \bm{x}^{(i-1)})$
    \STATE $\alpha \leftarrow \text{min} \left(\frac{p^*(\bm{x}^*)}{p^*(\bm{x}^{(i-1)})} \times \frac{q(\bm{x}^{(i-1)} | \bm{x}^*)}{q(\bm{x}^* | \bm{x}^{(i-1)})}, 1.0\right)$
    \STATE sample $u$ from $\mathcal{U}[0,1]$
    \IF{$u < \alpha$}
      \STATE $\bm{x}^{(i)} \leftarrow \bm{x}^*$
    \ELSE
      \STATE $\bm{x}^{(i)} \leftarrow \bm{x}^{(i-1)}$
    \ENDIF
  \ENDFOR
\end{algorithmic}
\end{algorithm}

% Random Walk Markov Chain
In the original paper of Metropolis et al.,
the proposal distribution was chosen to be a symmetric distribution such that $q(\bm{x}^* | \bm{x}^{(i-1)}) = q(\bm{x}^{(i-1)} | \bm{x}^*)$.
\marginpar{Random walk \gls[hyper=false]{mh} algorithm}
As a result the terms associated with the proposal density in Eq.~(\ref{eq:ch5_acceptance_probability}) cancel each other.
Consequently, any proposal move that yields an ``improvement'' on the target density evaluation will be accepted, otherwise it will only be accepted according to its acceptance probability. 
This particularly simple \gls[hyper=false]{mh} algorithm results in a \emph{random walk} Markov chain
and it is termed the \emph{random walk} \gls[hyper=false]{mh} \cite{Robert2010}.  

% Application on the Simple Running Example
To illustrate the application of the \gls[hyper=false]{mh}, particularly the random walk \gls[hyper=false]{mh},
\marginpar{Random walk \gls[hyper=false]{mh} algorithm, illustrated}
for generating samples from an arbitrary target distribution,
consider again the example of generating samples from the density given in Eq.~(\ref{eq:bc_unnormalized_gumbel}).
For this example, the proposal distribution is chosen to be a bivariate normal with a variance (the scale parameter) of $2.0$ equal in both dimensions and without correlation.
The initial state of the chain $\bm{x}^{(0)}$ is set to be at the origin.

% Proposing Move
The first three iterations of the random walk \gls[hyper=false]{mh} algorithm is illustrated in Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration}.
At the first iteration (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_1}),
a proposal move is drawn from the bivariate normal distribution (centered at the origin).
The proposal move brings the state closer to the center of the target density, thus it is accepted (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_2}).
A new proposal move is generated from the bivariate normal centered at the newly accepted move.
This time, because the proposal move moves farther away from the center of the target density, it is rejected.
The chain remains at the current state and a new proposal move is drawn (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_iteration_3}).
Note that this kind of proposal move will not always be rejected outright but is subject to chance based on the acceptance probability.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain_iteration},
			           maincaption={Illustration of the first three iterations in Markov Chain simulation by random walk \gls[hyper=false]{mh} algorithm to sample the density given in Eq.~(\ref{eq:bc_unnormalized_gumbel}) whose contours showed in solid lines. The proposal density is an independent bivariate normal distribution with $\sigma^2 = 2.0$ whose contours showed in dashed lines, centered at current state.},
			           mainshortcaption={Illustration of iterations in Markov Chain simulation by random walk Metropolis-Hastings algorithm.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_iteration_1},
			           leftcaption={Iteration $1$},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_iteration_2},
			           midcaption={Iteration $2$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_iteration_3},
			           rightcaption={Iteration $3$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_2}
{../figures/chapter5/figures/plotIllustrateMarkovChainIteration_3}

% Traversing the Target Density and Trace Plot
By repeating those steps multiple time, the chain traverses the parameter space according to the target distribution.
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_1} illustrates the chain traversing the $2$-dimensional parameter space of the density of Eq.~(\ref{eq:bc_unnormalized_gumbel}) for the first $250$ iterations.
In the long run, the chain will spend more time in the regions where the density are high and less time where the density low\footnote{The goal of an \gls[hyper=false]{mcmc} algorithm is not, on the other hand, to obtain the parameter value which maximizes the target distribution, at least not only. It seeks to explore the parameter space in proportion to value of the density function \cite{Tierney1994}.}.
Therefore, the resulting samples generated by the chain will be distributed according to the target distribution.
Figs.~\ref{fig:ch5_plot_illustrate_markov_chain_1} and~\ref{fig:ch5_plot_illustrate_markov_chain_2} are the \emph{trace} plots for the chain after $50'000$ iterations.
\marginpar{trace plot}
A trace plot shows the evolution of the chain during the iterations and it is often the first graphical diagnostic tool to spot any possible issue of convergence of a Markov chain \cite{Robert2010}.
In this particular case, the plots show that the chain seemingly converges to particular region of the input parameter space and within this region the chain randomly moves from one state to another. 
It also indicates that $x_1$ are centered differently than $x_2$,
and $x_2$ has a relatively larger dispersion than $x_1$ (as can be seen also from the contour plot).
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain},
			           maincaption={Illustration of Markov Chain simulation to generate samples from a target density of Eq.~(\ref{eq:bc_unnormalized_gumbel}). (Left) The chain traverses the parameter space. At each iteration, a move is proposed and accepted in probabilistic manner. (Center and Right) The trace plots.},
			           mainshortcaption={Illustration of Markov Chain simulation to generate samples from a target density given in the example.},%
			           leftopt={width=0.28\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_1},
			           leftcaption={Trace plots in $2$-dimensional parameter plane (the first $250$ iterations)},
			           midopt={width=0.28\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_2},
			           midcaption={Trace plot for $x_1$},
			           rightopt={width=0.28\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_3},
			           rightcaption={Trace plot for $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChain_1}
{../figures/chapter5/figures/plotIllustrateMarkovChain_2}
{../figures/chapter5/figures/plotIllustrateMarkovChain_3}
%This also means that the goal of \gls[hyper=false]{mcmc} algorithm is not to obtain the values at which the target density is maximized, 

% Marginal Distribution
After the iterations are completed, the resulting samples should be distributed according to the target distribution.
Indeed this is the case for the \gls[hyper=false]{mcmc} simulation.
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_sample} shows that the resulting samples are distributed according to the target distribution both as a joint as well as its marginal.
The joint distribution, in particular, shows that the samples generated by \gls[hyper=false]{mcmc} simulation are correlated according to the correlation contain in the density of Eq.~(\ref{eq:bc_unnormalized_gumbel}).
Note that, in practice, the correct distribution of the resulting samples cannot simply be verified by comparing it to the analytical formula.
The whole point of generating samples via \gls[hyper=false]{mcmc} simulation is exactly because such  arbitrary high-dimensional distributions are hard to characterize.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_illustrate_markov_chain_sample},
			           maincaption={Results of samples generated by Markov chain simulation for the target density given in the example. After $50'000$ iterations, the samples resembles the actual shape of the distribution both as a joint (Left) and as marginal distributions (Center and Right). The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Results of samples generated by Markov chain simulation for the target density given in the example.},%
			           leftopt={width=0.28\textwidth},
			           leftlabel={fig:ch5_plot_illustrate_markov_chain_sample_1},
			           leftcaption={Joint samples},
			           midopt={width=0.28\textwidth},
			           midlabel={fig:ch5_plot_illustrate_markov_chain_sample_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.28\textwidth},
			           rightlabel={fig:ch5_plot_illustrate_markov_chain_sample_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_2}
{../figures/chapter5/figures/plotIllustrateMarkovChainSample_3}

% The Importance of Proposal Distribution in Metropolis-Hastings Algorithm
Although the theorems that underlie the application of \gls[hyper=false]{mcmc} algorithm guarantee the convergence of the chain to the target distribution,
its rate of convergence is problem dependent.
For many \gls[hyper=false]{mh} algorithms, the choice of proposal distribution is particularly important in determining the convergence rate of the algorithm in reaching the target distribution as its stationary distribution.
For instance, Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_large_step} illustrates the case where the scale parameter of the proposal distribution is set to be much larger than the actual scale of the target distribution.
\marginpar{Over-dispersed proposal distribution.}
As shown, because the proposal moves can jump from one side of the parameter space to another,
they are rarely accepted and the chain stucks at the same values for a long period.
For the same length of the chain as before ($50'000$), the resulting distribution of the samples (Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_large_step_2}) hardly resembles the target distribution.
% Over-dispersed step
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_markov_chain_large_step},
                  maincaption={Convergence issue due to an over-dispersed proposal distribution ($\sigma^2 =100.0$).},%
									mainshortcaption={Convergence issue due to an over-dispersed proposal distribution.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_illustrate_markov_chain_large_step_1},
                  leftcaption={Trace plot of $x_1$},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_illustrate_markov_chain_large_step_2},
                  rightcaption={Marginal of $x_1$},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotIllustrateMarkovChainLargeStep_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainLargeStep_2}

% Under-dispersed step
Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step} shows the behavior of the chain in the case of too small variance parameter of the proposal distribution, in comparison to the scale of the target density.
\marginpar{Under-dispersed proposal distribution}
In this case, any proposal move around the previously accepted state would almost always be accepted and the chain traverses the parameter space very slowly Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step_1}.
Consequently, the resulting samples from the chain (with the same length of $50'000$ samples in this example) would not be representative of the target distribution as illustrated in Fig.~\ref{fig:ch5_plot_illustrate_markov_chain_small_step_2}.
It is important to note that, in both cases, the chain would eventually converge in distribution for both parameters.
But this convergence would not be attained for a Markov chain of a practical length.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_markov_chain_small_step},
                  maincaption={Convergence issue due to an under-dispersed proposal distribution ($\sigma^2 = 0.01$).},%
									mainshortcaption={Convergence issue due to an under-dispersed proposal distribution.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_illustrate_markov_chain_small_step_1},
                  leftcaption={Trace plot of $x_2$},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_illustrate_markov_chain_small_step_2},
                  rightcaption={Marginal of $x_2$},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotIllustrateMarkovChainSmallStep_1}
{../figures/chapter5/figures/plotIllustrateMarkovChainSmallStep_2}

% Adaptive Sampler
In fact, the optimal choice of the scale parameters for the proposal distribution is closely related to the characteristic length scale of the target distribution \cite{Mackay2005}.
\marginpar{Tuning and adaptive \gls[hyper=false]{mh} algorithms}
Unfortunately, in practice, there would be little information, if any, on the characteristic length scale for each parameter of a given density.
As such, some sort of tuning is required regarding the proposal distribution for it to be optimal with respect to some chosen measures.
This is the main motivation for the development of various \emph{adaptive} \gls[hyper=false]{mh} algorithms.
In such algorithms, the transition kernels are adapted during iteration to optimize the performance of the algorithm \cite{Robert2010}.

% Entry to AIES
Instead of delving into any of the particular improvements on the \gls[hyper=false]{mh} algorithm (such as through the adaptive schemes)\footnote{Ref.~\cite{Andrieu2008} provides an overview of adaptive \gls[hyper=false]{mc} algorithms, while Refs.~\cite{Haario2001,Haario2006,Griffin2011} are examples of adaptive algorithms.},
this thesis adopted a relatively new \gls[hyper=false]{mcmc} algorithm based on an \emph{ensemble} Markov chain.
The algorithm has the potential of requiring minimal tuning to any given particular problem.
The main ideas of the above (i.e., proposal move and its acceptance probability), however,
remain central in an ensemble \gls[hyper=false]{mcmc} algorithm.

%----------------------------------------------------------------------------
\subsection{Affine-Invariant Ensemble Sampler (AIES)}\label{sub:bc_mcmc_aies}
%----------------------------------------------------------------------------

% Introductory Paragraph and main motivation
\Glsfirst[hyper=false]{aies} is an \gls[hyper=false]{mcmc} algorithm proposed by Goodman and Weare \cite{Goodman2010}.
Its main motivation is exactly where the previous section left off: the difficulty in tuning or adapting \gls[hyper=false]{mh} algorithm applicable to wide class of target distribution.
The situation is typically worsened for a highly correlated distribution of high dimension where some of the length scales of the target distribution are very small forcing an adaptive algorithm to spend majority of its time tuning the scale of the proposal distribution,
eventually resulting in high overhead computational cost of adaptation \cite{Foreman-Mackey2013}.

Through some affine transformations,
a target distribution with a highly skewed aspect ratio can be made less skewed and thus less awkward to be sampled from.
By ensuring the algorithm to be affine-invariance, the performance of the algorithm would then be equal under all affine transformation of the target distribution.
Finally, implementing such a transformation to an ensemble sampler\footnote{\emph{sampler of many particles}, as opposed to a sampler of \emph{single particle} in the conventional \gls[hyper=false]{mcmc} algorithm, such as the above \gls[hyper=false]{mh} algorithm.} results in an algorithm that requires minimal tuning with respect to each of the state space dimensions (as will be explained further below).

% Ensemble Sampler
\gls[hyper=false]{aies} belongs to a class of \gls[hyper=false]{mcmc} algorithms that generates a Markov chain on the state space of \emph{ensembles} (i.e., \emph{ensemble samplers}) \cite{Goodman2010}.
\marginpar{Ensemble sampler}
An ensemble $\vec{\bm{\mathcal{X}}}$ is a collection of $L$ random variables $\{\bm{\mathcal{X}}_{l}\}_{l = 1}^{L}$ called \emph{walkers}, each of which is in $\mathbb{R}^D$, that is $\bm{\mathcal{X}}_{l} = [\mathcal{X}_{l,1}, \mathcal{X}_{l,2}, \ldots, \mathcal{X}_{l, D}]$.
The ensemble of $L$ walkers are independent to each other with respect to the target distribution $p (\circ)$.
Specifically,
\begin{equation}
	p(\vec{\bm{x}}) = p(\bm{x}_1) \cdot p(\bm{x}_2) \ldots \cdot p(\bm{x}_L) 
\label{eq:ch5_ensemble_independence}
\end{equation}
Eq.~\ref{eq:ch5_ensemble_independence} implies that the target distribution is being independently sampled by $L$ walkers.

A Markov chain of an ensemble, in turn, is a sequence of ensembles, $\{\vec{\bm{\mathcal{X}}}^{(i)}\}$ for $i \geq 0$ that follows the Markov property while preserving the condition in Eq.~(\ref{eq:ch5_ensemble_independence}).
Consequently, the Markov property lies on the state space of the ensemble\footnote{if each walker is in $\mathbb{R}^D$ then an ensemble of $L$ walkers can be thought of to be in $\mathbb{R}^{DL}$.}
and the sequence of each walker $\{\bm{\mathcal{X}}_l^{(i)}\}$ itself needs not be Markovian \cite{Goodman2010}.

% Affine Transformation
In \gls[hyper=false]{aies}, the transition between states of an ensemble is conducted by carrying out an affine transformation to the ensemble.
\marginpar{Affine Transformation}
The transition (thus the transformation) is carried out at the level of individual walkers one at a time.
In other words, an affine transformation $f_a$ is defined such that, 
\begin{equation}
	\begin{split}
		 & f_a: \vec{\bm{\mathcal{X}}} \mapsto \vec{\bm{\mathcal{Y}}} = M \vec{\bm{\mathcal{X}}} + b \\
		 & \vec{\bm{\mathcal{Y}}} = [M \bm{\mathcal{X}}_1 + b, M \bm{\mathcal{X}}_2 + b, \ldots, M \bm{\mathcal{X}}_L + b]
	\end{split}
\label{eq:ch5_affine_transformation}
\end{equation}
In the context of \gls[hyper=false]{mcmc} simulation,
$\vec{\bm{\mathcal{Y}}}$ would represent the proposal move of the chain transition whose acceptance is subject to chance as will be discussed further below.

% Affine Invariance

% AIES Algorithm (Stretch Move)
One particular implementation of an \gls[hyper=false]{aies} \gls[hyper=false]{mcmc} algorithm is the so-called \emph{stretch-move} \cite{Goodman2010,Foreman-Mackey2013}.
As mentioned, the transition between iterations of an ensemble starts at the level of individual walkers.
That is, the update is carried out one walker at a time and for stretch-move it proceeds as follows.

Let $\bm{\mathcal{X}}_l^{(i-1)}$ be the walker $l$ at iteration $(i-1)$, and is to be updated.
\marginpar{Complementary ensemble}
Let $\vec{\bm{\mathcal{X}}}_{\sim l}^{(i-1)}$, called a \emph{complementary ensemble},
be the ensemble of walkers at iteration $i$,
complementary to the walker $\bm{\mathcal{X}}_l^{(i-1)}$.
Specifically,
\begin{equation}
	\vec{\bm{\mathcal{X}}}_{\sim l}^{i-1} = [\bm{\mathcal{X}}_{1}^{(i)}, \ldots, \bm{\mathcal{X}}_{l-1}^{(i)}, \bm{\mathcal{X}}_{l+1}^{(i-1)}, \ldots, \bm{\mathcal{X}}_{L}^{(i-1)}]
\label{eq:ch5_aies_complementary_ensemble}
\end{equation}
where all the walkers $k < l$ have been updated to their respective new states.
Finally, let $\vec{\bm{x}}^{(0)} = [\bm{x}^{(0)}_1, \ldots, \bm{x}^{(0)}_L]$ be the initial state of the chain, arbitrarily chosen within the support of $\bm{\mathcal{X}}$.

% Proposal move
In transitioning the ensemble to $\vec{\bm{x}}^{(i)}$,
a proposal move is made on one walker at a time and it follows an affine transformation:
\marginpar{Proposal move}
\begin{equation}
	\bm{x}_l^* = \bm{x}_j + z (\bm{x}_l^{(i-1)} - \bm{x}_j)
\label{eq:ch5_proposal_move_stretch_move}
\end{equation}
where $\bm{x}_l^*$ is the proposal move for the walker $l$ at the current iteration;
$\bm{x}_l^{(i-1)}$ is the walker to be updated (i.e., walker $l$ at the previous state $(i-1)$);
$\bm{x}_j$ is the \emph{complementary walker}, randomly selected from the complementary ensembles of $\vec{\bm{x}}^{(i-1)}_{\sim l}$;
and $z$ is the scaler of the transformation (i.e., the \emph{stretcher}), randomly generated from,
\begin{equation}
	g(z) \propto z^{-0.5}, \,\,\,\, z \in [a^{-1}, a]
\label{eq:ch5_scaler_distribution}
\end{equation}
where $a$ is a free positive parameter,
and where the value of $2.0$ is widely used as default for many applications \cite{Goodman2010,Hou2012,Allison2013,Foreman-Mackey2013,Akeret2013}.

% Stretch-Move, Illustrated
Fig.~\ref{fig:ch5_aies_stretch} illustrates how a move is proposed in stretch-move for a single walker in an ensemble of $10$ walkers, in a $2$-dimensional state space.
\marginpar{Stretch-move, illustrated}
First, a walker in the ensemble will be updated while the rest of the walkers becomes its complementary walkers (Fig.~\ref{fig:ch5_aies_stretch_1}).
Secondly, a complementary walker is randomly selected among the ensemble of complementary walkers (Fig.~\ref{fig:ch5_aies_stretch_2}).
Thirdly and finally,
a proposal scaler is randomly generated according to $q$ and a proposed move is made according to Eq. (Fig.~\ref{fig:ch5_aies_stretch_3}).
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_aies_stretch},
			           maincaption={Illustration of a \emph{stretch move} update for a single walker in a $2$-dimensional state space.},
			           mainshortcaption={Illustration of a \emph{stretch move} update for a single walker in a $2$-dimensional state space.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_aies_stretch_1},
			           leftcaption={A walker in an ensemble of complementary walkers},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_aies_stretch_2},
			           midcaption={A complementary walker is randomly selected},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_aies_stretch_3},
			           rightcaption={A move is proposed},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/aies_stretch_1}
{../figures/chapter5/figures/aies_stretch_2}
{../figures/chapter5/figures/aies_stretch_3}

% Acceptance probability
The probability of accepting this proposal move is given by,
\marginpar{Acceptance probability}
\begin{equation}
	\alpha = \text{min} \left(\frac{p(\bm{x}_l^*)}{p(\bm{x}_l^{(i-1)})} \times z^{D-1}, 1.0\right)
\label{eq:ch5_acceptance_probability_stretch_move}
\end{equation}
where $p^*(\bm{x}_l^{*})$ and $p^*(\bm{x}_l^{(i-1)})$ are the values of the (unnormalized) target density at the proposed and previous states, respectively;
and $D$ is the dimension of the state space (or the dimension of the input parameter space).
As with the \gls[hyper=false]{mh} algorithm,
if accepted the proposal move becomes the current state of the walker; otherwise, it remains in the previous state.
The steps are then repeated for the current iteration until all the walkers in the ensemble have been updated.
Algorithm~\ref{alg:aies} summarizes the steps in the stretch-move \gls[hyper=false]{aies} \gls[hyper=false]{mcmc} algorithm.
\begin{algorithm}
\caption[Affine-Invariance Ensemble Sampler (Stretch-Move)]{Affine-Invariant Ensemble Sampler (Stretch Move)\\ Generate samples from $p(\bm{x}) \propto p^*(\bm{x})$ using $L$ walkers in $I$ iterations.}
\label{alg:aies}
\begin{algorithmic}
  \REQUIRE $I > 0$, $L \geq I + 1$, $p^*(\bm{x})$, and $q (z)$
		\STATE $\vec{\bm{x}}^{(0)} = [\bm{x}^{(0)}_1, \ldots, \bm{x}^{(0)}_L] \leftarrow \vec{\bm{x}}_0 \, ; \, \forall \vec{\bm{x}}_0 \in \mathbf{X}^L$
		\FOR{$i = 1$ to $I$}
			\FOR{$l = 1$ to $L$}
				\STATE pick randomly $\bm{x}_j$ from $\vec{\bm{x}}^{(i-1)}_{\sim l}$
				\STATE sample $z$ from $q(z)$
				\STATE $\bm{x}^* \leftarrow \bm{x}_j + z (\bm{x}^{(i-1)}_l - \bm{x}_j)$
				\STATE $\alpha \leftarrow \text{min} \left(\frac{p^*(\bm{x}^*)}{p^*(\bm{x}^{(i-1)}_l)} \times z^{D-1}, 1.0\right)$
			\STATE sample $u$ from $\mathcal{U}[0,1]$
			\IF{$u < \alpha$}
				\STATE $\bm{x}^{(i)}_l \leftarrow \bm{x}^*$
			\ELSE
				\STATE $\bm{x}^{(i)}_l \leftarrow \bm{x}^{(i-1)}_l$
			\ENDIF
			\ENDFOR
		\ENDFOR
\end{algorithmic}
\end{algorithm}

% Intuition
This is because the evolution of an ensemble across the state space inherently carried more information (more than an evolution of single chain in ) on the landscape of the target distribution for each of the state space dimensions.

\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_markov_chain_small_step},
                  maincaption={Convergence issue due to an under-dispersed proposal density.},%
									mainshortcaption={Convergence issue due to an under-dispersed proposal density. The marginal densities have been normalized to match the peak of the histogram.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_aies_walker_ensemble_1},
                  leftcaption={Trace plot of all walkers},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_aies_walker_running_stats_1},
                  rightcaption={Running empirical mean and standard deviation of the samples},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotAIESWalkerEnsemble_1}
{../figures/chapter5/figures/plotAIESWalkerRunningStats_1}

% Implementation


% Running Example
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_aies_walker_2},
                  maincaption={Convergence issue due to an under-dispersed proposal density.},%
									mainshortcaption={Convergence issue due to an under-dispersed proposal density. The marginal densities have been normalized to match the peak of the histogram.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_aies_walker_ensemble_2},
                  leftcaption={Trace plot of all walkers},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_aies_walker_running_stats_2},
                  rightcaption={Running empirical mean and standard deviation of the samples},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/plotAIESWalkerEnsemble_2}
{../figures/chapter5/figures/plotAIESWalkerRunningStats_2}

\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_plot_aies_walker_samples},
			           maincaption={Results of samples generated by \gls[hyper=false]{aies}. After $500$ iterations of an ensemble of $100$ walkers (for a total $50'000$ target density evaluations), the samples resembles the actual shape of the distribution both as a joint distribution (Left) and as marginal distributions (Center and Right). The marginal densities have been normalized to match the peak of the histogram.},
			           mainshortcaption={Results of samples generated by \gls[hyper=false]{aies} for the target density given in the example.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_plot_aies_walker_samples_1},
			           leftcaption={Joint samples},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_plot_aies_walker_samples_2},
			           midcaption={Marginal of $x_1$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_plot_aies_walker_samples_3},
			           rightcaption={Marginal of $x_2$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/plotAIESWalkerSamples_1}
{../figures/chapter5/figures/plotAIESWalkerSamples_2}
{../figures/chapter5/figures/plotAIESWalkerSamples_3}


% Possible problem and Why not
