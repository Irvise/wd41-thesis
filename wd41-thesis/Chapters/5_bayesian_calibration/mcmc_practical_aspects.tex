%******************************************************************************************************************************
\section[Diagnosing Convergence]{Diagnosing Convergence of an \gls[hyper=false]{mcmc} Simulation}\label{sec:bc_mcmc_diagnostic}
%******************************************************************************************************************************

% With a minimal tuning to deal with in AIES algorithm, we are left with the problem of assessing convergence.
% In particular two key issues persists
% The previous discussion spoke rarely about the condition.
% It was demonstrated, through graphical representations the main idea of convergence in distributin.
% This section seeks to answer, the following two important questions of Markov chain 
% For an arbitrary starting distribution, when the chain can be considered stationary
% It is not to say that these initial part of the chain is not part of the target distribution support.
% They are, but as a distribution they are heavily biased.
% Assuming a stationary distribution have been reached, how long the chain should be run to meet certain level of accuracy

% Opening paragraph
Consider once more Eq.~(\ref{eq:ch5_markov_chain_clt}), the central limit theorem (CLT) for the \gls[hyper=false]{mcmc} and repeated below,
\begin{equation*}
  \lim_{I \rightarrow \infty} \,\, \frac{1}{I} \sum_{i=1}^I f(\bm{x}^{(i)}) - \mathbb{E}_{\pi^*}[f] \thicksim \mathcal{N} \left(0, \frac{\sigma_f^2}{I}\right)
\end{equation*}
Where $\sigma^2_f$ is the variance of a given function $f$ evaluated under the stationary density of the chain.
As mentioned, by construction the successive realizations in a Markov chain is not independent because successive iterations are correlated.
Therefore, the term $\sigma^2_f$ is now given by \cite{Geyer2011},
\begin{equation*}
  \sigma_f^2 = \mathbb{V} + \text{Cov}
\label{eq:ch5_markov_chain_variance}
\end{equation*}

%-----------------------------------------------------------------------------------------------------------------
\subsection[Autocovariance and Autocorrelation of MCMC samples]{Autocovariance and Autocorrelation of MCMC samples}\label{sub:bc_mcmc_diagnostic_autocovariance}
%-----------------------------------------------------------------------------------------------------------------

The autocorrelation function $\rho_f$ is defined as
\begin{equation}
  \rho_f (t) \equiv \frac{C_f(t)}{C_f(0)}
\label{eq:ch5_autocorrelation}
\end{equation}
where $C_f(0)$ is the lag-$0$ autocovariance and is equal to the process variance $\mathbb{V}[f]$ for a stationary process.

%-----------------------------------------------------------------------------------------------------------------
\subsection[Initialization Bias and burn-in]{Initialization bias and burn-in}\label{sub:bc_mcmc_diagnostic_burnin}
%-----------------------------------------------------------------------------------------------------------------



%-------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection[Autocorrelation in Equilibrium and Chain Thinning]{Autocorrelation in Equilibrium and Chain Thinning}\label{sub:bc_mcmc_diagnostic_thinning}
%-------------------------------------------------------------------------------------------------------------------------------------------------------

Assuming that the stationary chain has been attained the asymptotic results of Eq.~(\ref{eq:ch5_markov_chain_clt}) applies.
To derive the expression for $\sigma^2_f$ in the equation first consider that the variance of the \gls[hyper=false]{mcmc} estimator $\hat{f}$ for the expected value of $f$ is given by,
\begin{equation}
  \mathbb{V}[\,\hat{f}\,] = \mathbb{E}\left[\left(\frac{1}{N} \sum_{i=1}^N \left( f[\bm{\mathcal{X}}^{(i)}]  - \mathbb{E}[f] \right)\right)^2 \right]
\label{eq:ch5_markov_chain_estimate_variance}
\end{equation}
by using nested sum, the definition can be rewritten as,
\begin{equation*}
  \begin{split}
  \mathbb{V}[\,\hat{f}\,] & = \mathbb{E}\left[\frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N \left(f[\bm{\mathcal{X}}^{(i)}]  - \mathbb{E}[f] \right) \cdot \left(f[\bm{\mathcal{X}}^{(j)}]  - \mathbb{E}[f] \right) \right] \\
  & = \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N \mathbb{E}\left[\left(f[\bm{\mathcal{X}}^{(i)}]  - \mathbb{E}[f] \right) \cdot \left(f[\bm{\mathcal{X}}^{(j)}]  - \mathbb{E}[f] \right) \right] \\
  & = \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N \text{Cov} \left[ f(\bm{\mathcal{X}}^{(i)}), f(\bm{\mathcal{X}}^{(j)}) \right]
  \end{split}
\label{eq:ch5_markov_chain_estimate_variance_nested_sum}
\end{equation*}

Assuming that the chain $\{\bm{\mathcal{X}}^{(i)}\}$ is stationary then the covariance function is simply a function of the separation between the two iterations $i$ and $j$,
\begin{equation*}
  \begin{split}
  \mathbb{V}[\,\hat{f}\,] & =  \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N \text{Cov} \left[ f(\bm{\mathcal{X}}^{(i)}), f(\bm{\mathcal{X}}^{(j)}) \right] \\
                          & = \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N C_f (|i - j|) \\
                          & \approx \frac{1}{N^2} \sum_{i=1}^N \sum_{t = -\infty}^{\infty} C_f (|t|) = \frac{1}{N} \sum_{t = -\infty}^{\infty} C_f (|t|)
  \end{split}
\label{eq:ch5_markov_chain_estimate_variance_stationary}
\end{equation*}
where $C_f$ is the (stationary) autocovariance function associated with function $f$.
The approximation in the last line above is valid assuming that $C_f$ decays as the separation $t = i - j$ becomes larger \cite{Sokal1997}.
Noting that the covariance function $C_f$ is a symmetric function about zero,
\begin{equation*}
  \begin{split}
  \mathbb{V}[\,\hat{f}\,] & = \frac{1}{N} \left( C_f (0) + 2 \sum_{t = 1}^{\infty} C_f (t) \right) \\
                          & = \frac{2 C_f (0)}{N} \left( \frac{1}{2} + \sum_{t = 1}^{\infty} \rho_f (t) \right)
  \end{split}
\label{eq:ch5_markov_chain_estimate_variance_expand}
\end{equation*}
where $\rho_f$ is the autocorrelation function defined in Eq.~(\ref{eq:ch5_autocorrelation}).
Rearranging the term, the variance of the \gls[hyper=false]{mcmc} estimator $\hat{f}$ is given by,
\begin{equation}
  \mathbb{V}[\,\hat{f}\,] = \frac{2 \tau_{\text{int},f}}{N} C_f (0) 
\label{eq:ch5_markov_chain_estimator_variance}
\end{equation}
where $\tau_{\text{int},f}$, the \emph{integrated autocorrelation time} (or simply autocorrelation time), is defined as,
\begin{equation}
  \tau_{\text{int},f} =  \left( \frac{1}{2} + \sum_{t = 1}^{\infty} \rho_f (t) \right)
\label{eq:ch5_markov_chain_autocorrelation_time}
\end{equation}

In making connection with Eq.~(\ref{eq:ch5_markov_chain_clt}), Eq.~(\ref{eq:ch5_markov_chain_estimator_variance}) can be interpreted in two ways.
First, the use of Markov chain of length $I$ to estimate the integral of $f$ inflates the true variance of $f$ (i.e., $C_f(0)$) by factor $2 \tau_{\text{int},f}$ in the computation of the \gls[hyper=false]{mc} sampling variance.
In other words, $\sigma^2_f \equiv 2 \tau_{\text{int},f} C_f(0)$.
Or, equivalently, the number of independent samples required in the computation of the \gls[hyper=false]{mc} sampling variance is only a factor of $\frac{1}{2 \tau_{\text{int},f}}$ of the total \gls[hyper=false]{mcmc} samples $I$.
In other words, $N_{\text{ind}} \equiv \frac{I}{2 \tau_{\text{int},f}}$ with $N_\text{ind}$ the number of independent samples.

In either interpretation, the statistical error associated with the \gls[hyper=false]{mc} estimation is larger for an estimation using \gls[hyper=false]{mcmc} samples than using independent samples due to the inherent correlation.
Moreover, in the case of estimation using \gls[hyper=false]{mcmc}, the autocorrelation time $\tau_{\text{int},f}$ directly affects the statistical error.
Note that the autocorrelation time is associated with a given function $f$ and have to be computed for different choice of $f$.

% On thinning