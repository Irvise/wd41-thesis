%*********************************************************************
\subsection{Simulation Experiment}\label{sub:bc_simulation_experiment}
%*********************************************************************

% Introductory paragraph
The application of the Bayesian calibration framework on the \gls[hyper=false]{trace} reflood model parameters against the \gls[hyper=false]{feba} experimental data is based on $6$ different statistical formulations, in the following referred to as \emph{calibration schemes}.
These schemes are distinguished by their respective assumption:
\begin{itemize}
	\item \texttt{w/ Bias, All}. The first calibration scheme assumes that the \gls[hyper=false]{trace} model is an imperfect simulator of the reflood phenomena in the the \gls[hyper=false]{feba} experiment.
		As such it considers a model bias term (as described furter below) in the calibration process. Furthermore, in this scheme, all available types of experimental data are considered.
		The data includes the clad temperature at different time points and at different axial locations (will be succinctly referred to below as the $TC$ output or data),
		the pressure drop at different time points and at different axial segments (referred to as the $DP$ output or data),
		and the collected liquid carryover at different time points (referred to as the $CO$ output or data).
		As mentioned, following the results of the previous chapter, only the most influential $8$ reflood model parameters are considered for the calibration. 
	\item \texttt{w/ Bias, TC}, \texttt{w/ Bias, DP}, and \texttt{w/ Bias, CO} are three variants of the scheme \texttt{w/ Bias, All} in which only one type of experimental data (respectively, output) is considered at a time for the calibration.
		The purpose of these schemes is to investigate the effect of using different types of data from the same experiment to constrain the model parameters prior uncertainties.
		The calibration is still conducted for the $8$ reflood model parameters and by considering the model bias term.
	\item \texttt{w/o Bias} is conducted to provide a comparison with the calibration. This scheme is similar to the scheme \texttt{w/ Bias, All};
		it uses all available types of experimental data to calibrate the $8$ reflood model parameters, except that no model bias term is included in the formulation.
		In essence, this scheme assumes that the \gls[hyper=false]{trace} model perfectly describes the reflood phenomena in the \gls[hyper=false]{feba} experiment.
	\item \texttt{w/ Bias, no dffbVIHT}. The last calibration scheme is conducted as to investigate the effect of excluding, from the calibration process, an influential parameter (\texttt{dffbVIHT}) that is later found from the scheme \texttt{w/ Bias, All} to be strongly correlated.
		Except for calibrating only the $7$ reflood model parameters, this scheme used similar assumptions as the first scheme.
\end{itemize}

The six calibration schemes above aim to update the prior uncertainties of the model parameters using the available experimental data from \gls[hyper=false]{feba} test No. 216.
The six posterior \gls[hyper=false]{pdf} formulation are then directly sampled using an ensemble \gls[hyper=false]{mcmc} sampler to obtain six different sets of posterior samples.
To avoid an excessive computational cost of having to run \gls[hyper=false]{trace} hundreds thousands of times (if not more), the \gls[hyper=false]{gp} metamodel for the \gls[hyper=false]{trace} model developed in Chapter~\ref{ch:gp_metamodel} is used to substitute \gls[hyper=false]{trace} run.

These different sets of samples are then analyzed to assess the effect of using different calibration schemes in constraining the prior uncertainties of the model parameters. 
Finally, the resulting posterior samples from different schemes are used in forward \gls[hyper=false]{uq} on the \gls[hyper=false]{trace} model of different \gls[hyper=false]{feba} tests (corresponding to different boundary conditions, namely system pressure and reflood rate).
This final exercise is aimed to assess the implication of the posterior uncertainties from different calibration schemes on (and their applicability for) the prediction under conditions different from the condition of the calibration data.

In the following, the important terms of Eq.~(\ref{eq:bc_observation_simulation_true}) will be discussed in the context of the present application on the \gls[hyper=false]{trace} model before detailing each calibration scheme.
Afterward, the \gls[hyper=false]{mcmc} sampler and simulation setting as well as a method to evaluate and compare different posterior prediction uncertainties are presented.

%-------------------------------------------------------------------------------------------
\subsubsection{Experimental Data and Observation Layout}\label{subsub:bc_observation_layout}
%-------------------------------------------------------------------------------------------

% Introductory Paragraph
The experimental data of the \gls[hyper=false]{feba} test No. $216$ used in the present calibration is based on the report provided to the participants of the \gls[hyper=false]{premium} benchmark \cite{Skorek2013}.
The data in the report, in turn, is based on the digitization of experimental curves on the original report \cite{Ihle1984} and additional review on the associated experimental uncertainties.
Therefore this were the data used by other participants of the benchmark.

% Clad Temperature
The experimental data provided for the clad temperature ($TC$) of the \gls[hyper=false]{feba} test No. $216$ consists of $33$ time points for each of the $8$ different axial locations of the thermocouples along the test section.
\marginpar{Clad temperature ($TC$) data}
Recall that by convention in the experiment, $TC1$ corresponds to the thermocouple measurement at the top of the test section ($\approx 4.1\,[m]$), while $TC8$ corresponds to the measurement at the bottom of the section ($\approx 0.3\,[m]$).

Due to the strong discontinuity of the clad temperature around the point of quenching, the model bias term cannot be modeled using stationary \gls[hyper=false]{gp} (see Section~\ref{subsub:bc_model_bias}) as it severely violates to the constant variance assumption as function of time and axial location (at the very least, before and after the quenching occurs).
To keep the simplicity from using the stationary \gls[hyper=false]{gp} formulation, the model bias term is modeled only for the part of transient before quenching occurs.
Thus the calibration is also conducted using the data prior to quenching.
This is further justified by the fact that after quenching there is much less relevant variation in the temperature transient.

Because of the different timing of quenching along the test section, the number of data points available for calibration changes per axial location.
Based on these data points, an observation layout for $TC$ data can be defined,
\begin{equation}
	\begin{split}
		\boldsymbol{\Lambda}_{TC} & = \{(z_1,t_1),(z_1,t_{2}),(z_2,t_1),\ldots,(z_2,t_{7}),\\
															& \quad\quad (z_3,t_1),\ldots,(z_3,t_{12}),(z_4,t_1),\ldots,(z_4,t_{17}),  \\
															& \quad\quad (z_5,t_1),\ldots,(z_5,t_{21}),(z_6,t_1),\ldots,(z_6,t_{24}),  \\
															& \quad\quad (z_7,t_1),\ldots,(z_7,t_{25}),(z_8,t_1),\ldots,(z_8,t_{27})\} \\
	\end{split}
\label{eq:bc_observation_layout_feba_tc}
\end{equation}
where $z$ denotes the axial location (or segment for the $DP$ data) and $t$ denotes time point.
The total number of data points associated with $TC$ output is $133$.

% Clad Temperature uncertainty
The reported experimental uncertainty associated with the clad temperature measurement is $\pm0.5\%$ of the measured value in $[^oC]$.
In this thesis, this statement of uncertainty is translated to a Gaussian probability distribution such that the uncertainty covers the $99.7\%$ probability (i.e., $3$ sigma levels).
Let $\mathbf{y}_{E,TC}$ is the vector of $TC$ data observed at $\boldsymbol{\Lambda}_{TC}$, then the experimental uncertainty is given as,
\begin{equation}
	\begin{split}
		& \mathcal{E}(\boldsymbol{\Lambda}_{TC}) \thicksim \mathcal{N}(0, \Sigma_{TC})\\
		& \Sigma_{E,TC} = (\mathbf{I} \frac{0.005}{3} \mathbf{y}_{E,TC}^T) (\mathbf{I} \frac{0.005}{3} \mathbf{y}_{E,TC}^T)
	\end{split}	
\label{eq:bc_experimental_uncertainty_feba_tc}
\end{equation}
where $\mathbf{I}$ is an identity matrix of size $133$, where $133$ is the length of the observation layout $\boldsymbol{\Lambda}_{TC}$.
The $133$-dimensional Gaussian distribution above is independent but not identically distributed as the variance change for each measurement point.

% Pressure Drop
The experimental data provided for the pressure drop ($DP$) of the \gls[hyper=false]{feba} test No. $216$ consists of $18$ time points for each of the $4$ different axial segments of the pressure drop measurements.
\marginpar{Pressure drop ($DP$) data}
Recall that in the experiment, the \emph{bottom} segment corresponds to the segment $0.0 - 1.7\,[m]$, the \emph{middle} to $1.7 - 2.3\,[m]$, the \emph{top} to $z = 2.3 - 4.1\,[m]$, and the \emph{total} to $0.0 - 4.1\,[m]$.
In the following, the bottom, the middle, the top, and the total segments are simply indices of the $DP$ output; $z_1$, $z_2$, $z_3$, $z_4$, respectively.
The observation layout for the $DP$ data is then defined as follow,
\begin{equation}
		\boldsymbol{\Lambda}_{DP} = \{(z_1,t_1),\ldots,(z_1,t_{18}),(z_2,t_1),\ldots,(z_4,t_{18})\}
\label{eq:bc_observation_layout_feba_dp}
\end{equation}
where $z$ denotes the axial location (or segment for the $DP$ data) and $t$ denotes time point.

% Pressure drop uncertainty
The reported experimental uncertainty associated with the pressure drop measurement is $\pm10\%$ of the measured value in $[Pa]$.
As before, this statement of uncertainty is translated to a Gaussian probability distribution covering the $99.7\%$ probability (i.e., $3$ sigma levels).
Let $\mathbf{y}_{E,DP}$ is the vector of $DP$ data observed at $\boldsymbol{\Lambda}_{DP}$, then the experimental uncertainty is given as a multivariate Gaussian,
\begin{equation}
	\begin{split}
		& \mathcal{E}(\boldsymbol{\Lambda}_{DP}) \thicksim \mathcal{N}(0, \Sigma_{DP})\\
		& \Sigma_{E,DP} = (\mathbf{I} \frac{0.1}{3} \mathbf{y}{E,DP}^T) (\mathbf{I} \frac{0.1}{3} \mathbf{y}_{E,DP}^T)
	\end{split}	
\label{eq:bc_experimental_uncertainty_feba_dp}
\end{equation}
where $\mathbf{I}$ is an identity matrix of size $P_2$, where $P_2$ is the length of the observation layout $\boldsymbol{\Lambda}_{DP}$.

% Liquid carryover
Finally, the experimental data provided for the liquid carryover ($CO$) of the \gls[hyper=false]{feba} test No. $216$ initially consists of $16$ time points.
\marginpar{Liquid carryover ($CO$) data}
However, because the collecting tank was saturated at $10\,[kg]$ only the transient up to that mass is of interest.
By excluding the data points where the tank has been saturated, only $7$ data points are available for the calibration.
Based on these data points, the observation layout for the $CO$ data is defined as,
\begin{equation}
		\boldsymbol{\Lambda}_{CO}  = \{(t_1),\ldots,(t_{7})\}
\label{eq:bc_observation_layout_feba_co}
\end{equation}
where $z$ denotes the axial location (or segment for the $DP$ data) and $t$ denotes time point.
The observed data according to the observation layout is denoted $\mathbf{y}_{E,CO}$.

% Liquid carryover uncertainty
A large uncertainty was indicated for the liquid carryover measurement that possibly includes biased measurement as the measured mass in the collecting tank does not always correspond to the liquid carryover \cite{Sanz2017}.
The suggested level of uncertainty for the benchmark was $\pm0.5\,[kg]$.
To cover the reported uncertainty and the possible bias, the reported level is assumed to be $1\sigma$ level of an independent identically distributed multivariate Gaussian,
\begin{equation}
	\begin{split}
		& \mathcal{E}(\boldsymbol{\Lambda}_{CO}) \thicksim \mathcal{N}(0, \mathbf{I} \sigma^2_{CO})\\
	\end{split}
\label{eq:bc_experimental_uncertainty_feba_co}
\end{equation}
where $\mathbf{I}$ is an identity matrix of size $P_3$, where $P_3$ is the length of the observation layout $\boldsymbol{\Lambda}_{CO}$ and
the $\sigma_{E,CO}$ it the standard deviation of the distribution, taken to be $0.5\,[kg]$.

% Full observation layout
Finally, the observation layout for each output (data) type can be combined into a single long vector of the full observation layout,
\marginpar{Full observation layout}
\begin{equation}
	\begin{split}
		\boldsymbol{\Lambda} & = \{(TC, z_1,t_1),\ldots,(TC,z_8,t_{27}),\\
		                     & \quad\quad (DP,z_1,t_{1}),\ldots,(DP,z_4,t_{18}),\\
												 & \quad\quad (CO,t_1),\ldots,(CO,t_7)\}\\
	\end{split}
\label{eq:bc_observation_layout_feba_full}
\end{equation}
That is, by convention here, $\boldsymbol{\Lambda} = \{\boldsymbol{\Lambda}_{TC}, \boldsymbol{\Lambda}_{DP}, \boldsymbol{\Lambda}_{CO}\}$.
The total number of data points of \gls[hyper=false]{feba} test No. $216$, and the length of the observation layout $\boldsymbol{\Lambda}$, used in the calibration is thus $212$.

%--------------------------------------------------------------------------------------------
\subsubsection{Gaussian Process Approximation for TRACE Simulation}\label{subsub:bc_gp_trace}
%--------------------------------------------------------------------------------------------

Following the results of Chapter~\ref{ch:gp_metamodel}, three separate multivariate \gls[hyper=false]{gp} metamodels are used to approximate \gls[hyper=false]{trace} prediction for each type of output ($TC$, $DP$, and $CO$).
The hyper-parameters associated with these metamodels are separately estimated using actual \gls[hyper=false]{trace} runs $\mathbf{Y}$ based on a design of experiment $\mathbf{DM}$ (see the details in Section~\ref{sec:gp_application_to_feba}).
After being estimated, the hyper-parameters of the \gls[hyper=false]{gp} metamodel are kept constant in the application of the metamodel.

Under the \gls[hyper=false]{gp} formulation, the simulator prediction for a given input $\bm{x}_o$ becomes a probabilistic model.
The prediction of $TC$ output at the observation layout $\boldsymbol{\Lambda}_{TC}$ is formulated as follows,
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}}_{M,TC} (\bm{x}_o) | \mathbf{Y} \sim \mathcal{N} (\boldsymbol{\mu}_{M,TC} (\bm{x}_o), \Sigma_{M,TC} (\bm{x}_o)) \\
		& \boldsymbol{\mu}_{M,TC}  = \bar{\mathbf{y}}_{TC} + \boldsymbol{\Phi}^*_{Q_{TC},TC} \mathbf{m}_{SK,TC}(\bm{x}_o) \\
		& \Sigma_{M,TC} = \boldsymbol{\Phi}^*_{Q_{TC},TC} \text{diag}(\mathbf{s}^2_{SK,TC}(\bm{x}_o)) \boldsymbol{\Phi}^{*T}_{Q,TC} + \boldsymbol{\Phi}^*_{>Q_{TC},TC} \mathbf{I}\boldsymbol{\Phi}^{*T}_{>Q_{TC},TC}) \\
		& \mathbf{m}_{SK,TC} = [m_{SK,TC,1}(\bm{x}_o), m_{SK,TC,2}(\bm{x}_o), \cdots, m_{SK,TC,Q_{TC}}(\bm{x}_o)] \\
		& \mathbf{s}^2_{SK,TC} = [s^2_{SK,TC,1}(\bm{x}_o), s^2_{SK,TC,2}(\bm{x}_o), \cdots, s^2_{SK,TC,Q_{TC}}(\bm{x}_o)]
	\end{split}
\label{eq:p_variate_metamodel_tc}
\end{equation}
where the notations above follow the convention of Section~\ref{sub:gp_multivariate} and following the development in Section~\ref{sec:gp_application_to_feba}, the number of retained principal components for the $TC$ output $Q_{TC}$ is selected to be $7$.

Recall that the \gls[hyper=false]{svd} was conducted on the full \gls[hyper=false]{trace} simulation output (in the case of the temperature output: at $8$ axial levels and at 10'000 time-steps) for the dimension reduction.
In relation with observation layout of Eq.~(\ref{eq:bc_observation_layout_feba_tc}), not all points in time of the full simulation output have corresponding experimental data.
As such, the only difference from the previous development is that now the observation layout $\boldsymbol{\Lambda}_{TC}$ is used to select the elements of the output mean vector $\bar{\mathbf{y}}_{TC}$, the eigenvectors $\boldsymbol{\Phi}^*_{Q_{TC},TC}$, and the unretained eigenvectors $\boldsymbol{\Phi}^*_{>Q_{TC},TC}$ such that they contain only the points in time where data are actually observed.
The resulting dimension of the Gaussian distribution is thus $133$.

The formulation for the $DP$ output at the $\boldsymbol{\Lambda}_{DP}$ (Eq.~(\ref{eq:bc_observation_layout_feba_dp})) follows accordingly,
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}}_{M,DP} (\bm{x}_o) | \mathbf{Y} \sim \mathcal{N} (\boldsymbol{\mu}_{M,DP} (\bm{x}_o), \Sigma_{M,DP} (\bm{x}_o)) \\
		& \boldsymbol{\mu}_{M,DP}  = \bar{\mathbf{y}}_{DP} + \boldsymbol{\Phi}^*_{Q_{DP},DP} \mathbf{m}_{SK,DP}(\bm{x}_o) \\
		& \Sigma_{M,DP} = \boldsymbol{\Phi}^*_{Q_{DP},DP} \text{diag}(\mathbf{s}^2_{SK,DP}(\bm{x}_o)) \boldsymbol{\Phi}^{*T}_{Q_{DP},DP} + \boldsymbol{\Phi}^*_{>Q_{DP},DP} \mathbf{I}\boldsymbol{\Phi}^{*T}_{>Q,DP}) \\
		& \mathbf{m}_{SK,DP} = [m_{SK,DP,1}(\bm{x}_o), m_{SK,DP,2}(\bm{x}_o), \cdots, m_{SK,DP,Q_{DP}}(\bm{x}_o)] \\
		& \mathbf{s}^2_{SK,DP} = [s^2_{SK,DP,1}(\bm{x}_o), s^2_{SK,DP,2}(\bm{x}_o), \cdots, s^2_{SK,DP,Q_{DP}}(\bm{x}_o)]
	\end{split}
\label{eq:p_variate_metamodel_dp}
\end{equation}
where $Q_{DP}$, the number of retained principal components with respect to the $DP$ output, is taken to be $10$.
The observation layout $\boldsymbol{\Lambda}_{DP}$ is used to select the elements of the output mean vector $\bar{\mathbf{y}}_{DP}$, the eigenvectors $\boldsymbol{\Phi}^*_{Q_{DP},DP}$, and the unretained eigenvectors $\boldsymbol{\Phi}^*_{>Q_{DP},DP}$ such that they contain only the relevant points in time.

Finally, the $CO$ output at the observation layout $\boldsymbol{\Lambda}_{CO}$ (Eq.~(\ref{eq:bc_observation_layout_feba_co})) for a given input $\bm{x}_o$,
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}}_{M,CO} (\bm{x}_o) | \mathbf{Y} \sim \mathcal{N} (\boldsymbol{\mu}_{M,CO} (\bm{x}_o), \Sigma_{M,CO} (\bm{x}_o)) \\
		& \boldsymbol{\mu}_{M,CO}  = \bar{\mathbf{y}}_{CO} + \boldsymbol{\Phi}^*_{Q_{CO},CO} \mathbf{m}_{SK,CO}(\bm{x}_o) \\
		& \Sigma_{M,CO} = \boldsymbol{\Phi}^*_{Q_{CO},CO} \text{diag}(\mathbf{s}^2_{SK,CO}(\bm{x}_o)) \boldsymbol{\Phi}^{*T}_{Q_{CO},CO} + \boldsymbol{\Phi}^*_{>Q_{CO},CO} \mathbf{I}\boldsymbol{\Phi}^{*T}_{>Q_{CO},CO}) \\
		& \mathbf{m}_{SK,CO} = [m_{SK,CO,1}(\bm{x}_o), m_{SK,CO,2}(\bm{x}_o), \cdots, m_{SK,CO,Q_{CO}}(\bm{x}_o)] \\
		& \mathbf{s}^2_{SK,CO} = [s^2_{SK,CO,1}(\bm{x}_o), s^2_{SK,CO,2}(\bm{x}_o), \cdots, s^2_{SK,CO,Q_{CO}}(\bm{x}_o)]
	\end{split}
\label{eq:p_variate_metamodel_co}
\end{equation}
where $Q_{CO}$, the number of retained principal components with respect to the $CO$ output, is taken to be $5$.
Lastly, similar to the two previous cases, the observation layout $\boldsymbol{\Lambda}_{CO}$ is used to select the elements of the output mean vector $\bar{\mathbf{y}}_{CO}$, the eigenvectors $\boldsymbol{\Phi}^*_{Q_{CO},CO}$, and the unretained eigenvectors $\boldsymbol{\Phi}^*_{>Q_{CO},CO}$ such that they contain only the points in time coincide with the observed data.

%-----------------------------------------------------------------------
\subsubsection{Modeling the Model Bias Term}\label{subsub:bc_model_bias}
%-----------------------------------------------------------------------

% Introductory Paragraph

% Model Bias Term

% GP 

% Summary
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}}_{E,TC} | \mathbf{x}_m \sim \mathcal{N} (\boldsymbol{\mu}_{TC} (\bm{x}_m), \Sigma_{TC} (\bm{x}_m)) \\
		& \boldsymbol{\mu}_{TC} (\bm{x}_m) = \boldsymbol{\mu}_{M,TC} (\bm{x}_m) + \mathbf{m}_{\delta,TC} \\
		& \Sigma_{TC} (\bm{x}_m), = \Sigma_{M,TC} (\bm{x}_m) + \Sigma_{\delta,TC} + \Sigma_{E,TC} \\
	\end{split}
\label{eq:feba_gp_bias}
\end{equation}

%-----------------------------------------------------------------------
\subsubsection{Calibration Schemes}\label{subsub:bc_calibration schemes}
%-----------------------------------------------------------------------

% Introductory Paragraph
Defining the elements 

% w/ Bias, TC
The calibration scheme \texttt{w/ Bias, All} combines the fomulation of the calibration schemes \texttt{w/ Bias, TC}, \texttt{w/ Bias, DP}, and \texttt{w/ Bias, CO}.
As such, in the following the latter three calibration schemes are first presented.
Combining the terms of the above according to Eq.~(\ref{eq:bc_observation_simulation_true}) gives similar formulation as Eq.~(\ref{eq:bc_data_model_gaussian}), but specifically for the $TC$ data generating model.
That is,
\begin{equation}
	\begin{split}
		& \bm{\mathcal{Y}}_{E,TC} | \mathbf{x}_m \sim \mathcal{N} (\boldsymbol{\mu}_{TC} (\bm{x}_m), \Sigma_{TC} (\bm{x}_m)) \\
		& \boldsymbol{\mu}_{TC} (\bm{x}_m) = \boldsymbol{\mu}_{M,TC} (\bm{x}_m) + \mathbf{m}_{\delta,TC} \\
		& \Sigma_{TC} (\bm{x}_m) = \Sigma_{M,TC} (\bm{x}_m) + \Sigma_{\delta,TC} + \Sigma_{E,TC} \\
	\end{split}
\label{eq:likelihood_tc}
\end{equation}
where $\boldsymbol{\mu}_{TC}$ and $\Sigma_{TC} (\bm{x}_m)$ are the $133$-dimensional mean vector and the $133\times 133$ covariance matrix associated with the $TC$ output/data, respectively.
The mean vector $\boldsymbol{\mu}_{TC}$ consists of the mean vector of the \gls[hyper=false]{gp} metamodel prediction $\boldsymbol{\mu}_{M,TC}$ (Eq.~(\ref{eq:p_variate_metamodel_tc}));
and the mean vector of the model bias term $\mathbf{m}_{\delta,TC}$ (Eq.~(\ref{eq:feba_gp_bias})).
The covariance matrix $\Sigma_{TC} (\bm{x}_m)$ comprises the covariance matrix of the \gls[hyper=false]{gp} metamodel prediction (Eq.~(\ref{eq:p_variate_metamodel_tc}));
the covariance matrix of the model bias term $\Sigma_{\delta,TC}$ (Eq.~(\ref{eq:p_variate_metamodel_tc}));
and the covariance matrix of the experimental uncertainty for $TC$ data (Eq.~(\ref{eq:bc_experimental_uncertainty_feba_tc})).

% w/ Bias. DP

% w/ Bias, CO

% w/ Bias, All

% w/o Bias

% w/ Bias, no dffbVIHT 

% Likelihood, Prior, Posterior
The likelihood function is obtained by making the density of the Gaussian distribution given above in terms of its parameterization.

% Summary
Table~\ref{tab:ch5_calibration_schemes} summarizes the different calibration schemes considered in this study.
\begin{table*}[!htbp]\centering
\ra{0.9}
\begin{adjustwidth*}{}{-3cm}
\caption{Bayesian calibration schemes conducted for the \gls[hyper=false]{trace} reflood model parameters against data from \gls[hyper=false]{feba} test No. $216$.}
\label{tab:ch5_calibration_schemes}
\begin{tabular}{@{}clccccrc@{}}\toprule
\multirow{2}{*}{No.} & \multirow{2}{*}{\shortstack[c]{Calibration Scheme}} 	& \multirow{2}{*}{\shortstack[c]{Model Bias\\Term}}	& \multicolumn{3}{c}{Types of Output} & \phantom{a} & \multirow{2}{*}{\shortstack[c]{Reflood Model\\Parameters \footnotesize{(total number)}}} \\
																															  \cmidrule{4-6}
    &                                 					& 						& $TC$				& $DP$     		& $CO$   				&&	\\ \midrule
1   & \texttt{w/ Bias, All}											& \Checkmark  & \Checkmark  & \Checkmark  & \Checkmark  	&& All \footnotesize{($8$)}          				\\
2   & \texttt{w/ Bias, TC}     									& \Checkmark  & \Checkmark	&							&          			&& All \footnotesize{($8$)}           			\\
3   & \texttt{w/ Bias, DP}     									& \Checkmark 	&         		& \Checkmark	&								&& All \footnotesize{($8$)} 								\\
4   & \texttt{w/ Bias, CO}       								& \Checkmark 	& 						& 						& \Checkmark		&& All \footnotesize{($8$)}           			\\
5   & \texttt{w/o Bias}               					&          		& \Checkmark  & \Checkmark  & \Checkmark		&& All \footnotesize{($8$)}	          			\\
6   & \texttt{w/ Bias, no dffbVIHT}             & \Checkmark  & \Checkmark  & \Checkmark  & \Checkmark		&& Excluding \texttt{dffbVIHT} \footnotesize{(7)}\\
\bottomrule
\end{tabular}
\end{adjustwidth*}
\end{table*}

%---------------------------------------------------------------------------------------
\subsubsection{MCMC Simulation using Ensemble Sampler}\label{subsub:bc_calibration_mcmc}
%---------------------------------------------------------------------------------------

% Introductory paragraph
Different calibration schemes above result in several likelihood functions, each of which when combined with common prior \glspl[hyper=false]{pdf} of the model parameters, yield several posterior \glspl[hyper=false]{pdf} of the model parameters.
These $8$-dimensional (respectively $7$ for the \texttt{w/ Bias, no dffbVIHT}) posterior \glspl[hyper=false]{pdf} contain all the information on the model parameters conditional on the experimental data and the assumed prior uncertainties, under the respective assumed calibration scheme (or, the error model).
To characterize the posterior uncertainties of the model parameters, samples are directly generated from the respective posterior \gls[hyper=false]{pdf} by means of \gls[hyper=false]{mcmc} simulation.
This study opts the use of an ensemble sampler as described in Section~\ref{sub:bc_mcmc_aies} to avoid having dealing with tuning of the simulation.

% Parallel requirement
Although the use of \gls[hyper=false]{gp} metamodel alleviates the burden of having to run \gls[hyper=false]{trace} directly, evaluating the likelihood function requires an inversion of the covariance matrix.
The computational cost of matrix inversion is still not negligible, especially considering the expected number of evaluations. 
Though the Algorithm~\ref{alg:aies} is straightforward to implement in any modern computing environment, it is not readily applicable for using multiple CPU \cite{Foreman-Mackey2013}.
Therefore, a parallelized implementation of the algorithm would be beneficial in accelerating the \gls[hyper=false]{mcmc} simulations.

% EMCEE and RGW
The parallelization of the \gls[hyper=false]{aies} sampler was originally developed and implemented in the python package \texttt{emcee} \cite{Foreman-Mackey2013}.
The main design philosophy of \texttt{emcee} (and its ported \texttt{R} package \texttt{rgw} \cite{Mantz2016} used in this thesis) is that of a \emph{portable} sampler.
That is, the user simply has to code the posterior formulation (the likelihood and the posterior) in the respective generic computing environment (\texttt{R} or python), without the need to put the probabilistic model within a new framework\footnote{Such as the approach adopted in the more established \texttt{WinBugs} \cite{Lunn2000}, \texttt{Jags} \cite{Plummer2003}, and \texttt{Stan} \cite{Carpenter2017a}. These samplers, however, has more extensive capabilities for conducting a Bayesian data analysis and tends to be faster as they port the specified probabilistic models to a lower level language (e.g., \texttt{C++}). Furthermore, being older, they have a larger and more diverse user base.}.

% Settings used in the simulation
In the present study, $2'000$ iterations are carried out for an ensemble of $1'000$ walkers.
The initial state of the ensemble is a tight random scatter around the nominal model parameter values.
The total number of iterations depends on the convergence of the \gls[hyper=false]{mcmc} simulation (discarding the initialization bias) and the required level statistical error as detailed in Section~\ref{sec:bc_mcmc_diagnostic}.
For the present study, they are assessed after-the-fact and the results is indeed found to be sufficient.
Meanwhile, there is no clear cut rule for choosing the number of walkers $L$, except to go as large as computational resources allowed \cite{Foreman-Mackey2013}.
Larger number of walkers requires more computational cost per iteration but yields more independent samples per iteration.
At the same time, larger number of walkers might cause more of the initial iterations to be discarded as more iterations are required to settle the ensemble in the typical region of the posterior distribution.
A thousand walkers were selected considering the available computational resources at the time of the analysis.

Each \gls[hyper=false]{mcmc} simulation corresponding to different calibration schemes results in $2'000'000$ posterior samples of the model parameters.
These samples are then further post-processed to remove the initialization bias and to reduce the autocorrelation among successive iterations.
 
%-------------------------------------------------------------------------------------
\subsubsection{Evaluating Calibration Results}\label{subsub:bc_calibration_evaluation}
%-------------------------------------------------------------------------------------

% Introductory paragraph
The results of the \gls[hyper=false]{mcmc} simulations are set of samples directly drawn from the respective posteriors.
These multivariate samples are visually represented as \emph{corner plot} depicting the joint posterior samples as a set of $1$-dimensional (univariate) and $2$-dimensional (bivariate) marginals of the posterior distribution (see Section~\ref{sub:bc_calibration_results}).
From the univariate marginal of each model parameter posterior uncertainties, the constraining ability of the data and the calibration scheme can be quickly, if not rigorously, assessed.
From the bivariate marginals, the correlation structure between the model parameters, if any, can be also quickly assessed.

% Forward uncertainty quantification
To investigate the implication of the different posterior samples on the prediction, simulation campaigns for forward \gls[hyper=false]{uq} (uncertainty propagation) are conducted on the \gls[hyper=false]{trace} \gls[hyper=false]{feba} model, using different model parameters posterior uncertainties. 
Furthermore, to assess the applicability of posterior uncertainties for the simulation of different conditions from the condition of the calibration experiment, the campaigns are conducted for $5$ additional \gls[hyper=false]{feba} tests.
In other words, these $5$ additional \gls[hyper=false]{feba} tests becomes the validation data sets.
Finally, to investigate the effect of correlated model parameters on the prediction, the campaigns are conducted both with and without considering the correlation structure in the posterior samples.

% Forward uncertainty quantification
The uncertainty propagation campaigns therefore consists of the campaigns on each \gls[hyper=false]{feba} test using model parameters posterior uncertainties derived from different calibration schemes with and without consideration of the correlation among model parameters.
For each of these campaign, actual \gls[hyper=false]{trace} runs are carried out using $1'000$ posterior samples.
Instead of having to come up with a closed form representation of the multivariate posterior distributions, and to explicitly model any possible correlation structure, these samples are directly drawn from the pool of posterior samples obtained from different calibration schemes.
The results of the propagation are represented in series of plots of prediction with the associated uncertainty bands for the three output types ($TC$, $DP$, and $CO$) similar to the ones presented in the result section of Chapter~\ref{ch:trace_reflood}.
From these plots the different propagation campaign can be compared.

% Calibration
At the same time, the numerous plots are unwieldy to deal with.
To circumvent this issue a more quantitative means of aggregating the results of different predictions uncertainties is required.
Although formal Bayesian approaches are available to assess the quality of the prediction using the model parameters posterior uncertainty\footnote{Formal computer model validation metrics, Bayesian or otherwise, is a research topic in its own right, see for instance the validation metrics proposed in \cite{Rebba2006,Rebba2006a,Jiang2008}. Their application is outside the scope of this thesis.}, this thesis adopts a more pragmatic assessment method based on the possibilistic theory proposed in \cite{Baccou2014}.

The aim of the method is to quickly compare the applicability of the different posterior uncertainties in making prediction.
Loosely speaking, the applicability is measured by the width of the prediction uncertainty as well as its coherence (in the most general sense of the word) with experimental data.
Admittedly, the aim concerns less about the inherent correctness of the posterior uncertainties.
The method was applied to synthesize the results of different participants in the context of benchmarking, namely \gls[hyper=false]{bemuse} \cite{Baccou2014} and \gls[hyper=false]{premium} \cite{Sanz2017} projects. 
The method consists of three steps: \emph{information modeling}, \emph{information evaluation}, and \emph{information synthesis}.
For the comparison purpose in this thesis, only the first two steps above are applied and discussed in the following.

% Information modeling
In the information modeling, the information in a prediction uncertainty of a \gls[hyper=false]{qoi} $y$ is represented by an interval (lower and upper bounds: $LUB_y$ and $UUB_y$, respectively) and a reference value $y_\text{ref.}$.
A \emph{source} of information $src$ for a particular \gls[hyper=false]{qoi} $y$ consists of such an interval and, optionally, a reference value.
A complete ignorance is represented simply by the interval given by $LUB_y$ and $UUB_y$ resulting in a rectangular model,
\begin{equation}
		\pi_{src,y} (y) = 
			\begin{cases}
				1.0, &  LUB_y \leq y \leq UUB_y \\
			  0.0, & \text{otherwise}
	\end{cases}
\label{eq:calibinfo_rectangular}
\end{equation}
where $\pi$ is the possibility measure, whose minimum and maximum are $0.0$ and $1.0$, respectively.

The presence of a reference value $y_\text{ref.}$ within that intervals allows the triangular model to be defined,
\begin{equation}
  \pi_{src,y} (y) = 
			\begin{cases}
				\frac{y - LUB}{y_\text{ref.} - LUB}, &  LUB \leq y < y_\text{ref.} \\
        \frac{UUB - y}{UUB - y_\text{ref.}}, &  y_\text{ref.} \leq y \leq UUB \\
			  0.0                                , & \text{otherwise}
	\end{cases}
\label{eq:calibinfo_triangular}
\end{equation}
These two information models are illustrated in Fig.~\ref{fig:ch5_calibinfo_modeling}.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_calibinfo_modeling},
                  maincaption={Information modeling to represent uncertainty propagation results for a \gls[hyper=false]{qoi} $y$.},%
									mainshortcaption={Information modeling to represent uncertainty propagation results for a QoI $y$.},
                  leftopt={width=0.375\textwidth},
                  leftlabel={fig:ch5_calibinfo_modeling_rectangular},
                  leftcaption={Rectangular model},
                  %leftshortcaption={},%
                  rightopt={width=0.375\textwidth},
                  rightlabel={fig:ch5_calibinfo_modeling_triangular},
                  rightcaption={Triangular model},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/calibinfo_rectangular}
{../figures/chapter5/figures/calibinfo_triangular}

% Information Evaluation, Informativeness
The information evaluation part of the method comprises two indices to evaluate the quality of information in a given source.
\emph{Informativeness}, associated with a source $src$ and a \gls[hyper=false]{qoi} $y$ is defined as,
\begin{equation}
  \text{Inf}(src, y) = \frac{|\pi_{i, \text{ign}}| - |\pi_{i, \text{s}}|}{|\pi_{i, \text{ign}}|} = 1 - \frac{1}{2} \frac{UUB - LUB}{UUB_{\text{max.}} - LUB_{\text{min.}}}
\label{eq:informativeness}
\end{equation}
where $|\circ|$ denotes the area under an information model.
Informativeness measures the precision of the uncertain prediction, regardless the position of the reference value within the bound;
it takes value between $0.5$ (the widest uncertainty range of a source, the same as the maximum and minimum bounds) and $1.0$ (the narrowest uncertainty range of a source, practically $0$).
Fig.~\ref{fig:ch5_calibinfo_informativeness} illustrates $\text{Inf}$ calculation.
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_calibinfo_informativeness},
                  maincaption={Informatives of two different information source. Wider uncertainty interval gives lower informativeness, and vice versa. Thus $\text{Inf}(src_1, y) > \text{Inf}(src_2, y)$},%
									mainshortcaption={Informativeness of two different information sources.},
                  leftopt={width=0.425\textwidth},
                  leftlabel={fig:ch5_calibinfo_informativeness_narrow},
                  leftcaption={Narrower prediction uncertainty},
                  %leftshortcaption={},%
                  rightopt={width=0.425\textwidth},
                  rightlabel={fig:ch5_calibinfo_informativeness_wide},
                  rightcaption={Wider prediction uncertainty},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/calibinfo_informativeness_narrow}
{../figures/chapter5/figures/calibinfo_informativeness_wide}

% Information Evaluation, Calibration Score
\emph{Calibration score} between a source $src$ and the observed value $y_\text{obs.}$ for a \gls[hyper=false]{qoi} is defined as,
\begin{equation}
  \text{Cal}(src, y) = \pi_{src, y}(y_\text{obs.})
\label{eq:calibration_score}
\end{equation}
that is, the calibration score is the possibility of the observed value $y_\text{obs.}$ under the information model of the source $src$.
It measures the discrepancy between the observed value and uncertain prediction represented by an interval and a reference value.
Following Eq.~(\ref{eq:calibinfo_triangular}), the score severely penalizes the observed data that falls outside the prediction interval; assigning calibration score of $0.0$.
The score is at maximum of $1.0$ for $y_\text{obs.} = y_\text{ref.}$.
It does not, however, takes into account the possible uncertainties associated with $y_\text{obs.}$.
\bigtriplefigure[pos=tbhp,
								 mainlabel={fig:ch5_calibinfo_calib},
			           maincaption={Calibration scores of three different sources with the same observed data $y_\text{obs.}$: $\text{Cal}(src_1, y) > \text{Cal}(src_2, y) > \text{Cal}(src_3, y)$},
			           mainshortcaption={Calibration scores of three different sources with the same observed data $y_\text{obs.}$.},%
			           leftopt={width=0.30\textwidth},
			           leftlabel={fig:ch5_calibinfo_calib1},
			           leftcaption={$\text{Cal}(src_1, y) \approx 1.0$},
			           midopt={width=0.30\textwidth},
			           midlabel={fig:ch5_calibinfo_calib2},
			           midcaption={$\text{Cal}(src_2, y)$},
			           rightopt={width=0.30\textwidth},
			           rightlabel={fig:ch5_calibinfo_calib3},
			           rightcaption={$\text{Cal}(src_3, y) = 0$},
			           spacing={},
			           spacingtwo={}]
{../figures/chapter5/figures/calibinfo_calib1}
{../figures/chapter5/figures/calibinfo_calib2}
{../figures/chapter5/figures/calibinfo_calib3}

% Aggregation
For multiple outputs $\mathbf{y} = [y_1, \ldots, y_D]$ of the same source, both informativeness and calibration score can be aggregated by,
\begin{equation}
  \text{Inf}(src) = \frac{1}{D} \sum_{d=1}^{D} Inf(src, y_d) \quad\quad \text{Cal}(src) = \frac{1}{D} \sum_{d=1}^{D} \text{Cal}(src, y_d)
\label{eq:calibinfo_aggregate}
\end{equation}

% From Probabilistic to Possibilistic
Translating from the Bayesian (probabilistic) framework, the bounds of the prediction interval of the above $LUB$ and $UUB$ can be taken to be two percentiles of the prediction probability distribution that cover a selected probability.
\marginpar{Application of the method}
For instance, the criteria used in the present study, selecting a symmetric $95\%$ probability interval implies the $LUB$ and $UUB$ to be the $2.5$-th and $97.5$-th percentiles, respectively.
The choice is rather arbitrary but as long as the criteria is applied consistently across different prediction uncertainties, the value of the method for comparison is preserved.
Furthermore, the $2.5$-th and $97.5$-th percentiles of the prior prediction distribution are taken to be the $LUB_{\text{min.}}$ and $UUB_{\text{max.}}$ and the reference value $y_\text{ref.}$ is taken to be the median value of each posterior prediction distribution.
Finally the experimental data is taken to be the observed value $y_\text{obs.}$ without considering the associated uncertainty following the original paper \cite{Baccou2014}.
Once again, this lack of consideration is less of an issue for comparing between different uncertain prediction.
 
% Analyzing convergence
\clearpage
\begin{sidewaysfigure}
	\centering
	\includegraphics[width=0.90\textwidth]{../figures/chapter5/figures/plotEnsTraceDiscAllCentered}
		\captionof{figure}[Ensemble trace plots for each model parameter of calibration with model bias term.]{Ensemble trace plots for each model parameter of calibration with model bias term. Shown here is for the last $100$ iterations (out of $1'240$ post-burn-in iterations) and for $400$ walkers (out of $1'000$ walkers).}
	\label{fig:ch5_plot_ens_trace_all_disc_centered}
\end{sidewaysfigure}
\clearpage