%**************************************************************************
\section{Bayesian Formulation of Calibration Problem}\label{sec:bc_modular}
%**************************************************************************

% Introductory paragraph
The Bayesian framework for model calibration begins by constructing a probabilistic model of $y_E$ given in an additive formulation of Eq.~(\ref{eq:bc_observation_true}). 
That is, it aims at formulating the data generating process $\mathcal{Y}_E(\bm{x}_c; \bm{\lambda})$.
This model implies that the experimental data $y_E$ taken at particular $\bm{x}_c$ observed at $\bm{\lambda}$ is a realization of a stochastic process.
Furthermore, this probabilistic modeling entails casting any \emph{uncertain} element in Eq.~(\ref{eq:bc_observation_true}) either as random variable or stochastic process.

%-----------------------------------------------------------------------------
\subsection{Probabilistic Model for the Model Bias}\label{sub:bc_modular_bias}
%-----------------------------------------------------------------------------

% Introductory Paragraph
Recall the relationship between the true system response and its prediction by a simulator (Eq.~(\ref{eq:bc_true_simulation})) rearranged below:
\begin{equation*}
    \delta (\bm{x}_c, \boldsymbol{\lambda}) = y_T(\bm{x}_c, \boldsymbol{\lambda}) - y_M (\bm{x}_c, \hat{\bm{x}}_m, \boldsymbol{\lambda})
\end{equation*}
where the prediction $y_M$ is made using the best but unknown value of the model parameters.
As such, the model bias function $\delta$ represents a possible systematic difference between the true system response and the simulator prediction that still remains, even from using a simulator with the \emph{best} set of model parameter values.
\marginpar{Model bias, possible origins}
Possible sources for this bias are missing physics in the constituent physical models, numerical approximations, or any other simplifications presence in the simulator whose effects on the prediction are unknown a priori.
As such, the bias term tends to be systematic and dependent on the controllable input $\bm{x}_c$ and the observation layout $\boldsymbol{\Lambda}$ \cite{Reichert2012}.
Note that, strictly speaking, there is a dependence of $\hat{\bm{x}}_m$ on $\delta$, but this dependence is suppress from the notation; $\hat{\bm{x}}_m$, though unknown, should in principle be a unique set of values valid for all $\bm{x}_c$ \cite{Bayarri2007,Arendt2012}.

% Probability Model for Model Discrepancy, Gaussian Process for Model Discrepancy
The unknown model bias function $\delta$ can be represented as a random function $\mathcal{D} (\circ)$,
\begin{equation}
        (\mathcal{Y}_T - y_M(\bm{x}_c, \hat{\bm{x}}_m, \bm{\lambda})) \equiv \mathcal{D}(\bm{x}_c, \bm{\lambda}) \thicksim p(\delta | \bm{\psi}_{\delta}, \bm{x}_c, \bm{\lambda})
\label{eq:bc_data_generating_bias}
\end{equation}
where $\bm{\psi}_{\delta}$ is the parametrization of the probability density describing the bias at $\bm{x}_c$ and $\bm{\lambda}$.

Casting the unknown model bias term as a stochastic process is the salient feature of Bayesian calibration framework proposed by Kennedy and O'Hagan \cite{Kennedy2001}.
\marginpar{Gaussian process formulation}
In particular, a stationary \glsfirst[hyper=false]{gp} $\mathcal{D} (\bm{x}_c, \bm{\lambda})$ on $\mathbf{X}_C \subseteq \mathbb{R}^{D_c}$ and on $\bm{\Lambda}$ is used to represent the term.
That is,
\begin{equation}
        \mathcal{D}(\circ, \circ) \thicksim \mathcal{GP}(m_\delta(\circ,\circ; \bm{\psi}_{\delta}), K_\delta((\circ,\circ), (\circ, \circ); \bm{\psi}_{\delta}))
\label{eq:bc_data_generating_bias_gp}
\end{equation}
where $m_\delta$ and $K_\delta$ are the mean function and the covariance function of the \gls[hyper=false]{gp}, respectively;
and $\bm{\psi}_{m}$ is the hyper-parameters associated with the specification of the \gls[hyper=false]{gp} for the model bias function (e.g., its covariance kernel, see Chapter~\ref{ch:gp_metamodel}).
Under a \gls[hyper=false]{gp} formulation, the notion of \emph{systematic} bias mentioned previously is described statistically in terms of the mean and the covariance \cite{Reichert2012}.

For a selected values of $\bm{x}_c$ and $\bm{\lambda}$, the \gls[hyper=false]{gp} becomes a Gaussian random variable,
\begin{equation}
        \mathcal{D}(\bm{x}_c, \bm{\lambda}) \thicksim \mathcal{N}(m_\delta(\bm{x}_c, \bm{\lambda}; \bm{\psi}_{\delta}), s^2_\delta(\bm{x}_c, \bm{\lambda}; \bm{\psi}_{\delta}))
\label{eq:bc_data_generating_bias_gp_restricted}
\end{equation}
where $s^2_\delta$ is the standard deviation at controllable input $\bm{x}_c$ observed at $\bm{\lambda}$, under the parametrization $\bm{\Psi}_\delta$ of the \gls[hyper=false]{gp}.
Finally, for observations on multiple combinations of the controllable inputs and/or the complete observation layout $\bm{\Lambda}$, the \gls[hyper=false]{gp} becomes a multivariate Gaussian random variable, taking into account correlations of the bias at different elements of the observation layout,  
\begin{equation}
        \mathcal{D}(\bm{x}_c, \bm{\Lambda}) \thicksim \mathcal{N}(m_\delta(\bm{x}_c, \bm{\Lambda}; \bm{\psi}_{\delta}), \Sigma_\delta(\bm{x}_c, \bm{\Lambda}; \bm{\psi}_{\delta}))
\label{eq:bc_data_generating_bias_gp_multivariate}
\end{equation}
where $\Sigma_\delta$ is the (symmetric) covariance matrix of the bias at controllable input $\bm{x}_c$ observed on $\bm{\Lambda}$, under the parametrization $\bm{\Psi}_\delta$ of the \gls[hyper=false]{gp}.
The size of the matrix is $P \times P$, with $P$ the sum of the number of different combinations of the controllable inputs and the number of elements in the observation layout.

% Why bias, unbiased model
Incorporating bias term in the calibration procedure is important to avoid overfitting in the model parameters estimates.
\marginpar{Model without bias, illustrated}
To illustrate this idea, consider a calibration of a computer simulator without the presence of bias, with a single uncertain model parameter and a single controllable input $x_c$ as shown in Fig.~\ref{fig:ch5_plot_illustrate_bias_1}.
The thin black lines between the two bounding thick black lines indicate simulator prediction at different values of the model parameter.
As can be seen, the range of the model parameter values can in principle be constrained to match the observed data (crosses) within the observation uncertainty.
Furthermore, the range of the model parameters will increasingly become smaller with increasing number of data (such that the associated observation uncertainty becomes increasingly narrow as well).
In other words, the calibrated model parameter converges to the ``true'' value \cite{Bayarri2007,OHagan2013,Brynjarsdottir2014}.
This parameter value will be valid for prediction outside the calibration domain (i.e., extrapolation at different values of controllable inputs where no data has been observed).
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:ch5_plot_illustrate_bias},
                  maincaption={Illustration of predictions made by computer simulator with and without bias, both with an uncertain model parameter and a controllable input $x_c$. Crosses are the observed data along with the associated uncertainty taken at different controllable inputs $x_c$. Bold lines are the simulator prediction using the maximum and minimum of the uncertain model parameter, thin lines are the prediction with varying values of the model parameter, and dotted lines are the prediction outside the calibration domain.},%
									mainshortcaption={Illustration of predictions made by computer simulator with and without bias, both with a single uncertain model parameter and a single controllable input $x_c$.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:ch5_plot_illustrate_bias_1},
                  leftcaption={Model without bias and with a single uncertain model parameter.},
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:ch5_plot_illustrate_bias_2},
                  rightcaption={Model with bias and with a single uncertain model parameter.}
                 ]
{../figures/chapter5/figures/plotIllustrateBias_1.pdf}
{../figures/chapter5/figures/plotIllustrateBias_2.pdf}

% Why bias, biased model
On the other hand,
some simulators would have apparent bias such that their predictions would remain inconsistent with the observed data, regardless the choice of the model parameter value (Fig.~\ref{fig:ch5_plot_illustrate_bias_2}).
\marginpar{Model with bias, illustrated}
Calibration can still be conducted such that the discrepancy between data and prediction is minimized in some sense (i.e., some kind of \emph{best-fitting} model parameter value).
The calibrated parameter would be able to predict calibration data well, but not for prediction outside the calibration domain.
The situation becomes more problematic when more precise data becomes available such that uncertainty associated with the observed data becomes narrower.
In that situation, the uncertainty associated with the calibrated model parameter will also become narrower up to a point value.
\marginpar{Overfitting}
This illustrates the two symptoms of overfitting the model parameter: the calibrated model parameter is \emph{biased} (i.e., having a wrong value) and the distribution characterizing its uncertainty is \emph{degenerate} (i.e., increasingly sure on the wrong value with higher precision of the observed data).
The latter symptom is particularly troublesome as it inflates the degree of confidence one has on the prediction.

% Physics-based Simulator
The situation of a biased model is prevalent in complex physics-based simulators, whose constituent physical models were developed using scientific theory and supported by experimental data.
\marginpar{Physics-based simulators}
This approach forms the scientific basis for making prediction, especially in the region outside the calibration domain \cite{Arhonditsis2008}.
It is hoped that such an approach would be more robust than using purely statistical model of observed data \cite{Bayarri2007,Reichert2012}.
However, certain degree of simplifications from numerical approximation to ignored physical process due to lack of knowledge are expected to persist.
Furthermore, the strong scientific foundation and the experimental data support of physical models often only apply to the separate constituent models of a complex simulator \cite{Campbell2006}.
In practice, the simulator consolidates numerous models to simulate the behavior of a (more) complex system outside the calibration domain.
As such, it can also be expected that the predictions from such simulators would exhibit certain degree of bias (from the true value) that is unknown a priori.

One might argue that if a model is known to be biased it simply requires more developmental effort to correct the bias by putting additional models for the missing physical processes.
However, as argued in \cite{Arhonditsis2008,Wulff2007}, this approach might not be the best solution as additional models often require even more model parameters to be calibrated and thus call for even more supporting data that cannot be met.
Additionally, as noted in \cite{Campbell2006,Bayarri2007,Brynjarsdottir2014}, it is often impractical (or unrealistic) for an analyst to revise the inner workings of a large complex simulator.
Yet, to wait until a better simulator is available before making any prediction is simply not constructive.

% Why model bias might help
In a Bayesian framework, the statistical description of the model bias term can potentially alleviate the problem overfitting.
\marginpar{Statistical description of the model bias}
Because the model parameters and the model bias are not fully identifiable according to Eq.~(\ref{eq:bc_true_simulation})\footnote{that is, without further prior information, arbitrary choice of $\hat{\bm{x}}_m$ fits the data perfectly well for arbitrary choice of $\delta$. In other words, the two terms are \emph{confounds}.}, 
having more precise data will not make the uncertainty associated with the calibrated model parameters to collapse (i.e., its distribution becomes degenerate) \cite{Bayarri2007,Brynjarsdottir2014}.
Whether the calibrated model parameters and the associated uncertainty are applicable for extrapolation outside the calibration domain, however, depends on whether the bias term is modeled properly \cite{Bayarri2007,Arhonditsis2008,Arendt2012,OHagan2013,Brynjarsdottir2014,Ling2014}.
Thus, such a statistical description of the model bias is not a magic bullet in the calibration of a biased model.
It does, however, provides additional flexibility in incorporating either prior knowledge or a prior expectation regarding model deficiency.
%This way, it also provides additional channel for assessing the suitability of the simulator to make prediction.

% Physical Parameter vs. Tuning Parameter, some philosophical issue
At this point, it is worth revisiting the meaning of calibrated parameters in a simulator with bias.
In a simulator without bias it is straightforward to justify the calibrated model parameters as the ``true'' parameter values of the specified model.
\marginpar{physical parameters, ``true'' values}
If the model is physics-based then the parameters also correspond to a physical parameter.
As argued in \cite{OHagan2013,Brynjarsdottir2014}, physical parameters often have meaning outside the world described by the model where the parameters currently reside.
Furthermore, having a true value, such physical parameters would be generally applicable to extrapolation outside the calibration domain.

On the contrary, as illustrated in one of the examples above, calibrated model parameters in a simulator with bias act as best-fitting parameters that allow the simulator to fit, in some sense, the calibration data.
\marginpar{Tuning parameters, best-fitting values}
Incorporating model bias term might help in alleviating the problem of overfitting, but the a priori arbitrariness of the model bias term confounds with the model parameters itself, making the resulting calibrated model parameters more difficult to interpret \cite{Higdon2004}.
As such, in practice, it is important to emphasize that calibrated model parameters in a simulator with bias will simply be optimal under particular assumptions (e.g., criteria, model bias term, etc.) \cite{Campbell2006}.
Ref.~\cite{Brynjarsdottir2014} went further by arguing that such model parameters (tuning) had limited scientific values and would not help for extrapolation.

This thesis takes a more pragmatic approach regarding this dichotomy: the distinction is rather irrelevant.
It is awkward to discuss the true and wrong values of model parameters if the model itself is considered biased (i.e., wrong).
\marginpar{A pragmatic view}
In such cases, the notion of true parameter values is difficult to justify, the model parameters might not have strict physical meaning and may not be of interest in their own right.
And yet, in a complex physics-based simulator (where possible systematic bias cannot be excluded), many of these model parameters are being used in conditions different from their calibration domain, regardless of the conceptual distinction (e.g., Refs.~\cite{Arendt2012,USNRC2012}).
Thus, the calibration of model parameters based on the available experimental data should be aimed such that the simulator remains applicable when it is applied outside its calibration domain\footnote{or more eloquently in the words of Leamer \cite{Saltelli2006}, the resulting uncertainty associated with calibrated model parameters is:``...wide enough to be credible and the corresponding interval of inferences is narrow enough to be useful''.}.
The Bayesian framework accommodates this aim of calibration in a flexible manner by taking into account multiple source of uncertainties through selection of prior uncertainties both for model parameters and for model bias which eventually results in the associated posterior uncertainties.

%------------------------------------------------------------------------------------
\subsection{Probabilistic Model for the Experimental Data}\label{sub:bc_modular_data}
%------------------------------------------------------------------------------------

% Introductory paragraph
Now recall the relationship between the true system response and its observation through a measurement (Eq.~(\ref{eq:bc_observation_true})),
\begin{equation*}
    y_E(\bm{x}_c, \boldsymbol{\lambda}) = y_T (\bm{x}_c, \boldsymbol{\lambda}) + \epsilon
\end{equation*}
The observation error term $\epsilon$ represents any possible error during the measurement process, either from the imprecision of the instrument or any other residual variability of the experiment.
\marginpar{Observation error, possible origins}
This variability, in turn, might be due to the inherently stochastic nature of the physical process (irreducible) or unrecognized and uncontrolled variables (reducible) \cite{Kennedy2001}.

% Generic Formulation
Because this term is considered unknown, a stochastic process is defined on the observation layout,
\begin{equation}
        \mathcal{E}(\bm{\lambda}) \thicksim p(\epsilon | \psi_{\epsilon}, \bm{\lambda})
\label{eq:bc_data_generating_exp}
\end{equation}
where $\bm{\psi}_{\epsilon}$ is the parametrization of the \gls[hyper=false]{pdf} describing the observation error $\bm{\lambda}$.
That is, it depends on which response is observed, as well as where and when it is observed.

% Independence assumption and its justification
An important assumption made on the distribution of the observation error is that it is independent conditional on the true value of the system response.
\marginpar{Conditional independence}
One can argue that the measurement data points taken from a spatio-temporal physical process would have (perhaps complicated) correlation structure among them.
But intuitively, as argued in \cite{Wikle2001}, this structure becomes much simplified once the true value is known;
it can mainly be attributed to the residual variability and instrument precision with a simpler description.
The true system response itself is already separately formulated in terms of the simulator prediction and a model bias term (Eq.~(\ref{eq:bc_true_simulation})).
As such, any possible complicated structure of the error (either bias or correlation) is already assigned to the model bias formulation and assuming a simpler measurement error model (i.e., independent) is sufficient \cite{Wikle2001}.
At the same time, as noted in \cite{Kennedy2001,Bayarri2007}, it will be difficult to distinguish two correlation structures separately for the model bias and observation error based on the data alone.

% Gaussian Assumption, some comments
The particular distribution of the observation error is often assumed to be a Gaussian in the
\marginpar{Gaussian observation error}
applied literature \cite{Wikle1998,Wikle2001,Kennedy2001,Bayarri2007,Arhonditsis2008},
\begin{equation}
        \mathcal{E} \thicksim \mathcal{N}(0, \sigma_{obs}^2(\bm{\lambda}))
\label{eq:bc_data_generating_exp_gaussian_1}
\end{equation}
or equivalently following the conditional independence assumption explained above,
\begin{equation}
  \mathcal{Y}_E | \mathcal{Y}_T = y_T(\bm{x}_c, \boldsymbol{\lambda}) \thicksim \mathcal{N}(y_T(\bm{x}_c, \boldsymbol{\lambda}), \sigma_{obs}^2(\bm{\lambda}))
\label{eq:bc_data_generating_exp_gaussian_2}
\end{equation}
where $\sigma_{obs}^2$ is the variance of the Gaussian distribution and the only hyper-parameter of this observation error specification.
The value of the variance depends on the element of the observation layout $\bm{\lambda}$.
Eq.~(\ref{eq:bc_data_generating_exp_gaussian_1}) implies that the observation is taken without bias and the error is independent (but need not be identically distributed) Gaussian random variable.

%---------------------------------------------------------------------------------
\subsection{Probabilistic Model for the Simulator}\label{sub:bc_modular_simulator}
%---------------------------------------------------------------------------------

% Introductory paragraph
For a deterministic simulator $y_M$,
the probabilistic modeling of the bias term $\delta$ and the observation error term $\epsilon$ are enough to formulate a probabilistic model for the experimental observation $\mathcal{Y}_E$.
However, following the development taken in Chapter~\ref{ch:gp_metamodel},
a \glsfirst[hyper=false]{gp} can also be used to represent a deterministic simulator using an explicit formulation of a stochastic process.
The prediction made by the simulator at particular values of $\bm{x}_c$, $\hat{\bm{x}}_m$, and $\bm{\lambda}$ is then given by,
\begin{equation}
	\mathcal{Y}_M (\bm{x}_c, \hat{\bm{x}}_m; \bm{\lambda}) \thicksim  \mathcal{N}(m(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\lambda}), s^2(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\lambda}))
\label{eq:bc_data_generating_simulator_gp}
\end{equation}
where $m$ and $s^2$ is the kriging mean and the kriging variance, respectively (see Section~\ref{sec:gp_metamodeling});
and $\bm{\psi}_{m}$ is the hyper-parameters associated with the specification of the \gls[hyper=false]{gp} (e.g., its covariance kernel).

This step is taken especially if the simulator is computationally expensive to evaluate and only a limited number of simulator runs can be afforded \cite{Kennedy2001,Bayarri2007,Arendt2012}.
The probability model in Eq.~(\ref{eq:bc_data_generating_simulator_gp}) then becomes an approximation to the actual simulator (i.e., a \gls[hyper=false]{gp} metamodel).
Furthermore, as explained in the Chapter~\ref{ch:gp_metamodel}, the uncertainty associated with a prediction by the metamodel at an arbitrary input point stems from the fact that the simulator itself was not run at that input.
This prediction is based on the outputs of which the simulator was run (i.e., the training data)\footnote{the statement conditional on the training data in Eq.~(\ref{eq:bc_data_generating_simulator_gp}), i.e., $\mathcal{Y}_M (\bm{x}_c, \hat{\bm{x}}_m; \bm{\lambda}) | \mathcal{Y}(\mathbf{DM})$ has been implicitly assumed.}.
As such, in this case, the uncertainty has an epistemic interpretation.

%-----------------------------------------------------------------------------
\subsection{Posterior of the Model Parameters}\label{sub:bc_modular_posterior}
%-----------------------------------------------------------------------------

% Introductory Paragraph
Summarizing the above discussions for a deterministic simulator $y_M$,
\marginpar{Data generating process, general}
\begin{equation}
    \begin{split}
				& \mathcal{Y}_M \equiv \mathcal{Y}_M \thicksim p(y_M | \hat{\bm{x}}_m, \bm{x}_c; \bm{\lambda}) = \delta_d (y_M - y_M(\hat{\bm{x}}_m, \bm{x}_c; \bm{\lambda})) \\
        & (\mathcal{Y}_T - \mathcal{Y}_M) \equiv \mathcal{D}(\bm{x}_c; \bm{\lambda}) \thicksim p(\delta | \bm{\psi}_{\delta}, \bm{x}_c; \bm{\lambda}) \\
        & (\mathcal{Y}_E - \mathcal{Y}_T) \equiv \mathcal{E}(\bm{\lambda}) \thicksim p(\epsilon | \bm{\psi}_{\epsilon}; \bm{\lambda}) \\
    \end{split}
\label{eq:bc_data_generating_models}
\end{equation}
where $\delta_d$ is the Dirac delta function indicating that the simulator prediction is exact (i.e., a \emph{degenerate} density).

Suppose that the form of the densities in Eq.~\ref{eq:bc_data_generating_models} are already given,
then the stochastic process $\mathcal{Y}_E$ is obtained by adding the terms on the right hand side of Eq.~\ref{eq:bc_true_simulation}.
Assuming that they are independent, the \gls[hyper=false]{pdf} of $\mathcal{Y}_E$ is defined as the convolution of the terms,
\begin{equation}
  \begin{split}
  p(y_E | & \bm{\psi}_{\delta}, \bm{\psi}_{\epsilon}, \hat{\bm{x}}_m, \bm{x}_c ; \bm{\lambda}) = \ldots \\
	& (p(y_M(\hat{\bm{x}}_m, \bm{x}_c; \bm{\lambda})) * p(\delta | \bm{\psi}_{\delta}, \bm{x}_c; \bm{\lambda}) * p(\epsilon | \bm{\psi}_{\epsilon}; \bm{\lambda}))(y_E)
  \end{split}
\label{eq:bc_additive_convolution}
\end{equation}
where $*$ is the symbol for the convolution operation.

% Normal Approximation
Following the Gaussian distribution formulations for the model bias, the observation error, and the
\marginpar{Data generating process, Gaussian}
simulator approximation, a normal likelihood for the calibration problem can be obtained as follows,
\begin{equation}
    \begin{split}
				& \mathcal{Y}_E = \mathcal{Y}_M + \mathcal{D} + \mathcal{E} \\
				& \mathcal{Y}_M (\bm{x}_c, \hat{\bm{x}}_m; \bm{\lambda}) \thicksim \mathcal{N}(m_M(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\lambda}), s_M^2(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\lambda})) \\
        & \mathcal{D}(\bm{x}_c; \bm{\lambda}) \thicksim \mathcal{N}(m_\delta(\bm{x}_c; \bm{\psi}_\delta, \bm{\lambda}), s_\delta^2(\bm{x}_c; \bm{\psi}_\delta, \bm{\lambda})) \\
        & \mathcal{E}(\boldsymbol{\lambda}) \thicksim \mathcal{N}(0, \sigma_{obs}^2(\bm{\lambda})) \\
    \end{split}
\label{eq:bc_data_generating_models_gaussian}
\end{equation}
As such, the data generating process $\mathcal{Y}_E$ under Gaussian formulation above is
\begin{equation}
	\begin{split}
		& \mathcal{Y}_E \thicksim \mathcal{N}(m_*, s^2_*) \\
		& m_*(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_m, \bm{\psi}_\delta, \bm{\lambda}) = m_M(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_m, \bm{\lambda}) + m_\delta(\bm{x}_c; \bm{\psi}_\delta, \bm{\lambda}) \\
		& s^2_*(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_m, \bm{\psi}_\delta, \sigma_{obs}^2, \bm{\lambda}) = s_M^2(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\lambda}) + s_\delta^2(\bm{x}_c; \bm{\psi}_\delta, \bm{\lambda}) + \sigma_{obs}^2(\bm{\lambda})
	\end{split}
\label{eq:bc_data_model_gaussian}
\end{equation}
where $m_*$ and $s^2_*$ are the mean and the standard deviation of the experimental data generating process under Gaussian formulation, respectively.

% Generic Likelihood
Given a set of experimental data $\mathbf{y}$ taken at $\mathbf{x}_c$ and observed on an observation layout $\bm{\Lambda}$,
\marginpar{Likelihood function}
the likelihood function is then defined as follows
\begin{equation}
  \mathcal{L}(\hat{\bm{x}}_m, \bm{\psi}_\delta, \bm{\psi}_\epsilon; \mathbf{y}, \mathbf{x}_c, \bm{\Lambda}) \equiv p(y_E = \mathbf{y} | \bm{x}_c = \mathbf{x}_c, \hat{\bm{x}}_m, \bm{\psi}_\delta, \bm{\psi}_{\epsilon} ; \bm{\Lambda})
\label{eq:bc_likelihood}
\end{equation}
Under Gaussian formulation, the likelihood function is obtained by using Gaussian density of Eq.~(\ref{eq:bc_data_model_gaussian}) for $p$.
Note that if the set of experimental data is simultaneously given on the observation layout $\bm{\Lambda}$ then the covariance matrix $\Sigma_*$ is used instead of the standard deviation $s^2_*$,
\begin{equation}
	\begin{split}
	\Sigma_*(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_m, \bm{\psi}_\delta, \bm{\psi}_\epsilon, \bm{\Lambda}) = & \Sigma_M(\bm{x}_c, \hat{\bm{x}}_m; \bm{\psi}_{m}, \bm{\Lambda}) + \ldots \\ 
	& \Sigma_\delta(\bm{x}_c; \bm{\psi}_\delta, \bm{\Lambda}) + \Sigma_{obs}(\bm{\psi}_\epsilon; \bm{\Lambda})
	\end{split}
\label{eq:bc_gaussian_covariance_matrix}
\end{equation}
where $\Sigma_M$, $\Sigma_\delta$, and $\Sigma_{obs}$ are the $P \times P$ covariance matrices of the observation under their respective parametric kernels, with $P$ the dimension of the experimental data.

% Full probability model
According to the Bayes' theorem, the joint posterior probability of the model parameters $\bm{x}_m$ and
\marginpar{Joint posterior density}
the hyper-parameters associated with the model bias and the observation error is given as, 
\begin{equation}
	\begin{split}
  p(\hat{\bm{x}}_m, & \bm{\psi}_\delta, \bm{\psi}_{\epsilon_y} | \mathbf{y}, \mathbf{x}_c; \bm{\Lambda}) = \ldots \\
	& \frac{\mathcal{L}(\hat{\bm{x}}_m, \bm{\psi}_\delta, \bm{\psi}_{\epsilon_y} ; \mathbf{y}, \mathbf{x}_c, \bm{\Lambda}) \cdot p(\hat{\bm{x}}_m) \cdot p(\bm{\psi}_{\epsilon_y}; \bm{\Lambda}) \cdot p(\bm{\psi}_{\delta}; \bm{\Lambda})}{p(y_E = \mathbf{y} | \bm{x}_c = \mathbf{x}_c ; \bm{\Lambda})}
	\end{split}
\label{eq:bc_joint_posterior}
\end{equation}
where $p(\hat{\bm{x}}_m)$, $p(\psi_{\bm{\epsilon}_y}; \bm{\Lambda})$, and $p(\bm{\psi}_{\delta}; \bm{\Lambda})$ are the prior probabilities for the model parameters, the model bias hyper-parameters, and the observation error hyper-parameters, respectively.

The denominator of the Eq.~(\ref{eq:bc_joint_posterior}) is a normalizing constant with respect to the model parameters and the hyper-parameters such that Eq.~(\ref{eq:bc_joint_posterior}) is a valid probability density (i.e., integration over the domain yields the value $1.0$).
\marginpar{Normalizing constant}
As such, it is defined as a multidimensional integral of the following,
\begin{equation}
	\begin{split}
	p(y_E = \mathbf{y} | \bm{x}_c = \mathbf{x}_c ; \bm{\Lambda}) = & \int \mathcal{L}(\hat{\bm{x}}_m,\bm{\psi}_\delta, \bm{\psi}_\epsilon ;  \mathbf{y}, \mathbf{x}_c, \bm{\Lambda}) \cdot \ldots \\
	& p(\hat{\bm{x}}_m) \cdot p(\bm{\psi}_\epsilon; \bm{\Lambda}) \cdot p(\bm{\psi}_\delta; \bm{\Lambda}) \, d\hat{\bm{x}}_m d\bm{\psi}_\epsilon d\bm{\psi}_\delta
	\end{split}
\label{eq:bc_normalizing_constant}
\end{equation}

The specifications of the likelihood and the associated priors completely specify the Bayesian statistical calibration framework for the model parameters.
An inference of the model parameters can be made based on the resulting probability model through a simulation method outlined in Section~\ref{sec:bc_mcmc}.
However, besides the model parameters and controllable inputs, the complete formulation above also involves additional parameters associated with the statistical models: $\bm{\psi}_\delta$, $\bm{\psi}_{obs}$, and $\bm{\psi}_M$ the hyper-parameters for the model bias, the observation error, and the simulator approximation (if apply), respectively.
In principle, they are now also part of the calibration problem, increasing the size (in dimension) and the complexity of it.
To simplify the problem, a modularized approach is taken in this thesis as briefly explained in the following section.

%---------------------------------------------------------------------------------
\subsection{Modularization of the Bayesian Framework}\label{sub:bc_modularization}
%---------------------------------------------------------------------------------

%The actual forms of the densities in 
%then, under the additive formulation, the data generating process for $\mathcal{Y}_E$ can be obtained by adding all the three terms above.
