%*********************************************************************************
\section{Objectives and Scope of the Thesis}\label{sec:intro_objectives_and_scope}
%********************************************************************************* 

With a larger context provided above,
this section presents briefly and specifically the statement of the problem,
followed by the objectives as well as the scope of the present doctoral research.

%--------------------------------------------------------------------------
\subsection{Statement of the Problem}\label{sub:intro_statement_of_problem}
%--------------------------------------------------------------------------

% Introductory Paragraph
The development of closure laws for reflooding described in \cite{Nelson1992,USNRC2012} showed the difficulties and the amount of assumptions used.
In a nutshell, system code development is an effort to consolidate correlations and mechanistic models, to create a phenomenological-based simulation code that can provide best-estimate results.
This consolidated effort results in a code that can simulate a wide range of transients foreseen in nuclear power plant operation in a best-estimate manner.
Alas, to come up with a consistent set of closure laws is a great challenge for code developers.

% Closure Laws Difficulty, Conceptual
The closure laws required to close the two-fluid model pose particularly difficult challenges \cite{Wulff2007}.
For instance, to have a correlation of heat transfer between the wall and the fluid, temperature data from each of the constituents are needed (i.e., the wall, the liquid phase, and the gas phase).
But measuring temperature of the individual phases in an arbitrary interfacial topology has its own technical difficulties to the extend that no such data is available to be implemented in the closure laws.
Additionally, the experiments to obtain hydrodynamic closure laws (e.g., interfacial friction factor, wall friction factor, etc.) were generally carried out in adiabatic conditions.
As a result, this excludes the coupling of any heat transfer phenomena between the phases and the wall in such correlation.

% Closure Laws Difficult, Practical
Furthermore, during the development of a simulation code, programming considerations also came into the picture.
For robustness, simplification is often required and continuity is enforced.
Transitionary flow regime between two known (observed) flow regimes for which experimental data is not available is modeled to be the average of the two bounding regimes.
Different code developments, which used different assumptions and experimental databases, come up with different set of closure laws with their own parametrization (see for instance \cite{Nelson1992} for TRAC code and \cite{Bestion1990} for CATHARE code).
Several authors have expressed their concerns about the uncertainty stemming from the closure laws \cite{Wulff2007,Petruzzi2008a,DAuria2012}.

% an Illustration
As an example of the above point, consider that in the \gls[hyper=false]{trace} code, after some derivations the interfacial drag coefficient closure law in the inverted slug flow regime $C_{i,\text{IS}}$ is given by,
\begin{equation*}
	C_{i,\text{IS}} = \hat{x}_{m,\text{SET}} \times \frac{1}{24} \frac{\rho_g}{\text{La}} \frac{(1-\alpha)}{\alpha^{1.8}} \,\,\,;\,\,\, \hat{x}_{m,\text{SET}} = 0.75 
\label{eq:intf_drag_isf}
\end{equation*}
where $\rho_g$ is the density of the gas phase;
$\text{La}$ is the Laplace number;
$\alpha$ is the void fraction;
and $\hat{x}_{m,\text{SET}}$ is a fitting parameter.

There are several remarks that can be made about the closure law given above.
First, the second term in the right-hand side was derived from experimental data but not directly.
In the inverted slug regime, saturated liquid core breaks up into ligaments.
These ligaments are \emph{assumed} to take form as prolate ellipsoid.
The drag coefficient is then taken from the experimental database of coefficient for distorted droplet.
Then to take into account the multi-particle effect, the coefficient is divided by the void fraction $\alpha$ raised to the power of $1.8$ (this, in turn, was taken from experimental data of inertial regime).
Lastly, the first term of the equation, $\hat{x}_{m,\text{SET}} = 0.75$ was added \emph{to calibrate against} the experimental data from the FLECHT-SEASET reflood experimental facility.
This first term, although clearly \emph{non-physical} in nature, is nevertheless an important tuning parameter of the model.
Its uncertainty should be considered in uncertainty analysis, especially when reflood is expected to occur.
Yet, no statement regarding the associated uncertainty is given.
Several other such terms exist \cite{USNRC2012}. 

% Statement of Problem
As illustrated above, it is clear that models in thermal-hydraulics system code are, to a certain extent, limited.
Various experimental programs were carried out to gain better understanding of important phenomena,
and to validate (and, as noted, to calibrate) the models.
Series of the experiments, carried out in \glspl[hyper=false]{setf} with well-specified boundary conditions were aimed to reproduce limited part of the transient in a selected component following a postulated scenario.
For example, in the case of reflooding, several facilities existed and data was gathered (FEBA, PERICLES, etc.).
But, there has not been an orchestrated effort to incorporate the accumulated data into the calibration process of the physical models, in a systematic way, while acknowledging the multiple sources of uncertainty in the process.

%--------------------------------------------------
\subsection{Objectives}\label{sub:intro_objectives}
%--------------------------------------------------

% Introductory (Overall Objective)
The purpose of the doctoral research is to quantify the uncertainty of physical model parameters
implemented in a thermal-hydraulics system code.
The physical models of interest describe the phasic interactions in a complex multiphase flow during a reactor transient, namely heat, mass, and momentum exchanges between vapor, water and structures.
These models are parametrized by physical or empirical tuning parameters, the values of which are uncertain.
This results in uncertain code prediction of important safety quantities, such as the evolution of the fuel cladding temperature during a postulated reactor transient.

Adopting a probabilistic framework to conform to the statistical uncertainty propagation widely adopted in the field of nuclear engineering,
the uncertainties in the parameters are represented in form of \glsfirst[hyper=false]{pdf} or their approximations.
The derivation of these functions is posed as an inverse statistical problem following a Bayesian framework as the parameters themselves are not directly observable.
Although subjectivity cannot be removed completely from the analysis,
the research aims to develop a methodology to incorporate the available, albeit indirect, experimental data to better inform, in a more objective and transparent manner,
the uncertainties associated with code model parameters.
This is done by consolidating and adapting recent developments in the applied statistics literature:

% Aim 1 (Global sensitivity analysis)
\begin{enumerate}
	\item \emph{to analyze and to better understand} the inputs/outputs relationship in a computer simulation with uncertain inputs.
	This is aimed at answering the question whether the current physical model in thermal-hydraulics system code \gls[hyper=false]{trace} can be identified with the available experimental data from test facilities.
	In other words, how to select important parameters to be inferred.
	\Glsfirst[hyper=false]{gsa} methodologies can be used to assist in identifying which parameters can be calibrated using the available data.
	A test facility might have multiple types of data and although the information content might not be the same for the different types, it might be worthwhile to consider each one of them.
	Finally, for each of the different types,
	the analysis is also conducted on various derived \glspl[hyper=false]{qoi}, some of which explicitly consider the output as function.
	By doing so, it is hoped that interesting model behavior with respect to its parameters perturbation can be revealed.

% Aim 2 (Statistical Metamodeling)
	\item \emph{to approximate} the inputs/outputs relationship in a complex computer simulation for a faster evaluation.
	The step is required as the statistical calibration method adopted in thesis is computationally expensive, requiring numerous code runs in the order of hundreds of thousands and beyond.
	This approximation is done through a \glsfirst[hyper=false]{gp} metamodel resulting in a statistical metamodel.
	The highly multivariate nature of the outputs (time- and space-dependent) is dealt by a dimension reduction technique.
	Build upon the results of previous step, only parameters that are identified to be influential are included in the construction of the metamodel.

% Aim 3 (Bayesian Calibration)
	\item \emph{to calibrate} the physical model parameters against various relevant experimental data.
	The word \emph{to calibrate} carries a disparaging interpretation related to \emph{to tweak}.
	However, using a Bayesian statistical framework, the aim of calibration is extended to simultaneously quantify the uncertainty of the parameter estimation.
	The framework includes various sources of uncertainty which can be modeled probabilistically, including the model bias term.
	At the end, the parameters of interest will be either in the form of distributions conditioned on the data or samples generated from such distributions (i.e., posterior distribution) to conform with the practice of statistical uncertainty propagation widely adopted in the field of nuclear engineering.

\end{enumerate}

Finally, as the calibration is only conducted using experimental data in a limited set of experimental conditions,
it is important to validate the proposed methods by demonstrating the applicability of the results to the simulation of the phenomena in the same facility but in different experimental conditions.
That is to propagate the posterior uncertainty of the parameters and to compare the results against experimental data not used in the calibration step.
	
%----------------------------------------
\subsection{Scope}\label{sub:intro_scope}
%----------------------------------------

% Introductory paragraph
Although the proposed set of strategies in this research can be applicable to the analysis and calibration of any physical model of a system code,
\marginpar{Simulation of reflooding}
it is illustrated by its application on the models of particular importance during simulation of reflooding,
i.e., the so-called \gls[hyper=false]{postchf} flow regimes.
There are several reasons for this emphasis:
\begin{itemize}

	% Reason 1
	\item Reflooding is an important part in the simulation of \glspl[hyper=false]{lwr} transient during \gls[hyper=false]{loca}.
	Modeling reflooding determines the appropriate representation of the dynamics of heat transfer phenomena during the effort to rewet an uncovered core.
	Of paramount interest is the estimation of the time at which the rod can be expected to be rewet as well as the maximum temperature reached prior to rewet.
	Reflood is a transient with highly coupled hydrodynamic-heat-transfer effects and it challenges the assumption made on the implemented closure laws.
	Indeed, several reflood experimental programs conducted in \glspl[hyper=false]{setf} existed and were designed to validate reflood models in system codes.
	Unfortunately, no orchestrated effort was done so far to consolidate the generated data in general and into the \gls[hyper=false]{trace} code in particular.

% Reason 2
	\item The models are adequately complex.
  It is complex that $5$ flow regimes are involved in a single phenomena: multiple sub-models, parametrized with numerous inputs, with multivariate outputs (both time- and space-dependent).
	But as the source of data is from reflooding \glspl[hyper=false]{setf}, real plant system (and full scale) effects can be excluded and the ensuing analysis can be concentrated on a limited set of models.
	In fact, as already pointed out, reflooding \glspl[hyper=false]{setf} were designed to validate and (to calibrate) reflood models in system codes.

% Reason 3
	\item Multiple data of various types (pressure, temperature, etc.), taken with different experimental conditions (flow rate, system pressure, etc.), are typically available from experiment within the same facility.
	As calibration in the present research is conducted using one experimental condition, it is important to validate the resulting calibration result against the data with different experimental conditions albeit from the same experimental facility. 
	Moreover, additional data from other reflooding \glspl[hyper=false]{setf} are also available.
	This is important for validating the proposed method further and expanding it to calibration against data from multiple facilities. 

% Reason 4
	\item It is the model considered in the \gls[hyper=false]{premium} benchmark. Some of the works in this doctoral research was conducted as part of \gls[hyper=false]{psi} participation to the benchmark.
  Furthermore, qualitative comparison can be made with the results of other participants.
	
\end{itemize}

As such, while it is important to acknowledge that reflood simulation and the associated relevant model (or models) are only parts of a large and complex \gls[hyper=false]{th} system code,
they can provide a representative and relevant illustration on the particulars of analyzing and calibrating the code using experimental data from \gls[hyper=false]{setf} in general; providing a suitable testing ground for the proposed methods.

% Statistical nature
The methods and practices of sensitivity analysis, approximation, and calibration of computer model need not be statistical.
\marginpar{Statistical framework}
This thesis, however, focuses on the statistical approach for each of the aforementioned steps.
The main reasoning for this choice are twofold:
First, statistical methods tend to require less assumption on the model complexity.
While they may be more computationally expensive than their non-statistical counterparts,
they are also easier to set up, with minimal intrusion to the code itself, and subject to less severe dependence on the size of the problem with respect to the number of input parameters.
Secondly, the ultimate results of the model parameters calibration (i.e., its uncertainty quantification) should be represented in terms of probability.
As mentioned previously, this is to conform with the widely accepted practice of statistical uncertainty propagation in the nuclear engineering community.  
The motivations for this choice will become clearer in the subsequent section and they will also be elaborated further in their respective chapters.

% Closing
As a last note, the \glsfirst[hyper=false]{th} system code considered in this thesis is the \glsfirst[hyper=false]{trace} code developed by the \glsfirst[hyper=false]{usnrc}.
\marginpar{TRACE code}
The main reason to consider solely this particular code is the fact that \gls[hyper=false]{trace} is the thermal-hydraulics system code used for the purpose of Swiss \glspl[hyper=false]{npp} safety analysis conducted within the \glsfirst[hyper=false]{stars} program \cite{PSI2017} at the \glsfirst[hyper=false]{psi}.