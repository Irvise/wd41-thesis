%*********************************************************************************
\section{Objectives and Scope of the Thesis}\label{sec:intro_objectives_and_scope}
%********************************************************************************* 

With the larger context provided above,
this section presents briefly and specifically the statement of the problem,
followed by the objectives as well as the scope of the present doctoral research.

%--------------------------------------------------------------------------
\subsection{Statement of the Problem}\label{sub:intro_statement_of_problem}
%--------------------------------------------------------------------------

% Introductory Paragraph
System code development is an effort to consolidate correlations and mechanistic models to create a phenomenological-based simulation code that can provide best-estimate results.
This consolidated effort results in a code that can simulate a wide range of transients foreseen in \gls[hyper=false]{npp} operation in a best-estimate manner.
Alas, to come up with a consistent set of closure laws is a great challenge for code developers.

% Closure Laws Difficulty, Conceptual
The closure laws required to close the two-fluid model pose particularly difficult challenges \cite{Wulff2007}.
For instance, to have a correlation of heat transfer between the wall and the fluid, temperature data from the wall, and the liquid and gas phases are needed.
But measuring temperature of the individual phases in an arbitrary interfacial topology has its own technical difficulties to the extend that no such data is available to be implemented in the closure laws.
Additionally, the experiments to obtain hydrodynamic closure laws (e.g., interfacial friction factor, wall friction factor) were generally carried out in adiabatic conditions.
As a result, this excludes the coupling of any heat transfer phenomena between the phases and the wall in such correlations.

% Closure Laws Difficult, Practical
Furthermore, during the development of a code, programming considerations also came into the picture.
For robustness, simplification is often required and continuity is enforced.
Transitionary flow regime for which data is not available is often modeled to be the average of the two known bounding regimes.
Different code developments, which used different assumptions and experimental databases, come up with different set of closure laws with their own parametrization (see for instance \cite{Nelson1992} for TRAC code and \cite{Bestion1990} for CATHARE code).
Several authors have expressed their concerns about the uncertainty stemming from the closure laws \cite{Wulff2007,Petruzzi2008a,DAuria2012}.

% an Illustration
As an example of the above point, consider that in the \gls[hyper=false]{trace} code, after some derivations the interfacial drag coefficient closure law in the inverted slug flow regime $C_{i,\text{IS}}$ is given by,
\begin{equation*}
	C_{i,\text{IS}} = \hat{x}_{m,\text{SET}} \times \frac{1}{24} \frac{\rho_g}{\text{La}} \frac{(1-\alpha)}{\alpha^{1.8}} \,\,\,;\,\,\, \hat{x}_{m,\text{SET}} = 0.75 
\label{eq:intf_drag_isf}
\end{equation*}
where $\rho_g$ is the density of the gas phase;
$\text{La}$ is the Laplace number;
$\alpha$ is the void fraction;
and $\hat{x}_{m,\text{SET}}$ is a fitting parameter.

There are several remarks that can be made about the closure law given above.
First, the second term in the right-hand side was derived from experimental data but based on several simplifying assumptions.
In the inverted slug regime, saturated liquid core breaks up into ligaments.
These ligaments are \emph{assumed} to take form as prolate ellipsoid.
The drag coefficient is then taken from the experimental database of coefficient for distorted droplet.
Then to take into account the multi-particle effect, the coefficient is divided by the void fraction $\alpha$ raised to the power of $1.8$ (this, in turn, was taken from experimental data of inertial regime).
Lastly, the first term of the equation, $\hat{x}_{m,\text{SET}} = 0.75$ was added \emph{to calibrate against} the experimental data from the FLECHT-SEASET reflood experimental facility.
This first term, although clearly \emph{non-physical} in nature, is nevertheless an important tuning parameter of the model.
Its uncertainty should be considered in uncertainty analysis, especially when reflood is expected to occur.
Yet, no statement regarding the associated uncertainty is given.
Similar adjustment on several other closure laws exists \cite{USNRC2012}. 

% Statement of Problem
As illustrated above, it is clear that models in thermal-hydraulics system code are, to a certain extent, limited.
Various experimental programs were carried out to better understand the important phenomena,
and to validate (and, as noted, to calibrate) the models.
Series of experiments, carried out in \glspl[hyper=false]{setf} with well-specified boundary conditions were aimed to reproduce limited part of the transient in a selected component following a postulated scenario.
For example, in the case of reflooding, several facilities existed and data was gathered (FEBA, PERICLES, etc.).
But, there has not been an orchestrated effort to incorporate the accumulated data into the calibration process of the physical models, in a systematic way, while acknowledging the multiple sources of uncertainty in the process.

%--------------------------------------------------
\subsection{Objectives}\label{sub:intro_objectives}
%--------------------------------------------------

% Introductory (Overall Objective)
The purpose of this research is to quantify the uncertainty of physical model parameters implemented in a \gls[hyper=false]{th} system code.
The physical models of interest describe the phasic interactions in a complex multiphase flow during a reactor transient, namely heat, mass, and momentum exchanges between vapor, liquid and structures.
These models are parametrized by physical or empirical parameters, the values of which are uncertain.
This results in uncertain code predictions of important safety quantities, such as the evolution of the fuel cladding temperature during a postulated reactor transient.

Adopting a probabilistic framework to conform to the statistical uncertainty propagation widely adopted in the field of nuclear engineering,
the uncertainties in the parameters are represented as probability density functions (PDFs) or their approximations.
The derivation of these functions is posed as an inverse statistical problem following a Bayesian framework as the parameters themselves are not directly observable.
Although subjectivity cannot be removed completely from the analysis,
the research aims to develop a methodology to incorporate the available, albeit indirect, experimental data to inform in a more objective and transparent manner
the uncertainties associated with the model parameters.
This is done in three steps by consolidating and adapting recent developments in the applied statistics literature to:

% Aim 1 (Global sensitivity analysis)
\begin{enumerate}
	\item \emph{Analyze and better understand} the inputs/outputs relationship in a computer simulation with uncertain inputs.
	\Glsfirst[hyper=false]{sa}, in particular \glsfirst[hyper=false]{gsa}, methods can be used to assist identifying which of the model parameters can be calibrated using the available data.
  An uncertainty analysis often starts a with large list of input parameters that may and may not be relevant (i.e., influential) to the simulation at hand.
  A \emph{screening} method can be used to remove the least influential parameters from the list.
  Afterward, a variance decomposition method is employed to quantitatively analyze the contribution of the remaining influential parameters uncertainty on the prediction uncertainty.
	Multiple types of data can be measured during experiments in a test facility (e.g., cladding temperature, pressure drop, etc.), it might be worthwhile to consider each one of them.
  Finally, for each of the different types,
	the analysis is conducted on various derived \glspl[hyper=false]{qoi}, some of which explicitly consider the output as function.
	By doing so, it is hoped that interesting model behavior with respect to the input parameters perturbation can be revealed.
  
  Section~\ref{sub:intro_sa} provides an overview of the wide range of \gls[hyper=false]{sa} methods in the applied literature,
  while Chapter~\ref{ch:gsa} presents the details of the selected \gls[hyper=false]{sa} methods and their applications to a \gls[hyper=false]{trace} model.

% Aim 2 (Statistical Metamodeling)
	\item \emph{Approximate} the inputs/outputs relationship of a complex computer simulation for a faster evaluation.
	The step is required as the statistical calibration method adopted in this thesis is computationally expensive (requiring numerous code runs in the order of hundreds of thousands and beyond).
	This approximation is done through a \glsfirst[hyper=false]{gp} metamodel resulting in a statistical metamodel.
	The highly multivariate nature of the outputs (time- and space-dependent) is dealt by a dimension reduction technique.
	Build upon the results of the previous step, only parameters that are identified to be influential are included in the construction of the metamodel.
  
  Section~\ref{sub:intro_statistical_metamodeling} introduces a broad overview of metamodeling in the literature and Chapter~\ref{ch:gp_metamodel} presents the details and the application of \glsfirst[hyper=false]{gp} metamodel to a \gls[hyper=false]{trace} model.

% Aim 3 (Bayesian Calibration)
	\item \emph{Calibrate} the physical model parameters against various relevant experimental data.
	The word \emph{calibrate} carries a disparaging interpretation related to \emph{tweaking}.
	However, using a Bayesian statistical framework, the aim of calibration is extended to simultaneously quantify the uncertainty of the parameter estimation.
	The framework includes various sources of uncertainty, which can be modeled probabilistically, including the model bias term.
	At the end, the parameters of interest will either be given in the form of posterior \glspl[hyper=false]{pdf} conditioned on the data or samples generated from such distributions to conform with the practice of statistical uncertainty propagation widely adopted in the field of nuclear engineering.
  
  Section~\ref{sub:intro_bayesian_calibration} provides the practice of Bayesian calibration of computer model from the literature.
  Chapter~\ref{ch:bayesian_calibration} the details the formulation of a Bayesian calibration problem for model parameters, the ways to solving it, and an application of it to a \gls[hyper=false]{trace} model.

\end{enumerate}

Finally, as the calibration is only conducted using experimental data in a limited set of experimental conditions,
it is important to validate the proposed methods by demonstrating the applicability of the results to the simulation of the phenomena in the same facility but in different experimental conditions.
That is to propagate the posterior uncertainty of the parameters and to compare the results against experimental data not used in the calibration step.
	
%----------------------------------------
\subsection{Scope}\label{sub:intro_scope}
%----------------------------------------

% Introductory paragraph
Although the proposed set of strategies in this PhD research work can be applicable to the analysis and calibration of any physical model of a system code,
\marginpar{Simulation of reflooding}
it is illustrated by its application on the models of particular importance during simulation of reflooding,
i.e., the so-called \gls[hyper=false]{postchf} flow regimes.
There are several reasons for this emphasis as recognized by the \gls[hyper=false]{bemuse} and \gls[hyper=false]{premium} projects (see Section\ref{sub:intro_premium}):
\begin{itemize}

	% Reason 1
	\item Reflooding is an important part in the simulation of \glspl[hyper=false]{lwr} transient during \gls[hyper=false]{loca}.
	Modeling reflooding determines the appropriate representation of the dynamics of heat transfer phenomena during the effort to rewet an uncovered core.
	Of paramount interest is the estimation of the time at which the rod can be expected to be rewet as well as the maximum temperature reached prior to rewet.
	Reflood is a transient with highly coupled hydro\-dynamic-heat-transfer effects and it challenges the assumption made on the implemented closure laws.
	Indeed, several reflood experimental programs conducted in \glspl[hyper=false]{setf} exist.
	Unfortunately, no orchestrated effort was made so far to consolidate the generated data in general and into the \gls[hyper=false]{trace} code in particular.

% Reason 2
	\item The models are adequately complex.
  It is complex that $4$ flow regimes are involved in a single phenomena: multiple sub-models, parametrized with numerous inputs, with multivariate outputs (both time- and space-dependent).
	But as the source of data is from reflooding \glspl[hyper=false]{setf}, real plant system (and full scale) effects can be excluded and the ensuing analysis can be concentrated on a limited set of models.
	In fact, as already pointed out, reflooding \glspl[hyper=false]{setf} were designed to validate and to calibrate reflood models in system codes.

% Reason 3
	\item Multiple data of various types (pressure, temperature, etc.), taken with different experimental conditions (flow rate, system pressure, etc.), are typically available from experiment within the same facility.
	As calibration in the present research is conducted using one experimental condition, it is important to validate the calibration results against the data with different experimental conditions albeit from the same experimental facility. 
	Moreover, additional data from other reflooding \glspl[hyper=false]{setf} are available.
	This is important for validating the proposed method further and expanding it to calibration against data from multiple facilities.
	
\end{itemize}

As such, while it is important to acknowledge that reflood simulation and the associated relevant model (or models) are only parts of a large and complex \gls[hyper=false]{th} system code,
they can provide a representative and relevant illustration on the particulars of analyzing and calibrating the code using experimental data from \gls[hyper=false]{setf} in general; providing a suitable testing ground for the proposed methods.

% Statistical nature
The methods and practices of sensitivity analysis, approximation, and calibration of computer model need not be statistical.
\marginpar{Statistical framework}
This thesis, however, focuses on the statistical approach for each of the aforementioned steps.
The main reasoning for this choice are twofold:
First, statistical methods tend to require fewer assumptions regarding the model complexity.
While they may be more computationally expensive than their non-statistical counterparts,
they are also easier to set up, with minimal intrusion to the code itself, and subject to less severe dependence on the number of input parameters.
Secondly, the ultimate results of the model parameters calibration (i.e., their quantified uncertainties) should be represented in terms of probability.
As mentioned previously, this is to conform with the widely accepted practice of statistical uncertainty propagation in the nuclear engineering community.  

% Closing
As a last note, the \glsfirst[hyper=false]{th} system code considered in this thesis is the \glsfirst[hyper=false]{trace} code developed by the \glsfirst[hyper=false]{usnrc}.
\marginpar{TRACE code}
The main reason to consider solely this particular code is the fact that \gls[hyper=false]{trace} is the thermal-hydraulics system code used for the safety analysis of the Swiss \glspl[hyper=false]{npp} conducted within the \glsfirst[hyper=false]{stars} program \cite{PSI2017} at the \glsfirst[hyper=false]{psi}.