%********************************************************************************************************************************************************************
\section[Statistical Framework]{Statistical Framework for Computer Model Sensitivity Analysis, Approximation, and Calibration}\label{sec:intro_statistical_framework}
%********************************************************************************************************************************************************************

% Introductory paragraph
The set of strategies for sensitivity analysis, model approximation, and calibration presented above constitutes a consolidated statistical framework that will be used in this thesis for quantifying the uncertainty in model parameters of a \gls[hyper=false]{th} system code.
This section presents a broad, and by no means exhaustive, literature review of the strategies used in this thesis.
For each strategy, the review first reiterates its main motivation followed by the generic classification and the steps involved before briefly summarizing its applications in nuclear engineering \gls[hyper=false]{th}, both in the past and more recent times.
As will be outlined in Section~\ref{sec:intro_thesis_structure}, three main chapters of the thesis will be dedicated for each of the proposed strategies detailing the selected methods further and presenting their applications on a \gls[hyper=false]{trace} model.

%-------------------------------------------------------------------------------
\subsection[Sensitivity Analysis]{Sensitivity Analysis (SA)}\label{sub:intro_sa}
%-------------------------------------------------------------------------------

% Introductory Paragraph
An essential part of model development and assessment is to properly describe and understand the impact of model input parameter variations on the model predictions.
\Gls[hyper=false]{sa} is an important methodological step in that context \cite{Trucano2006}.
\gls[hyper=false]{sa} is the process of investigating the role of input parameters in determining the model output \cite{Iooss2015} variation and it seeks to quantify the importance of each model input parameter on the output.

% Classifications
Various classifications exist in the literature to categorize \gls[hyper=false]{sa} techniques \cite{Frey2002, Ionescu-Bujor2004, Cacuci2004, Saltelli2008, Iooss2015}.
\marginpar{Classifications}
In the review by Ionescu-Bujor and Cacuci \cite{Ionescu-Bujor2004, Cacuci2004}, 
\gls[hyper=false]{sa} techniques are classified with respect to their scope (local vs. global) and to their framework (deterministic vs. statistical).
In the review of \gls[hyper=false]{sa} methods by Iooss and Lemaître \cite{Iooss2015}, 
and the works by Saltelli et al. \cite{Saltelli2008} and by Santner et al. \cite{Santner2003}, 
the statistical framework is implicitly assumed, 
and the classification is based on the parameter space of interest (local vs. global).

% Local sensitivity analysis
Local analysis is based on calculating the effect on the model output of small perturbations around nominal parameter values. 
\marginpar{Local sensitivity analysis}
Often, the perturbation is done one parameter at a time thus approximating the first-order partial derivative of the model output with respect to the perturbed parameter. 
The derivative can be computed through efficient adjoint formulation \cite{Cacuci2003,Cacuci2010} capable of handling numerous input parameters.

% Local sensitivity analysis, advantages
Besides being numerically efficient, sensitivity coefficients obtained from local deterministic sensitivity analysis have the advantage of being intuitive in their interpretation, irrespective of the method employed \cite{Razavi2015}.
The intuitiveness stems from the equivalence to the derivative of the output with respect to each parameter \cite{Ionescu-Bujor2004} around a specifically defined point (i.e., nominal parameter values). 
Thus the coefficients can be readily compared over different modeled systems, independently of the range of parameters variations.

% Global sensitivity analysis
Global analysis, on the other hand, 
\marginpar{Global sensitivity analysis}
seeks to explore the input parameter space across its range of variation and then quantify the input parameter importance based on a characterization of the resulting output response (hyper-)surface. 
In the global deterministic framework \cite{Ionescu-Bujor2004,Cacuci2010}, the characterization is aimed at the identification of the critical points of the system (e.g., maxima, minima, saddle points, etc.). 
In statistical global methods \cite{Saltelli2008, Saltelli2004, Saltelli2006}, the characterization is aimed at measuring the dispersion of the output based on variance \cite{Sobol2001,Cukier1978}, correlation \cite{Helton1993}, or elementary effects \cite{Morris1991}.

% Global sensitivity analysis, difficulty
Due to the different characterizations, the global statistical framework can potentially give spurious results not comparable to the results from the local method as there is no unique definition of sensitivity coefficient provided by different global methods \cite{Razavi2015}. 
In some cases, different methods can give different and inconsistent parameter importance ranking \cite{Saltelli2008,Saltelli2004}.
Furthermore, the result of the analysis can be highly dependent to the assumed input parameters probability distributions or their range of variations \cite{Cacuci2004,Cacuci2010}.

% Global sensitivity analysis, advantages
Yet, despite the aforementioned shortcomings, 
\marginpar{Global statistical sensitivity analysis}
the global statistical framework has three particular attractive features relevant to the present study. 
First, the statistical method for sensitivity analysis is non-intrusive in the sense that minimal or no modification to the original code is required. 
In other words, the code can be taken as a black box and the analysis is focused on the input/output relationship \cite{Saltelli2008}. 
This is the case especially in comparison to adjoint-based sensitivity \cite{Cacuci2000,Ionescu-Bujor2000}, which is a highly efficient and accurate method applicable to a large number of parameters, 
provided that the code is designed/modified for adjoint analysis.

Second, no a priori knowledge on the model structure (linearity, additivity, etc.) is required. 
This is essential in many cases because depending on the model complexity and for large parameter variations, the linearity or additivity assumption might not hold.

Last, 
the choice of a statistical framework for sensitivity analysis fits the Monte Carlo (MC)-based uncertainty propagation method widely adopted in nuclear reactor evaluation models \cite{Boyack1990, Nutt2004, Wallis2007, Glaeser2008}. 
The method prescribes that the uncertain model input parameters (modeled as random variables) 
should be simultaneously and randomly perturbed across their range of variations. 
Multiple randomly generated input values are then propagated through the code to quantify the dispersion of the prediction (e.g., peak cladding temperature) 
which serves as a measure of the prediction reliability. 
Statistical global sensitivity analysis thus complements the propagation step 
by addressing the follow-up question on the identification of the most important parameters in driving the prediction uncertainty. 

% Quantity of Interests
Saltelli et al. \cite{Saltelli2006} emphasized that an analysis using computer simulation 
\marginpar{Choosing model output as the quantity of interest}
should be focused on the specific question the simulation is required to answer 
as opposed to the analysis of each and every individual model output. 
This is done through judicious choice of representative quantities of interest (QoIs) 
that properly substantiate the problem at hand. 
In particular, computer code output often comes in a form of time series. 
In such case, Saltelli et al. \cite{Saltelli2008,Saltelli2004} proposed to derive the relevant QoI from time-dependent output 
using a predefined scalar function such as the maximum, the minimum, the average, etc. that fits the initial question.

% Function as Model Output
However, in some cases, the whole course of a transient is of primary interest 
\marginpar{Function as model output}
such as in assessing the ability of a model to reproduce the overall dynamics of the simulated system. 
If the attention is focused on the overall change in shape of the time-dependent output (a shift in the Y-axis, a delay, a distortion, etc.), 
the descriptions provided by the aforementioned scalar functions might be incomplete and overlook important features of the variation. 
To tackle this problem, Campbell et al. \cite{Campbell2006} proposed to represent the functional (time-dependent) output in a certain basis function expansion 
and to carry out the sensitivity analysis on the coefficients of the expansion. 
In accordance to such approach, \glsfirst[hyper=false]{fda} popularized by Ramsay and Silverman \cite{Ramsay2005} is useful to reduce the high dimensionality of time-dependent output.

% Recent Development in Nuclear Engineering Application
Despite these recent developments, 
\marginpar{More recent development in nuclear engineering application}
there are very few publications on the application of global sensitivity analysis to nuclear thermal-hydraulics evaluation models specifically dealing with time-dependent output.
Notable recent examples related to sensitivity analysis for a time-dependent \gls[hyper=false]{th} problem were the work done by Ionescu-Bujor et al. \cite{Ionescu-Bujor2005} 
for reflooding experiment of degraded fuel rods, utilizing adjoint sensitivity method; 
by Auder et al. \cite{Auder2012} for pressurized thermal shock analysis, 
utilizing statistical methods with emphasis on metamodeling; 
and by Prošek and Leskovar \cite{Prosek2015} for \gls[hyper=false]{lbloca} analysis, 
utilizing \gls[hyper=false]{fftbm} and local sensitivity analysis.

%------------------------------------------------------------------------------
\subsection{Statistical Metamodeling}\label{sub:intro_statistical_metamodeling}
%------------------------------------------------------------------------------

% Introductory paragraph
Many tasks involving computer simulations can be boiled down to making predictions.
A \emph{computer experiment}, an experiment using computer simulations, evaluates the output based on different inputs to achieve various objectives.
\marginpar{Computer experiment}
In the aforementioned sensitivity analysis, the objective is to identify the influential inputs that drives the variation in the outputs of the computer simulator.
In the forward uncertainty quantification, \gls[hyper=false]{mc} simulation are used to propagate the uncertainty of the inputs to quantify the uncertainty of the simulator prediction using the notion of probability;
while in its inverse counterpart, the goal is to identify a region of the input parameter space that is consistent with both the observed data and the assumed prior uncertainty of the inputs.
The latter objective, in turn, is related to \emph{optimization} where the goal is to identify particular value of inputs that maximize a certain objective function as computed by the simulator.

% The use of metamodel
The objectives above are arguably distinct, but they share a common characteristic of involving analyses of outputs from numerous simulator runs.
\marginpar{Complex simulator, expensive simulator}
An increasingly more realistic and complex computer simulator, however, often translates into a long running simulation and may have to be evaluated a large number of times due to the complexity of the relationship between high-dimensional inputs and high-dimensional outputs (e.g., non-linearities, interactions).
The high computational cost hinders the analysis and the effort to achieve the aforementioned objectives of computer experiment.

% Why Metamodel
As a result, having a fast approximating model of a complex simulator is beneficial in conducting a computer experiment and its value was acknowledged by Sacks et al. in their seminal paper \cite{Sacks1989} and formalized further in several textbooks \cite{Santner2003,Fang2006}.
\marginpar{An approximating model}
The approximating model, while simpler and much faster to evaluate than the \emph{original} simulator, is designed to capture the dominant features of the inputs/outputs relationship of the original complex simulator \cite{Asher2015}.
Capturing the dominant features allows the approximating model to be used in lieu of the original simulator in the experiment.
This approximating model in the literature is referred to as \emph{metamodel}, \emph{surrogate model}, \emph{response surface model}, \emph{proxy model}, or \emph{emulator}.

% Broad classification
Nowadays, any of the terms above are used interchangeably and all are used to substitute the original simulator to reduce the computational cost of conducting computer experiments \cite{Simpson2001,Razavi2012,Asher2015}. 
Subtle differences do exist.
Thus, it is worthwhile to consider a broad classification of surrogate models and the approaches to their derivations (i.e., surrogate modeling) according to the literature.
\marginpar{Classification}
Surrogate models according to their derivations can be broadly classified in two categories: the data-driven \emph{response surface surrogates} and the mechanistic \emph{reduced-order models} \cite{Razavi2012}.

% Reduced order model
The reduced order models are perhaps more familiar in the scientific community where a complex physical model is being simplified by putting more stringent assumptions or reducing the numerical resolution while trying to preserve the most important physical processes present in the more complex model.
\marginpar{Reduced-order model}
The point neutron kinetics model is an example of a reduced-order model, substituting the more complex $3$-dimensional nodal code.
A \gls[hyper=false]{th} system code is also a reduced-order model of the more expensive multi-phase computational fluid dynamics code.
When applicable, reduced-order models can be useful as first approximations and didactic tools to build intuition. 

% Response Surface Surrogate
The response surface surrogates, on the other hand, make no pretense of preserving the underlying physical process modeled in a complex simulator.
\marginpar{Response surface surrogates}
It seeks to \emph{emulate} the relationship (i.e., \emph{mapping}) between inputs and outputs of the simulator.
The term \emph{metamodel} is used throughout the thesis and exclusively refer to this particular type of surrogate model.
The workflow of constructing a response surface surrogate consists of three steps.
The first step is to gather the data, that is by running the simulator at limited and selected points across the input parameter space of interest and evaluate its outputs.
The selection of such points are known as the \emph{design of experiment} \cite{Simpson2001a,Santner2003}.
The second step is to choose an approximating function that emulates well the relationship between the inputs and outputs and \emph{train} this function based on the data.
Training a surrogate model involves \emph{fitting} the parameters associated with the selected approximating function.
The function is chosen such that it is simpler and faster to evaluate at arbitrary inputs, relative to the original simulator.
Finally, a validation step is conducted to assess the quality of the resulting metamodel.
These are a typical workflow of constructing a metamodel, though variation exists \cite{Razavi2012}.

% Gaussian Process, What, Renewed interest due to Machine learning
The surrogate model introduced in the papers of Sacks et al. \cite{Sacks1989,Sacks1989a} were \gls[hyper=false]{gp} metamodel.
\marginpar{Gaussian process metamodel}
The metamodel was constructed as a tool to interpolate between observed data, that is, between the inputs and outputs of actual simulator runs.
Once constructed the output at any arbitrary input point can be predicted faster using the metamodel.
This idea was borrowed from a spatial interpolation tool in geostatistics (where the inputs were spatial coordinates) developed by Krige dating back to the 1950s \cite{Krige1951} and formalized by Matheron in the 1960s \cite{Matheron1963}.
\gls[hyper=false]{gp} metamodel is arguably the most popular approach to metamodeling and it enjoys a renewed interest due to its application for machine learning \cite{Rasmussen2006}.

% Gaussian Process, Why
A \gls[hyper=false]{gp} metamodel is a statistical metamodel.
It is based on the extension of multivariate Gaussian distribution to a continuous multidimensional input parameter space.
Under the Bayesian interpretation, the metamodel assumes a prior probability distribution over functions (i.e., a probability distribution of which each realization is a function) to initially describe an unknown complex function that underlies the simulator.
The observed data (i.e., design of experiment plus the corresponding outputs) is then used to update the prior and learn more about the true underlying function. 
Though the simulator itself might be deterministic, the limited size of the observed data renders prediction at arbitrary non-observed input \emph{uncertain}.
This measure of uncertainty makes a \gls[hyper=false]{gp} metamodel an attractive choice to be incorporated into a model calibration framework where multiple sources of uncertainty are considered.
This research adopts \gls[hyper=false]{gp} for constructing a metamodel of a \gls[hyper=false]{th} system code model as detailed in Chapter~\ref{ch:gp_metamodel}.

% Gaussian Process, and others
\gls[hyper=false]{gp} metamodel is by no means the only method to construct a data-driven metamodel, though it can be considered as the most popular choice in the literature (Table~\ref{tab:metamodel_in_literature}).
\marginpar{Other metamodeling approaches}
\emph{Response Surface Method} (RSM), originally developed as a technique in the design and analysis of physical experiments \cite{Box2007}, 
has a long history of being adapted to the design and analysis of computer experiments \cite{Kerrigan1979,Lucia1982,Faravelli1989,Engel1990}.
It is mostly based on either linear or quadratic regression (with interaction terms) (see for instance \cite{Kleijnen2000}, and more recent reviews \cite{Simpson1998,Simpson2001}).
In recent times, other methods such as the ones based on artificial neural network \cite{Fonseca2003} and polynomial chaos expansion (PCE) \cite{Sudret2008,Sudret2012} have also gained traction.
For comparison, Table~\ref{tab:metamodel_in_literature} shows the search hits from Scopus, an online bibliographic database \cite{Elsevier2017}, for the different selected metamodeling approaches.
Note that the list is not at all exhaustive.
\begin{table}[ht]
    \myfloatalign
    \caption{Number of publications related to different metamodeling approaches based on Scopus web search as of Feb. $14$. $2017$.}
    \label{tab:metamodel_in_literature}
    \begin{tabularx}{\textwidth}{cXcc} \toprule
        \tableheadline{\scriptsize{Metamodeling}}	& \tableheadline{\scriptsize{Search}}\parnote{\scriptsize{\texttt{(...) AND ("surrogate" OR "metamodel")}}}  & \tableheadline{\scriptsize{Number of}}  & \tableheadline{\scriptsize{Since}} \\ 
				\tableheadline{\scriptsize{Approach}}     & \tableheadline{\scriptsize{Keyword}}  & \tableheadline{\scriptsize{Publications}}  &  \\ \midrule
        \multicolumn{1}{l}{\scriptsize{Gaussian Process / Kriging}} 		& \scriptsize{(\texttt{"Gaussian Process OR kriging"})}		& \scriptsize{$1838$} & \scriptsize{$1992$} \\
        \multicolumn{1}{l}{\scriptsize{Artificial Neural Network}}      & \scriptsize{\texttt{"neural network"}} 	 & \scriptsize{$997$} & \scriptsize{$1993$} \\
        \multicolumn{1}{l}{\scriptsize{Response Surface Method}}			  & \scriptsize{\texttt{"response surface"}} & \scriptsize{$947$} & \scriptsize{$1977$} \\
        \multicolumn{1}{l}{\scriptsize{Polynomial Chaos Expansion}}     & \scriptsize{\texttt{"polynomial chaos"}} & \scriptsize{$208$} & \scriptsize{$2004$} \\ \bottomrule
    \end{tabularx}
		\parnotes
\end{table}

% Development in nuclear engineering application
Metamodel applications have a long history in nuclear engineering analyses due to the complexity of the simulators and the long-understood importance of quantifying the uncertainty of the predictions.
\marginpar{Developments in nuclear engineering application}
As such, historically, metamodels (specifically, the response surface method) have been applied for quantifying the prediction uncertainty forward through \gls[hyper=false]{mc} sampling as well as for statistical sensitivity analysis \cite{Cox1977,Ishigami1989}.
The range of applications varied from quantifying the reactor safety margin \cite{Lellouche1990} for a \gls[hyper=false]{lbloca} scenario, propagating the uncertainty of fuel rods failure in the core \cite{Meyder1986} during the same scenario, to the uncertainty and sensitivity analyses of severe accident progressions \cite{Khatib-Rahbar1989}.
In recent times, more advanced metamodels (e.g., \gls[hyper=false]{gp} metamodel) have been applied to more diverse engineering analysis;
from the design optimization problem of fuel assembly \cite{Raza2008} and spacer grid \cite{Kim2005} to the calibration of physical models in \gls[hyper=false]{th} system code \cite{Wu2017} and fuel performance \cite{Higdon2013} code.

%----------------------------------------------------------------------
\subsection{Bayesian Calibration}\label{sub:intro_bayesian_calibration}
%----------------------------------------------------------------------

% Introductory paragraph
The objective of model calibration is to increase the agreement between simulation predictions and measurement data by adjusting some of the simulator inputs \cite{Campbell2006a,Trucano2006}.
\marginpar{Model calibration, goal and approach}
Traditionally, calibration is closely related to an optimization problem of an objective function measuring the error between simulation predictions and measurement data (e.g., root-mean-square-error).
However, statistical approach to calibration using a Bayesian framework has become a popular practice in the scientific simulation community.
Instead of minimizing a measure of error, Bayesian framework treats the uncertainty of the inputs probabilistically and update their prior probability distributions based on the available, albeit uncertain, measurement data.
The framework offers flexibility in modeling various sources of uncertainty \cite{Kennedy2001,Kaipio2011}.

% Bayesian Calibration
Bayesian framework for the calibration of computer simulation mo\-del was popularized by the work of Kennedy and O'Hagan \cite{Kennedy2001}.
\marginpar{Bayesian framework, Kennedy and O'Hagan approach}
The main goal of the framework is similar to any calibration framework, that is to learn the proper values of model parameters and their uncertainties by taking into account different sources of uncertainty based on the observed data.
The distinct idea is to acknowledge that a systematic bias between a physics-based simulator and reality might exist and is often not known a priori.
If the bias is not modeled properly, the calibration process might overfit the model parameters.
That is, the calibrated model parameters will be overly sensitive to the calibration data and thus not applicable for prediction.
As such, Kennedy and O'Hagan proposed to model the unknown bias term probabilistically by putting a prior probability distribution on the bias term to be updated simultaneously with that of the model parameters, by using the observed data.
The proposed prior distribution is a \gls[hyper=false]{gp}.
Due to its popularity, the term \emph{KOH approach} became synonymous to this particular approach of computer model calibration \cite{Trucano2006,Hall2011,Ling2014}.
It is adapted here to deal with the particular problem posed in the \gls[hyper=false]{premium} benchmark that itself represents a typical problem in nuclear engineering \gls[hyper=false]{th} analysis, as detailed in Chapter~\ref{ch:bayesian_calibration}.

% Bayesian data analysis
Bayesian framework for model calibration consists of two main aspects \cite{Gelman2013}: a formulation of a posterior distribution for the model parameters of interest and the computation involving the posterior distribution.
\marginpar{Steps in Bayesian framework}
Regarding the first aspect, the KOH approach for model calibration, in essence, prescribes a probabilistic model for the data-generating process of the experimental data, incorporating the prediction by the simulator, the uncertain model bias term, and the uncertain model parameters into it.
\marginpar{Bayesian framework, formulation}
The formulation will eventually results in a posterior probability distribution of the model parameters as presented in Section~\ref{sub:intro_uq_inverse}.
Extension and modification to the KOH approach includes dealing with high-dimensional output \cite{Higdon2008,Bayarri2007a,Wilkinson2010}, multiple types of output \cite{Arendt2012a}, different choices of the model bias term \cite{Ling2014}, and various simplifications \cite{Arendt2012,Bayarri2007,Goldstein2012}.

% Regarding prior
Regarding the choice for the prior distribution, there is a tendency in the modern literature \cite{Gelman2013,McElreath2015} to move away from the notion of \emph{Bayesianism}, which emphasizes specifying the correct prior (if not the \emph{true} prior altogether) so as to guarantee that the resulting posterior will always be true relative to that prior.
\marginpar{On prior distribution}
In a more modern practice of Bayesian statistics, the use of prior is seen in a more pragmatic light, i.e., as a starting assumption that can be changed if not appropriate \cite{Gelman2002,Gelman2006}.
Gelman et al.~\cite{Gelman2013} advocates to check if the resulting parameters posterior distributions and the prediction using them make sense and are useful, rather than to check whether the posterior parameter distribution is \emph{true}.

% Bayesian Computation 1, Analytical and Early Sampler
The second aspect of the Bayesian framework is related to the computations involving the posterior distribution of the model parameters.
\marginpar{Bayesian framework, computation}
The formulated posterior distribution in practical problems is multidimensional.
Numerically integrating the posterior distribution to summarize a given \gls[hyper=false]{qoi} (see Section~\ref{sub:intro_uq_forward}) might not be efficient.
Traditionally, maximum a posteriori estimates\footnote{or maximum likelihood estimates, if the prior is non-informative} is often used to describe each of the posterior model parameter values using a single number.
\marginpar{Semi analytical approach}
It simply requires the maximization of the posterior density function (i.e., its mode).
An extension of this, giving description of the shape of the posterior, is done by the so-called \emph{Laplace's approximation} or the \emph{normal approximation} \cite{Mackay2005,Sivia2012,Gelman2013,McElreath2015}.
In this approximation, the distribution of the posterior is approximated as a normal distribution. 
The mean corresponds to the maximum of the posterior density function, while the variance of the distribution is approximated as the function of the second derivative of the posterior around the mean (i.e., its curvature).
Multidimensionality of the posterior distribution is taken into account by using the Hessian matrix.
The approximation works well if the posterior distribution is approximately normal (i.e., unimodal, bell-shaped, and linearly correlated).

% Bayesian Computation 2, Early Samplers
To deal with more generic formulations of the posterior distribution, modern approach to Bayesian computation involves random simulation to directly generate samples from the posterior.
\marginpar{MCMC, classical samplers}
This can be seen as an extension of the simulation method for estimating integral quantities (the \glsfirst[hyper=false]{mc} simulation), for the case of a complicated sampling distribution not easily sampled from.
The idea of \gls[hyper=false]{mc} simulation dates back to the $1940$ for solving the problems in neutron transport \cite{Metropolis1949} and statistical mechanics \cite{Metropolis1953}.
In the latter, generation of a \emph{Markov chain} by \gls[hyper=false]{mc} simulation paved the way to a generic simulation method applicable to generate samples from any kind of probability distribution; thus the origin of \gls[hyper=false]{mcmc} simulation.
Later on, its usage for data analysis in general and Bayesian data analysis in particular were revived in the $1970$s and the early $1980$s by the generalization of the simulation algorithm by Hastings \cite{Hastings1970} (i.e., the \gls[hyper=false]{mh} algorithm) and by the invention of a computationally efficient sampler and its application for image reconstruction by Geman and Geman \cite{Geman1984} (i.e., the Gibbs sampler).

% Bayesian Computation 3, Advanced Samplers
Nowadays, \gls[hyper=false]{mcmc} sampler is the backbone of Bayesian computation \cite{Allison2013,Geyer2011,Gelman2013}.
\marginpar{MCMC, modern samplers}
Loosely speaking, its improvement and modern implementations can be broadly classified into three different families: adaptive \gls[hyper=false]{mh} samplers (e.g., \cite{Andrieu2008}), Hamiltonian \gls[hyper=false]{mc} samplers \cite{Neal2011}, and ensemble samplers \cite{Goodman2010}.
Adaptive \gls[hyper=false]{mh} sampler deals with the adaptation of the algorithm to achieve faster convergence.
Hamiltonian \gls[hyper=false]{mc} sampler simulates the movement of a particle in the parameter space as described by the posterior distribution according to the Hamiltonian dynamic.
Finally, ensemble sampler uses multiple particles that move together in the input parameter space each of them moving according to the position of the others.
Ensemble sampler has a particularly simple implementation, is easily parallelized, and requires minimal tuning \cite{Foreman-Mackey2013}.
Chapter~\ref{ch:bayesian_calibration} describes in more detail the basics of \gls[hyper=false]{mh} and ensemble samplers.

% Development in nuclear engineering application
Unlike the forward uncertainty propagation and statistical sensitivity analysis, the use of Bayesian model calibration is relatively new in nuclear engineering applications \cite{Unal2011}.
\marginpar{Developments in nuclear engineering application}
A notable early example was the previously mentioned CIRC\'E \cite{Crecy2001} for the calibration of model parameters in a \gls[hyper=false]{th} system code.
A recent demonstration on the applicability of the method can be found in Ref.~\cite{Freixa2016}.
More recent examples are in-line to the KOH approach but provide extensions:
for dealing with high-dimensional output of the same type (i.e., time- and space-dependent) with \cite{Wicaksono2016} or without \cite{Higdon2013,Wu2017} explicit treatment of the model bias, 
and with \cite{Li2017} or without \cite{Heo2018} the use of metamodels.
%CIRC\'E assumes a linear relationship between model parameters and the outputs of interest and the difference between predicted value and experimental data is described by first order approximation.
%It is further assumed that the parameters are normally distributed, before and after calibration.
%Under these assumptions, a probabilistic model for the difference between predicted value and experimental data can be formulated.
%Finally, the maximum likelihood estimates for the mean and covariance parameters of the model parameters are calculated from the resulting model.
