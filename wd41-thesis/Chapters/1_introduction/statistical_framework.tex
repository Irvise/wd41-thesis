%********************************************************************************************************************************************************************
\section[Statistical Framework]{Statistical Framework for Computer Model Sensitivity Analysis, Approximation, and Calibration}\label{sec:intro_statistical_framework}
%********************************************************************************************************************************************************************

% Introductory paragraph
The set of strategies above, for sensitivity analysis, for model approximation, and for calibration constitutes a consolidated statistical framework for quantifying the uncertainty in model parameters of a \gls[hyper=false]{th} system code proposed in this thesis.
In the following a broad, and by no means exhaustive, overview on the research landscape from the literature on each of the elements of the proposed strategy.
 
%--------------------------------------------------------------------------
\subsection[Sensitivity Analysis]{Sensitivity Analysis}\label{sub:intro_sa}
%--------------------------------------------------------------------------

% Introductory Paragraph
An essential part of model development and assessment is properly describing and understanding the impact of model parameter variations on the model prediction.
\Gls[hyper=false]{sa} is an important methodological step in that context \cite{Trucano2006}.
\gls[hyper=false]{sa} is the process of investigating the role of input parameters in determining the model output \cite{Iooss2015}. 
It seeks to quantify the importance of each model input parameter on the output.

% Classifications
Various classifications exist in the literature to categorize \gls[hyper=false]{sa} techniques \cite{Frey2002, Ionescu-Bujor2004, Cacuci2004, Saltelli2008, Iooss2015}.
\marginpar{Classifications}
In the review by Ionescu-Bujor and Cacuci \cite{Ionescu-Bujor2004, Cacuci2004}, 
\gls[hyper=false]{sa} techniques are classified with respect to their scope (local vs. global) and to their framework (deterministic vs. statistical).
In the review of \gls[hyper=false]{sa} methods by Iooss and Lemaître \cite{Iooss2015}, 
and the work by Saltelli et al. \cite{Saltelli2008} and by Santner et al. \cite{Santner2003}, 
the statistical framework is implicitly assumed deriving ideas from design of experiment, 
and the classification is based on the parameter space of interest (local vs. global).

% Local sensitivity analysis
Local analysis is based on calculating the effect on the model output of small perturbations around a nominal parameter value. 
\marginpar{local sensitivity analysis}
Often the perturbation is done one parameter at a time thus approximating the first-order partial derivative of the model output with respect to the perturbed parameter. 
The derivative can be computed through efficient adjoint formulation \cite{Cacuci2003,Cacuci2010} capable of handling large number of parameters.

% Local sensitivity analysis, advantages
Besides being numerically efficient, sensitivity coefficients obtained from local deterministic sensitivity analysis have the advantage of being intuitive in their interpretation, irrespective of the method employed \cite{Razavi2015}.
The intuitiveness stems from the aforementioned equivalence to the derivative of the output with respect to each parameter \cite{Ionescu-Bujor2004} around a specifically defined point (i.e., nominal parameter values). 
Thus the coefficients can be readily compared over different modeled systems, independently of the range of parameters variations.

% Global sensitivity analysis
The global analysis, on the other hand, 
\marginpar{global sensitivity analysis}
seeks to explore the input parameters space across its range of variation and then quantify the input parameter importance based on a characterization of the resulting output response surface. 
In global deterministic framework \cite{Ionescu-Bujor2004,Cacuci2010}, the characterization is aimed at the identification of the system’s critical points (e.g., maxima, minima, saddle points, etc.). 
In statistical global methods \cite{Saltelli2008, Saltelli2004, Saltelli2006}, the characterization is aimed at measuring the dispersion of the output based on variance \cite{Sobol2001,Cukier1978}, correlation \cite{Helton1993}, or elementary effects \cite{Morris1991}.

% Global sensitivity analysis, difficulty
Due to the different characterizations, the global statistical framework can potentially give spurious results not comparable to the results from local method as there is no unique definition of sensitivity coefficient provided by different global methods \cite{Razavi2015}. 
In some cases, different methods can give different and inconsistent parameters importance ranking \cite{Saltelli2008,Saltelli2004}.
Furthermore, the result of the analysis can be highly dependent to the assumed input parameters probability distribution and/or their range of variation \cite{Cacuci2004,Cacuci2010}.

% Global sensitivity analysis, advantages
Yet, despite the aforementioned shortcomings, 
\marginpar{global statistical sensitivity analysis}
the global statistical framework has three particular attractive features relevant to the present study. 
First, the statistical method for sensitivity analysis is non-intrusive in the sense that minimal or no modification to the original code is required. 
In other words, the code can be taken as a black box and the analysis is focused on the input/output relationship \cite{Saltelli2008} of the code. 
This is the case especially in comparison to adjoint-based sensitivity \cite{Cacuci2000,Ionescu-Bujor2000} which is a highly efficient and accurate method applicable to a large number of parameters, 
provided that the code is designed/modified for adjoint analysis.

Second, no a priori knowledge on the model structure (linearity, additivity, etc.) is required. 
Depending on the model complexity and as the parameter variation range can be large, 
the linearity or additivity assumption might not hold.

Third and finally, 
the choice of a statistical framework for sensitivity analysis fits the Monte Carlo (MC)-based uncertainty propagation method widely adopted in nuclear reactor evaluation models \cite{Boyack1990, Nutt2004, Wallis2007, Glaeser2008}. 
The method prescribes that the uncertain model input and parameters (modeled as random variables) 
should be simultaneously and randomly perturbed across their range of variations. 
Multiple randomly generated input values are then propagated through the code to quantify the dispersion of the prediction (e.g., peak cladding temperature) 
which serves as a measure of the prediction reliability. 
Statistical global sensitivity analysis thus complements the propagation step 
by addressing the follow-up question on the identification of the most important parameters in driving the prediction uncertainty. 

% Quantity of Interests
Saltelli et al. \cite{Saltelli2006} emphasized that an analysis using computer simulation 
\marginpar{choosing model output as the quantity of interest}
should be focused on the specific question the simulation is required to answer 
as opposed to the analysis of each and every individual model output. 
This is done through judicious choice of representative quantity of interest (QoI) 
that properly substantiates the problem at hand. 
In particular, computer code output often comes in a form of time series. 
In such case, Saltelli et al. \cite{Saltelli2008,Saltelli2004} proposed to derive the relevant QoI from time-dependent output 
using a predefined scalar function such as the maximum, the minimum, the average, etc. that fits the initial question.

% Function as Model Output
However, in some cases, the whole course of a transient is of primary interest 
\marginpar{function as model output}
such as in assessing the ability of a model to reproduce the overall dynamics of the simulated system. 
If the attention is focused on the overall change in shape of the time-dependent output (a shift in the Y-axis, a delay, a distortion, etc.), 
the descriptions provided by the aforementioned scalar functions might be incomplete and overlook important features of the variation. 
To tackle this problem, Campbell et al. \cite{Campbell2006} proposed to represent the functional (time-dependent) output in a certain basis function expansion 
and to carry out the sensitivity analysis on the coefficients of the expansion. 
In accordance to such approach, \glsfirst[hyper=false]{fda} popularized by Ramsay and Silverman \cite{Ramsay2005} can be useful to reduce the high dimensionality of time-dependent output.

% Recent Development in Nuclear Engineering Application
Despite these recent developments, 
\marginpar{more recent development in nuclear engineering application}
there are very few publications on the application of global sensitivity analysis to nuclear thermal-hydraulics evaluation models specifically dealing with time-dependent output.
Notable recent examples related of sensitivity analysis for a time-dependent \gls[hyper=false]{th} problem were the work done by Ionescu-Bujor et al. \cite{Ionescu-Bujor2005} 
for reflooding experiment of degraded fuel rods, utilizing adjoint sensitivity method; 
by Auder et al. \cite{Auder2012} for pressurized thermal shock analysis, 
utilizing statistical method with emphasis on metamodeling; 
and by Prošek and Leskovar \cite{Prosek2015} for \gls[hyper=false]{lbloca} analysis, 
utilizing \gls[hyper=false]{fftbm} and local sensitivity analysis.

%------------------------------------------------------------------------------
\subsection{Statistical Metamodeling}\label{sub:intro_statistical_metamodeling}
%------------------------------------------------------------------------------

% Introductory paragraph
Many tasks involving computer simulation can be boiled down to making prediction.
\emph{Computer experiment}, an experiment using computer simulation evaluates the output based on different inputs to achieve various objectives.
\marginpar{Computer experiment}
In the aforementioned sensitivity analysis, the objective is to identify influential inputs that drives the variation in the outputs of the computer simulator.
In the forward uncertainty quantification, \gls[hyper=false]{mc} simulation are used to propagate the uncertainty in the inputs to quantify the uncertainty in the simulator prediction using the notion of probability; while in its inverse counterpart, the goal is to identify a region of input parameter space that is consistent with both the observed data and the assumed prior uncertainty of the parameters.
The latter objective, in turn, is related to \emph{optimization} where the goal is to identify particular value of inputs that maximize a certain objective function as computed by the simulator.

% The use of metamodel
The examples of the objectives above are arguably distinct, but they share a common characteristic of involving analyses of outputs from numerous simulator runs.
\marginpar{Complex simulator, expensive simulator}
An increasingly more realistic and complex computer simulator, however, often translates to a long running simulation (i.e., computationally expensive) or requiring vast amount of evaluations due to the complexity of relationship between high-dimensional inputs and high-dimensional outputs (e.g., non-linearities, interaction), or both.
They, in turn, hinder the analyses and the effort to achieve the aforementioned objectives of computer experiment.

% Why Metamodel
As a result, having a fast approximating model of a complex simulator is beneficial in conducting a computer experiment and its value was acknowledged by Sacks et al. in their seminal paper \cite{Sacks1989} and formalized further in several textbooks \cite{Santner2003,Fang2006}.
\marginpar{An approximating model}
The approximating model, while simpler and much faster to evaluate than the \emph{original} simulator, is designed to capture the dominant features of the inputs/outputs relationship of the original complex simulator \cite{Asher2015}.
Capturing the dominant features allows the approximating model to be used in lieu the original simulator in the experiment.
This approximating model in the literature is referred to as \emph{metamodel}, \emph{surrogate model}, \emph{response surface model}, \emph{proxy model}, or \emph{emulator}.

% Broad classification
Nowadays, any of the terms above are used interchangeably and all are used to substitute the original simulator to reduce the computational cost of conducting computer experiment \cite{Simpson2001,Razavi2012,Asher2015}. 
Subtle difference does exist.
Thus, it is worthwhile to consider a broad classification of surrogate models and the approaches to their derivation (i.e., surrogate modeling) according to the literature.
\marginpar{Classification}
Surrogate models according to their derivation can be broadly classified in two categories: the data-driven \emph{response surface surrogates} and the mechanistic \emph{reduced-order models} \cite{Razavi2012}.

% Reduced order model
The reduced order models are perhaps more familiar in the scientific community where a complex physical model is being simplified by putting more stringent assumption or reducing the numerical resolution while trying to preserve the most important physical processes present in the more complex model.
\marginpar{Reduced-order model}
The point neutron kinetics model is an example of a reduced-order model, substituting the more complex $3$-dimensional nodal code;
and loosely speaking, \gls[hyper=false]{th} system code is a reduced-order model of the more expensive multi-phase computational fluid dynamics code.
When applicable, reduced-order models can be useful as first approximation as well as didactic tools to build intuition. 

% Response Surface Surrogate
The response surface surrogates, on the other hand, make no pretext of preserving the underlying physical process of a complex simulator.
\marginpar{Response surface surrogates}
It seeks to \emph{emulate} the relationship (i.e., mapping) between inputs and outputs of the simulator.
The term \emph{metamodel} is used throughout the thesis and exclusively refer to this particular type of surrogate model.
The workflow of constructing a response surface surrogate typically consists of three steps.
The first step is to gather the data, that is by running the simulator at limited and selected points across the input parameter space of interest and evaluate its outputs.
The selection of such points are known as the \emph{design of experiment} \cite{Simpson2001a,Santner2003}.
The second step is to choose an approximating function that emulates well the relationship between the inputs and outputs and \emph{train} them based on the data.
Training a surrogate model involves \emph{fitting} the parameters associated with the selected approximating function.
The function is chosen such that it is simpler and faster to evaluate at arbitrary inputs, relative to the original simulator.
Finally, a validation step is conducted to assess the quality of the resulting metamodel.
Variation of this workflow exists, especially whether iteration is carried out \cite{Razavi2012}.

% Gaussian Process, What, renewed interest due to Machine learning
Gaussian process The original paper was 

% Gaussian Process, Why

% Gaussian Process, and others
\gls[hyper=false]{gp} metamodel is by no means the only method to construct a data-driven metamodel of a simulator, though it can be considered as the most popular choice in the literature (Table~\ref{tab:metamodel_in_literature}).
\marginpar{Other metamodeling approaches}
\emph{Response Surface Method} (RSM), originally developed as a technique in the design and analysis of physical experiments \cite{Box2007}, 
has a long history of being adapted to the design and analysis of computer experiments \cite{Kerrigan1979,Lucia1982,Faravelli1989,Engel1990}.
It is mostly based on either linear or quadratic regression (with interaction terms) (see for instance cite \cite{Kleijnen2000}, and more recent reviews \cite{Simpson1998,Simpson2001}).
In recent times, other methods such as the ones based on artificial neural network \cite{Fonseca2003} and polynomial chaos expansion \cite{Sudret2008,Sudret2012} have also gained traction.
For comparison, Table~\ref{tab:metamodel_in_literature} shows the search hits from Scopus, an online bibliographic database \cite{Elsevier2017}, for the different select metamodeling approaches.
\begin{table}[ht]
    \myfloatalign
    \caption{Number of publications related to different metamodeling approaches based on Scopus web search as of Feb. $14$. $2017$.}
    \label{tab:metamodel_in_literature}
    \begin{tabularx}{\textwidth}{cXcc} \toprule
        \tableheadline{\footnotesize{Metamodeling}}	& \tableheadline{\footnotesize{Search}}\parnote{\texttt{(...) AND ("surrogate" OR "metamodel")}}  & \tableheadline{\footnotesize{Number of}}  & \tableheadline{\footnotesize{Since}} \\ 
				\tableheadline{\footnotesize{Approach}}     & \tableheadline{\footnotesize{Keyword}}  & \tableheadline{\footnotesize{Publications}}  &  \\ \midrule
        \multicolumn{1}{l}{\footnotesize{Gaussian Process / Kriging}} 		& \footnotesize{(\texttt{"Gaussian Process OR kriging"})}		& \footnotesize{$1838$} & \footnotesize{$1992$} \\
        \multicolumn{1}{l}{\footnotesize{Artificial Neural Network}}      & \footnotesize{\texttt{"neural network"}} 	 & \footnotesize{$997$} & \footnotesize{$1993$} \\
        \multicolumn{1}{l}{\footnotesize{Response Surface Method}}			  & \footnotesize{\texttt{"response surface"}} & \footnotesize{$947$} & \footnotesize{$1977$} \\
        \multicolumn{1}{l}{\footnotesize{Polynomial Chaos Expansion}}     & \footnotesize{\texttt{"polynomial chaos"}} & \footnotesize{$208$} & \footnotesize{$2004$} \\ \bottomrule
    \end{tabularx}
		\parnotes
\end{table}

% Recent development in nuclear engineering application

\subsection{Bayesian Calibration}
% Complex Simulator
Granted, a simplicistic model cannot be expected to imitate all the important features of a complex physical phenomenon.
And yet, there is a tendency of developing and applying overly complex model, with numerous parameters and multiple non-linear relationships, for computer simulation.
This tendency has invited many critics over the year (citation needed).
In nuclear science and engineering, for instance, Zuber \cite{Zuber2001} and Wullf \cite{Wulff2007} have long critized the development and the use of multi-fluid model for thermal-hydraulics simulation as high in complexity and in maintenance cost, but low in fidelity and its usefulness.

The goal of computer simulation, complex or otherwise, is to provide prediction.
The main characteristic (and source of criticism) of using multi-parameter complex model for simulation is that the the relationship between numerous inputs and outputs becomes increasingly opaque.
The impact of changing one parameter alone, or especially together, on the prediction is hard to disentangle or intuit.
Furthermore, as appropriate values of inputs might not be fully known, they are often given simply over range of interest containing different possible permissible values.
It is thus seldom the case that one single simulation is sufficient to provide a reliable answer according to hardly any objective of computer simulation.
As analytical solution



