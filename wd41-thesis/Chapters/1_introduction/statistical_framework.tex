%********************************************************************************************************************************************************************
\section[Statistical Framework]{Statistical Framework for Computer Model Sensitivity Analysis, Approximation, and Calibration}\label{sec:intro_statistical_framework}
%********************************************************************************************************************************************************************

% Introductory paragraph
The set of strategies above, for sensitivity analysis, for model approximation, and for calibration constitutes a consolidated statistical framework for quantifying the uncertainty in model parameters of a \gls[hyper=false]{th} system code proposed in this thesis.
In the following a broad, and by no means exhaustive, overview on the research landscape from the literature on each of the elements of the proposed strategy.
 
%--------------------------------------------------------------------------
\subsection[Sensitivity Analysis]{Sensitivity Analysis}\label{sub:intro_sa}
%--------------------------------------------------------------------------

% Introductory Paragraph
An essential part of model development and assessment is properly describing and understanding the impact of model parameter variations on the model prediction.
\Gls[hyper=false]{sa} is an important methodological step in that context \cite{Trucano2006}.
\gls[hyper=false]{sa} is the process of investigating the role of input parameters in determining the model output \cite{Iooss2015}. 
It seeks to quantify the importance of each model input parameter on the output.

% Classifications
Various classifications exist in the literature to categorize \gls[hyper=false]{sa} techniques \cite{Frey2002, Ionescu-Bujor2004, Cacuci2004, Saltelli2008, Iooss2015}.
\marginpar{Classifications}
In the review by Ionescu-Bujor and Cacuci \cite{Ionescu-Bujor2004, Cacuci2004}, 
\gls[hyper=false]{sa} techniques are classified with respect to their scope (local vs. global) and to their framework (deterministic vs. statistical).
In the review of \gls[hyper=false]{sa} methods by Iooss and Lemaître \cite{Iooss2015}, 
and the work by Saltelli et al. \cite{Saltelli2008} and by Santner et al. \cite{Santner2003}, 
the statistical framework is implicitly assumed deriving ideas from design of experiment, 
and the classification is based on the parameter space of interest (local vs. global).

% Local sensitivity analysis
Local analysis is based on calculating the effect on the model output of small perturbations around a nominal parameter value. 
\marginpar{local sensitivity analysis}
Often the perturbation is done one parameter at a time thus approximating the first-order partial derivative of the model output with respect to the perturbed parameter. 
The derivative can be computed through efficient adjoint formulation \cite{Cacuci2003,Cacuci2010} capable of handling large number of parameters.

% Local sensitivity analysis, advantages
Besides being numerically efficient, sensitivity coefficients obtained from local deterministic sensitivity analysis have the advantage of being intuitive in their interpretation, irrespective of the method employed \cite{Razavi2015}.
The intuitiveness stems from the aforementioned equivalence to the derivative of the output with respect to each parameter \cite{Ionescu-Bujor2004} around a specifically defined point (i.e., nominal parameter values). 
Thus the coefficients can be readily compared over different modeled systems, independently of the range of parameters variations.

% Global sensitivity analysis
The global analysis, on the other hand, 
\marginpar{global sensitivity analysis}
seeks to explore the input parameters space across its range of variation and then quantify the input parameter importance based on a characterization of the resulting output response surface. 
In global deterministic framework \cite{Ionescu-Bujor2004,Cacuci2010}, the characterization is aimed at the identification of the system’s critical points (e.g., maxima, minima, saddle points, etc.). 
In statistical global methods \cite{Saltelli2008, Saltelli2004, Saltelli2006}, the characterization is aimed at measuring the dispersion of the output based on variance \cite{Sobol2001,Cukier1978}, correlation \cite{Helton1993}, or elementary effects \cite{Morris1991}.

% Global sensitivity analysis, difficulty
Due to the different characterizations, the global statistical framework can potentially give spurious results not comparable to the results from local method as there is no unique definition of sensitivity coefficient provided by different global methods \cite{Razavi2015}. 
In some cases, different methods can give different and inconsistent parameters importance ranking \cite{Saltelli2008,Saltelli2004}.
Furthermore, the result of the analysis can be highly dependent to the assumed input parameters probability distribution and/or their range of variation \cite{Cacuci2004,Cacuci2010}.

% Global sensitivity analysis, advantages
Yet, despite the aforementioned shortcomings, 
\marginpar{global statistical sensitivity analysis}
the global statistical framework has three particular attractive features relevant to the present study. 
First, the statistical method for sensitivity analysis is non-intrusive in the sense that minimal or no modification to the original code is required. 
In other words, the code can be taken as a black box and the analysis is focused on the input/output relationship \cite{Saltelli2008} of the code. 
This is the case especially in comparison to adjoint-based sensitivity \cite{Cacuci2000,Ionescu-Bujor2000} which is a highly efficient and accurate method applicable to a large number of parameters, 
provided that the code is designed/modified for adjoint analysis.

Second, no a priori knowledge on the model structure (linearity, additivity, etc.) is required. 
Depending on the model complexity and as the parameter variation range can be large, 
the linearity or additivity assumption might not hold.

Third and finally, 
the choice of a statistical framework for sensitivity analysis fits the Monte Carlo (MC)-based uncertainty propagation method widely adopted in nuclear reactor evaluation models \cite{Boyack1990, Nutt2004, Wallis2007, Glaeser2008}. 
The method prescribes that the uncertain model input and parameters (modeled as random variables) 
should be simultaneously and randomly perturbed across their range of variations. 
Multiple randomly generated input values are then propagated through the code to quantify the dispersion of the prediction (e.g., peak cladding temperature) 
which serves as a measure of the prediction reliability. 
Statistical global sensitivity analysis thus complements the propagation step 
by addressing the follow-up question on the identification of the most important parameters in driving the prediction uncertainty. 

% Quantity of Interests
Saltelli et al. \cite{Saltelli2006} emphasized that an analysis using computer simulation 
\marginpar{choosing model output as the quantity of interest}
should be focused on the specific question the simulation is required to answer 
as opposed to the analysis of each and every individual model output. 
This is done through judicious choice of representative quantity of interest (QoI) 
that properly substantiates the problem at hand. 
In particular, computer code output often comes in a form of time series. 
In such case, Saltelli et al. \cite{Saltelli2008,Saltelli2004} proposed to derive the relevant QoI from time-dependent output 
using a predefined scalar function such as the maximum, the minimum, the average, etc. that fits the initial question.

% Function as Model Output
However, in some cases, the whole course of a transient is of primary interest 
\marginpar{function as model output}
such as in assessing the ability of a model to reproduce the overall dynamics of the simulated system. 
If the attention is focused on the overall change in shape of the time-dependent output (a shift in the Y-axis, a delay, a distortion, etc.), 
the descriptions provided by the aforementioned scalar functions might be incomplete and overlook important features of the variation. 
To tackle this problem, Campbell et al. \cite{Campbell2006} proposed to represent the functional (time-dependent) output in a certain basis function expansion 
and to carry out the sensitivity analysis on the coefficients of the expansion. 
In accordance to such approach, \glsfirst[hyper=false]{fda} popularized by Ramsay and Silverman \cite{Ramsay2005} can be useful to reduce the high dimensionality of time-dependent output.

% Recent Development in Nuclear Engineering Application
Despite these recent developments, 
\marginpar{more recent development in nuclear engineering application}
there are very few publications on the application of global sensitivity analysis to nuclear thermal-hydraulics evaluation models specifically dealing with time-dependent output.
Notable recent examples related of sensitivity analysis for a time-dependent \gls[hyper=false]{th} problem were the work done by Ionescu-Bujor et al. \cite{Ionescu-Bujor2005} 
for reflooding experiment of degraded fuel rods, utilizing adjoint sensitivity method; 
by Auder et al. \cite{Auder2012} for pressurized thermal shock analysis, 
utilizing statistical method with emphasis on metamodeling; 
and by Prošek and Leskovar \cite{Prosek2015} for \gls[hyper=false]{lbloca} analysis, 
utilizing \gls[hyper=false]{fftbm} and local sensitivity analysis.

%------------------------------------------------------------------------------
\subsection{Statistical Metamodeling}\label{sub:intro_statistical_metamodeling}
%------------------------------------------------------------------------------

% Introductory paragraph
Many tasks involving computer simulation can be boiled down to making prediction.
\emph{Computer experiment}, an experiment using computer simulation evaluates the output based on different inputs to achieve various objectives.
\marginpar{Computer experiment}
In the aforementioned sensitivity analysis, the objective is to identify influential inputs that drives the variation in the outputs of the computer simulator.
In the forward uncertainty quantification, \gls[hyper=false]{mc} simulation are used to propagate the uncertainty in the inputs to quantify the uncertainty in the simulator prediction using the notion of probability; while in its inverse counterpart, the goal is to identify a region of input parameter space that is consistent with both the observed data and the assumed prior uncertainty of the parameters.
The latter objective, in turn, is related to \emph{optimization} where the goal is to identify particular value of inputs that maximize a certain objective function as computed by the simulator.

% The use of metamodel
The examples of the objectives above are arguably distinct, but they share a common characteristic of involving analyses of outputs from numerous simulator runs.
\marginpar{Complex simulator, expensive simulator}
An increasingly more realistic and complex computer simulator, however, often translates to a long running simulation (i.e., computationally expensive) or requiring vast amount of evaluations due to the complexity of relationship between high-dimensional inputs and high-dimensional outputs (e.g., non-linearities, interaction), or both.
They, in turn, hinder the analyses and the effort to achieve the aforementioned objectives of computer experiment.

% Why Metamodel
As a result, having a fast approximating model of a complex simulator is beneficial in conducting a computer experiment and its value was acknowledged by Sacks et al. in their seminal paper \cite{Sacks1989} and formalized further in several textbooks \cite{Santner2003,Fang2006}.
\marginpar{An approximating model}
The approximating model, while simpler and much faster to evaluate than the \emph{original} simulator, is designed to capture the dominant features of the inputs/outputs relationship of the original complex simulator \cite{Asher2015}.
Capturing the dominant features allows the approximating model to be used in lieu the original simulator in the experiment.
This approximating model in the literature is referred to as \emph{metamodel}, \emph{surrogate model}, \emph{response surface model}, \emph{proxy model}, or \emph{emulator}.

% Broad classification
Nowadays, any of the terms above are used interchangeably and all are used to substitute the original simulator to reduce the computational cost of conducting computer experiment \cite{Simpson2001,Razavi2012,Asher2015}. 
Subtle difference does exist.
Thus, it is worthwhile to consider a broad classification of surrogate models and the approaches to their derivation (i.e., surrogate modeling) according to the literature.
\marginpar{Classification}
Surrogate models according to their derivation can be broadly classified in two categories: the data-driven \emph{response surface surrogates} and the mechanistic \emph{reduced-order models} \cite{Razavi2012}.

% Reduced order model
The reduced order models are perhaps more familiar in the scientific community where a complex physical model is being simplified by putting more stringent assumption or reducing the numerical resolution while trying to preserve the most important physical processes present in the more complex model.
\marginpar{Reduced-order model}
The point neutron kinetics model is an example of a reduced-order model, substituting the more complex $3$-dimensional nodal code;
and loosely speaking, \gls[hyper=false]{th} system code is a reduced-order model of the more expensive multi-phase computational fluid dynamics code.
When applicable, reduced-order models can be useful as first approximation as well as didactic tools to build intuition. 

% Response Surface Surrogate
The response surface surrogates, on the other hand, make no pretext of preserving the underlying physical process modeled in a complex simulator.
\marginpar{Response surface surrogates}
It seeks to \emph{emulate} the relationship (i.e., \emph{mapping}) between inputs and outputs of the simulator.
The term \emph{metamodel} is used throughout the thesis and exclusively refer to this particular type of surrogate model.
The workflow of constructing a response surface surrogate typically consists of three steps.
The first step is to gather the data, that is by running the simulator at limited and selected points across the input parameter space of interest and evaluate its outputs.
The selection of such points are known as the \emph{design of experiment} \cite{Simpson2001a,Santner2003}.
The second step is to choose an approximating function that emulates well the relationship between the inputs and outputs and \emph{train} them based on the data.
Training a surrogate model involves \emph{fitting} the parameters associated with the selected approximating function.
The function is chosen such that it is simpler and faster to evaluate at arbitrary inputs, relative to the original simulator.
Finally, a validation step is conducted to assess the quality of the resulting metamodel.
Variation of this workflow exists, especially whether iteration is carried out \cite{Razavi2012}.

% Gaussian Process, What, Renewed interest due to Machine learning
The surrogate model introduced in the papers of Sacks et al. \cite{Sacks1989,Sacks1989a} were \gls[hyper=false]{gp} metamodel.
\marginpar{Gaussian process metamodel}
The metamodel was constructed as a tool to interpolate between observed data, that is, between the inputs and outputs of actual simulator runs simulator.
Once constructed the output at any arbitrary input point can be predicted faster using the metamodel.
This idea was borrowed from a spatial interpolation tool in geostatistics (where the inputs were spatial coordinates) developed by Krige dating back to the 1950s \cite{Krige1951} and formalized by Matheron in the 1960s \cite{Matheron1963}. 
\gls[hyper=false]{gp} metamodel is arguably the most popular approach to metamodeling (details further below) and it enjoys a renewed interest due to its application for machine learning \cite{Rasmussen2006}.

% Gaussian Process, Why
A \gls[hyper=false]{gp} metamodel is a statistical metamodel.
It is based on the extension of multivariate Gaussian distribution to a continuous multidimensional input parameter space.
Under the Bayesian interpretation, the metamodel assumes a prior distribution over functions to initially describe an unknown complex function that underlies the simulator.
The observed data (i.e., design of experiment plus the corresponding outputs) is then used to update the prior and learn more about the true underlying function. 
Though the simulator itself might be deterministic, the limited size of the observed data renders prediction at arbitrary input not observed \emph{uncertain}.
This measure of uncertainty makes a \gls[hyper=false]{gp} metamodel an attractive choice to be incorporated into a model calibration framework where multiple sources of uncertainty are considered.
The present research adopts \gls[hyper=false]{gp} for constructing a metamodel of a \gls[hyper=false]{th} system code model as detailed in Chapter~\ref{ch:gp_metamodel}.

% Gaussian Process, and others
\gls[hyper=false]{gp} metamodel is by no means the only method to construct a data-driven metamodel, though it can be considered as the most popular choice in the literature (Table~\ref{tab:metamodel_in_literature}).
\marginpar{Other metamodeling approaches}
\emph{Response Surface Method} (RSM), originally developed as a technique in the design and analysis of physical experiments \cite{Box2007}, 
has a long history of being adapted to the design and analysis of computer experiments \cite{Kerrigan1979,Lucia1982,Faravelli1989,Engel1990}.
It is mostly based on either linear or quadratic regression (with interaction terms) (see for instance \cite{Kleijnen2000}, and more recent reviews \cite{Simpson1998,Simpson2001}).
In recent times, other methods such as the ones based on artificial neural network \cite{Fonseca2003} and polynomial chaos expansion \cite{Sudret2008,Sudret2012} have also gained traction.
For comparison, Table~\ref{tab:metamodel_in_literature} shows the search hits from Scopus, an online bibliographic database \cite{Elsevier2017}, for the different select metamodeling approaches. Note that the list is not at all exhaustive.
\begin{table}[ht]
    \myfloatalign
    \caption{Number of publications related to different metamodeling approaches based on Scopus web search as of Feb. $14$. $2017$.}
    \label{tab:metamodel_in_literature}
    \begin{tabularx}{\textwidth}{cXcc} \toprule
        \tableheadline{\scriptsize{Metamodeling}}	& \tableheadline{\scriptsize{Search}}\parnote{\scriptsize{\texttt{(...) AND ("surrogate" OR "metamodel")}}}  & \tableheadline{\scriptsize{Number of}}  & \tableheadline{\scriptsize{Since}} \\ 
				\tableheadline{\scriptsize{Approach}}     & \tableheadline{\scriptsize{Keyword}}  & \tableheadline{\scriptsize{Publications}}  &  \\ \midrule
        \multicolumn{1}{l}{\scriptsize{Gaussian Process / Kriging}} 		& \scriptsize{(\texttt{"Gaussian Process OR kriging"})}		& \scriptsize{$1838$} & \scriptsize{$1992$} \\
        \multicolumn{1}{l}{\scriptsize{Artificial Neural Network}}      & \scriptsize{\texttt{"neural network"}} 	 & \scriptsize{$997$} & \scriptsize{$1993$} \\
        \multicolumn{1}{l}{\scriptsize{Response Surface Method}}			  & \scriptsize{\texttt{"response surface"}} & \scriptsize{$947$} & \scriptsize{$1977$} \\
        \multicolumn{1}{l}{\scriptsize{Polynomial Chaos Expansion}}     & \scriptsize{\texttt{"polynomial chaos"}} & \scriptsize{$208$} & \scriptsize{$2004$} \\ \bottomrule
    \end{tabularx}
		\parnotes
\end{table}

% Development in nuclear engineering application
Metamodel applications have a long history in nuclear engineering analyses due to the complexity of the simulators and the long-understood importance of quantifying the uncertainty in the prediction.
\marginpar{Developments in nuclear engineering application}
As such, historically, metamodels (specifically, of type the response surface method) have been applied for quantifying the prediction uncertainty forward through \gls[hyper=false]{mc} sampling as well as for statistical sensitivity analysis \cite{Cox1977,Ishigami1989}.
The range of applications varied from quantifying the reactor safety margin \cite{Lellouche1990} for a \gls[hyper=false]{lbloca} scenario, propagating the uncertainty of fuel rods failure in the core \cite{Meyder1986} during the same scenarion, to the uncertainty and sensitivity analyses of severe accident progression \cite{Khatib-Rahbar1989}.
In the recent times, more advanced metamodels have been applied to more variety of engineering analysis.
From the design optimization problem of fuel assembly \cite{Raza2008} and spacer grid \cite{Kim2005} to the calibration of physical models in \gls[hyper=false]{th} system code \cite{Wu2017} and fuel performance \cite{Higdon2013}.

%----------------------------------------------------------------------
\subsection{Bayesian Calibration}\label{sub:intro_bayesian_calibration}
%----------------------------------------------------------------------

% Introductory paragraph
The objective of model calibration is to increase the agreement between simulation prediction and its corresponding measurement data by adjusting some of the simulator inputs \cite{Campbell2006,Trucano2006}.
\marginpar{Model calibration, goal and approach}
Traditionally, calibration closely related to an optimization problem with the objective function of some error measure between simulation prediction and measurement data (e.g., root-mean-square-error).
However, statistical approach to calibration using a Bayesian framework has become a popular practice in scientific simulation community.
Instead of minimizing a measure of error, Bayesian framework treats the uncertainty of the inputs probabilistically whose prior probability distribution is updated such that it is consistent with the available, albeit uncertain, measurement data.
The framework offers flexibility in modeling various sources of uncertainty \cite{Kennedy2001,Kaipio2011}.

% Bayesian Calibration
Bayesian framework for the calibration of computer simulation model was popularized by the work of Kennedy and O'Hagan \cite{Kennedy2001}.
\marginpar{Bayesian framework, Kennedy and O'Hagan approach}
The main goal of the framework is similar to any calibration framework, that is to learn the proper values of model parameters along with the associated uncertainty taking into account different sources of uncertainty based on the observed data.
The distinct idea of it, however, is acknowledging that a systematic bias between a physics-based simulator and reality might be present.
Yet, the magnitude and sign of this systematic bias are often not known a priori.
At the same time, if not modeled properly its presence can overfit the calibration of the model parameters.
That is, the calibrated model parameters will be overly sensitive to the calibration data and thus not applicable for prediction.
As such, Kennedy and O'Hagan proposed to model the unknown bias term probabilistically by putting a prior over functions to be updated simultaneously by the observed data.
The proposed prior over function is a \gls[hyper=false]{gp}.
Finally, acknowledging that computational cost of numerous simulator runs might not be negligible, the approach embedded the use of \gls[hyper=false]{gp} metamodel (termed \emph{emulator}) to substitute the original simulator. 
Due to its popularity, and while extension and improvement exist \cite{Bayarri2007,Higdon2008,Arendt2012}, the term \emph{KOH approach} becomes synonymous to this particular approach of computer model calibration \cite{Trucano2006,Hall2011,Ling2014}.
It is adapted here in the present research to deal with the particular problem posed in \gls[hyper=false]{premium} benchmark that itself represents typical problem in the nuclear engineering \gls[hyper=false]{th} analysis.

% Bayesian data analysis
Bayesian framework for model calibration consists of two main steps \cite{Gelman2013}: a formulation of a posterior distribution for the model parameters of interest and the computation involving the posterior distribution, from its summaries to its propagation through some functionals.
The KOH approach for model calibration, in essence, prescribes a probabilistic model for data-generating process of the observed experimental data, incorporating the simulator, the uncertain bias, and the uncertain model parameters into it.
This probabilistic model is then conditioned on the observed data that results in the (i.e., \emph{likelihood}) function.
In Bayesian analysis, any involving unknown parameters associated with the probabilistic model is then assigned prior probability distribution and its specification complete the formulation of the posterior distribution for the parameter of interest.

Regarding the prior, there is a tendency in the literature \cite{Gelman2013,McElreath2015} to reject the old notion of \emph{Bayesianism} where emphasis is put heavily on the prior distribution of the parameters as \emph{subjective opinion} and the results will always be correct according to that prior.
On the contrary, the use of prior should be seen in more pragmatic light, as a starting assumption that can be rejected if not appropriate.
Some motivation for the choice of prior can be even purely numerical. 
Gelman et al.~\cite{Gelman2013} advocates to check the implication of the resulting posterior parameters distribution on the posterior prediction in the sense whether whether the prediction makes sense and useful, but not in the sense of the posterior parameter distribution being \emph{true}.

% Bayesian Computation 1, Analytical and Early Sampler
The computation involving the posterior distribution consist the second 
% Bayesian Computation 2, Advanced Samplers

% Development in nuclear engineering application




