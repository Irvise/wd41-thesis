\section{Multivariate Random Variable (Random Vector)}\label{app:probability}

% Jointly PDF
A collection of finite $D$ continuous random variables (or random vector) $\bm{\mathcal{X}} = [\mathcal{X}_1, \mathcal{X}_2, \cdots, \mathcal{X}_D] \in \mathcal{X} \subseteq \mathbb{R}^D$ is \emph{jointly continuous} if a non-negative \emph{joint probability density function} $p_{\bm{\mathcal{X}}}: \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}_{\geq 0}$ exists such that, for any set of $B \in \chi \subseteq \mathbb{R}^D$, the probability of $\bm{\mathcal{X}}$ belonging to $B$ is defined as,
\begin{equation}
  \mathbb{P} (\bm{\mathcal{X}} \in B) = \int_{\mathbf{x} \in B} p_{\bm{\mathcal{X}}} (\mathbf{x}) d\mathbf{x}
\label{eq:joint_continuous}
\end{equation}

% Valid Joint PDF
Additionally,
the joint density function is also required to sum up to $1.0$ over the whole domain $\mathcal{X}$ for it to be a valid probability density function.
In other words, the probability of $\mathcal{X}$ belongs to the domain $\mathbf{X}$ is 1.0,
\begin{equation}
  \mathbb{P} (\mathcal{X} \in \mathbf{X}) = \int_{\mathbf{x} \in \mathbf{X}} p_{\bm{\mathcal{X}}} (\mathbf{x}) d\mathbf{x} = 1.0
\label{eq:valid_pdf}
\end{equation}

% Convention
In Chapter~\ref{ch:gp_metamodel}, but also elsewhere in this thesis, the type of random variable is restricted to continuous random variable and the term \emph{probability} is often used referring to the \emph{probability density}.
When the distinction is required (such as in the definition Eq.~\ref{eq:joint_continuous} and condition Eq.~\ref{eq:valid_pdf} above) the notations used are $p$ and $\mathbb{P}$ for density and probability, respectively.
Furthermore, the density function of random vector $\bm{\mathcal{X}}$ written as $p_{\bm{\mathcal{X}}} (\mathbf{x})$ is shortened simply to $p (\mathbf{x})$ as it is often clear from the context.

% Partitioning a Random Vector
\marginpar{partitioning random vector}
Now suppose that $\bm{\mathcal{X}}$ is partitioned into two disjoint sets $\bm{\mathcal{X}}_A$ and $\bm{\mathcal{X}}_B$ with $\mathbf{card}(\bm{\mathcal{X}}_A), \mathbf{card}(\bm{\mathcal{X}}_B) \geq 1$ 
such that $\bm{\mathcal{X}} = \{\bm{\mathcal{X}}_A, \bm{\mathcal{X}}_B\}$; 
$p(\mathbf{x}) = p(\mathbf{x}_a,\mathbf{x}_b)$; 
and $\bm{\mathcal{X}}_A \in \mathbf{X}_A \subseteq \mathbb{R}^{D_1}$, $\bm{\mathcal{X}}_B \in \mathbf{X}_B \subseteq \mathbb{R}^{D_2}$, with $D_1 + D_2 = D$.
The symbol $\mathbf{card} (\circ)$ denotes the cardinality of a set, that is the number of elements of the set.
  
%Marginal Probability
\marginpar{marginal probability}
The \emph{marginal} probability of $\bm{\mathcal{X}}_A$ is defined as
\begin{equation}
  p(\mathbf{x}_A) = \int p(\mathbf{x}_A,\mathbf{x}_B) d\mathbf{x}_B
\label{eq:marginal_set_a}
\end{equation}
where the integration is carried out only on the domain of random variables $\bm{\mathcal{X}}_B$, $\mathbf{X}_B \subseteq \mathbb{R}^{D_2}$. 
Note that if $\mathbf{card}(\mathbf{X}_A) \geq 1$ then $p(\mathbf{x}_A)$ itself is a joint probability.
The marginal probability of $\bm{\mathcal{X}}_B$ follows suit,
\begin{equation}
  p(\mathbf{x}_B) = \int p(\mathbf{x}_A,\mathbf{x}_B) d\mathbf{x}_A
\label{eq:marginal_set_b}
\end{equation}
where now the integration is carried out only on the domain of random variables $\bm{\mathcal{X}}_A$, $\mathbf{X}_A \subseteq \mathbb{R}^{D_1}$.

% Conditional Probability
\marginpar{conditional probability}
The \emph{conditional} probability of $\bm{\mathcal{X}}_A$ given (or conditioned on) $\bm{\mathcal{X}}_B$ is defined as,
\begin{equation}
  p(\mathbf{x}_A|\mathbf{x}_B) = \frac{p(\mathbf{x}_A, \mathbf{x}_B)}{p(\mathbf{x}_B)}
\label{eq:conditional_on_b}
\end{equation}
for $p(\mathbf{x}_B) > 0$. That is, the notion of conditional probability cannot be defined given an impossible event, $p(\mathbf{x}_B) = 0$. 
The definition of the conditional probability of $\bm{\mathcal{X}}_B$ given (or conditioned on) $\bm{\mathcal{X}}_A$ follows suit, 
\begin{equation}
  p(\mathbf{x}_B|\mathbf{x}_A) = \frac{p(\mathbf{x}_A, \mathbf{x}_B)}{p(\mathbf{x}_A)}
\label{eq:conditional_on_a}
\end{equation}
for $p(\mathbf{x}_A) > 0$.

% Independence
Random variables $\mathbf{X}_A$ and $\mathbf{X}_B$ are said to be \emph{independent} of each
\marginpar{independence}
other if and only if their joint probability $p(\mathbf{x}_A, \mathbf{x}_B)$ is defined as,
\begin{equation}
  p(\mathbf{x}_A, \mathbf{x}_B) = p(\mathbf{x}_A) p(\mathbf{x}_B)
\label{eq:independence}
\end{equation}
that is, it is the product of the marginals $p(\mathbf{x}_A)$ and $p(\mathbf{x}_B)$.
The random variables $\bm{\mathcal{X}}_A$ and $\bm{\mathcal{X}}_B$ are said to be \emph{dependent} otherwise.
Also, following Eq.~(\ref{eq:conditional_on_b}) and Eq.~(\ref{eq:conditional_on_a}), the two random variables are independent from each other if and only the marginal is equal to the conditional, $p(\mathbf{x}_A) = p(\mathbf{x}_A|\mathbf{x}_B)$ and $p(\mathbf{x}_B) = p(\mathbf{x}_B|\mathbf{x}_A)$.

% Exchangeability
Random variables $\bm{\mathcal{X}}_A$ and $\bm{\mathcal{X}}_B$ are said to be \emph{exchangeable}
\marginpar{exchangeability}
if and only if their joint probability is symmetric \cite{Greenland2005}, that is
\begin{equation}
  p(\mathbf{x}_A, \mathbf{x}_B) = p(\mathbf{x}_B, \mathbf{x}_A)
\label{eq:exchangeable}
\end{equation}

% Bayes' Theorem
The exchangeability of random variables (Eq.~(\ref{eq:exchangeable})) combined with the definition of conditional probability (Eqs.~(\ref{eq:conditional_on_b}) and (\ref{eq:conditional_on_a})) lead to the \emph{Bayes' Theorem},
\marginpar{Bayes' Theorem}
\begin{equation}
  \begin{split}
    p(\mathbf{x}_A | \mathbf{x}_B) & = \frac{p(\mathbf{x}_B | \mathbf{x}_A) p(\mathbf{x}_A)}{p(\mathbf{x}_B)} \\
    p(\mathbf{x}_B | \mathbf{x}_A) & = \frac{p(\mathbf{x}_A | \mathbf{x}_B) p(\mathbf{x}_B)}{p(\mathbf{x}_A)}
  \end{split}
\label{eq:bayes_theorem}
\end{equation}