\section{Multivariate Random Variable (Random Vector)}\label{app:probability}

% Jointly PDF
A collection of finite $D$ continuous random variables (or random vector) $\mathbf{X} = [X_1, X_2, \cdots, X_D] \in \mathcal{X} \subseteq \mathbb{R}^D$ is \emph{jointly continuous} if a non-negative \emph{joint probability density function} $p_{\mathbf{X}}: \mathcal{X} \subseteq \mathbb{R}^D \mapsto \mathbb{R}_{\geq 0}$ exists such that, for any set of $B \in \mathcal{X} \subseteq \mathbb{R}^D$, the probability of $\mathbf{X}$ belongs to $B$ is defined as,
\begin{equation}
  \mathbb{P} (\mathbf{X} \in B) = \int_{\mathbf{x} \in B} p_{\mathbf{X}} (\mathbf{x}) d\mathbf{x}
\label{eq:joint_continuous}
\end{equation}

% Valid Joint PDF
Additionally, the joint density function is also required to sum up to $1.0$ over the whole domain $\mathcal{X}$ for it to a valid probability density function.
In other words, the probability of $\mathcal{X}$ belongs to the domain $\mathcal{X}$ is 1.0,
\begin{equation}
  \mathbb{P} (\mathbf{X} \in \mathcal{X}) = \int_{\mathbf{x} \in \mathcal{X}} p_{\mathbf{X}} (\mathbf{x}) d\mathbf{x} = 1.0
\label{eq:valid_pdf}
\end{equation}

% Convention
In Chapter~\ref{ch:gp_metamodel}, but also elsewhere in this thesis, the type of random variable is restricted to continuous random variable and the term \emph{probability} is often used referring to the \emph{probability density}.
When the distinction is required (such as in the definition Eq.~\ref{eq:joint_continuous} and condition Eq.~\ref{eq:valid_pdf} above) the notations used are $p$ and $\mathbb{P}$ for density and probability, respectively.
Furthermore, the density function of random variable $\mathbf{X}$ written as $p_\mathbf{X} (\mathbf{x})$ is shortened simply to $p (\mathbf{x})$ as it is often clear from the context.

% Partitioning a Random Vector
\marginpar{partitioning random vector}
Now suppose that $\mathbf{X}$ is partitioned into two disjoint sets $\mathbf{X}_A$ and $\mathbf{X}_B$ with $\mathbf{card}(\mathbf{X}_A), \mathbf{card}(\mathbf{X}_B) \geq 1$ 
such that $\mathbf{X}=\{\mathbf{X}_A,\mathbf{X}_B\}$; 
$p(\mathbf{x}) = p(\mathbf{x}_a,\mathbf{x}_b)$; 
and $\mathbf{X}_A \in \mathcal{X}_A \subseteq \mathbb{R}^{D_1}$, $\mathbf{X}_B \in \mathcal{X}_B \subseteq \mathbb{R}^{D_2}$, with $D_1 + D_2 = D$.
  
%Marginal Probability
\marginpar{marginal probability}
The \emph{marginal} probability of $\mathbf{X}_A$ is defined as
\begin{equation}
  p(\mathbf{x}_A) = \int p(\mathbf{x}_A,\mathbf{x}_B) d\mathbf{x}_B
\label{eq:marginal_set_a}
\end{equation}
where the integration is carried out only on the domain of random variables $\mathbf{X}_B$, $\mathcal{X}_B \subseteq \mathbb{R}^{D_2}$. 
Note that if $\mathbf{card}(\mathbf{X}_A) \geq 1$ then $p(\mathbf{x}_A)$ itself is a joint probability.
The marginal probability of $\mathbf{X}_B$ follows suit,
\begin{equation}
  p(\mathbf{x}_B) = \int p(\mathbf{x}_A,\mathbf{x}_B) d\mathbf{x}_A
\label{eq:marginal_set_a}
\end{equation}
where now the integration is carried out only on the domain of random variables $\mathbf{X}_A$, $\mathcal{X}_A \subseteq \mathbb{R}^{D_1}$.

% Conditional Probability
\marginpar{conditional probability}
The \emph{conditional} probability of $\mathbf{X}_A$ given (or conditioned on) $\mathbf{X}_B$ is defined as,
\begin{equation}
  p(\mathbf{x}_A|\mathbf{x}_B) = \frac{p(\mathbf{x}_A, \mathbf{x}_B)}{p(\mathbf{x}_B)}
\label{eq:conditional_on_b}
\end{equation}
for $p(\mathbf{x}_B) > 0$. That is, the notion of conditional probability cannot be defined given an impossible event, $p(\mathbf{x}_B) = 0$. 
The definition of the conditional probability of $\mathbf{X}_B$ given (or conditioned on) $\mathbf{X}_A$ follows suit, 
\begin{equation}
  p(\mathbf{x}_B|\mathbf{x}_A) = \frac{p(\mathbf{x}_A, \mathbf{x}_B)}{p(\mathbf{x}_A)}
\label{eq:conditional_on_a}
\end{equation}
for $p(\mathbf{x}_A) > 0$.

% Independence
Random variables $\mathbf{X}_A$ and $\mathbf{X}_B$ are said to be \emph{independent} of each
\marginpar{independence}
other if and only if their joint probability $p(\mathbf{x}_A, \mathbf{x}_B)$ is defined as,
\begin{equation}
  p(\mathbf{x}_A, \mathbf{x}_B) = p(\mathbf{x}_A) p(\mathbf{x}_B)
\label{eq:independence}
\end{equation}
that is, it is the product of the marginals $p(\mathbf{x}_A)$ and $p(\mathbf{x}_B)$.
The random variables $\mathbf{x}_A$ and $\mathbf{X}_B$ are \emph{dependent} otherwise.
Also, following Eq.~(\ref{eq:conditional_on_b}) and Eq.~(\ref{eq:conditional_on_a}), the two random variables are independent to each other if and only the marginal is equal to the conditional, $p(\mathbf{x}_A) = p(\mathbf{x}_A|\mathbf{x}_B)$ and $p(\mathbf{x}_B) = p(\mathbf{x}_B|\mathbf{x}_A)$.

% Exchangeability
Random variables $\mathbf{X}_A$ and $\mathbf{X}_B$ are said to be \emph{exchangeable}
\marginpar{exchangeability}
if and only if their joint probability is symmetric, that is
\begin{equation}
  p(\mathbf{x}_A, \mathbf{x}_B) = p(\mathbf{x}_B, \mathbf{x}_A)
\label{eq:exchangeable}
\end{equation}

% Bayes' Theorem
The exchangeability of random variables (Eq.~(\ref{eq:exchangeable})) combined with the definition of conditional probability (Eqs.~(\ref{eq:conditional_on_b}) and (\ref{eq:conditional_on_a})) lead to the \emph{Bayes' Theorem},
\marginpar{Bayes' Theorem}
\begin{equation}
  \begin{split}
    p(\mathbf{x}_A | \mathbf{x}_B) & = \frac{p(\mathbf{x}_B | \mathbf{x}_A) p(\mathbf{x}_A)}{p(\mathbf{x}_B)} \\
    p(\mathbf{x}_B | \mathbf{x}_A) & = \frac{p(\mathbf{x}_A | \mathbf{x}_B) p(\mathbf{x}_B)}{p(\mathbf{x}_A)}
  \end{split}
\label{eq:bayes_theorem}
\end{equation}