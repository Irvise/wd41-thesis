%*********************************************
\section{Markov Chain}\label{app:markov_chain}
%*********************************************

%---------------0---------------------------------
\subsection{Discrete-State}\label{app:mc_discrete}
%-------------------------------------------------

% Markov Chain
Markov chain on a discrete-state space $\mathcal{S}$ is defined as a sequence of random variables $\{\mathcal{X}^{(i)}, i \geq 0\}$ where the indices represents successive time, step, or iteration,
\emph{such that the conditional probability distribution of $\mathcal{X}^{(i+1)}$ follows the Markov assumption}.
\marginpar{Markov chain}
That is,
\begin{equation}
  \mathcal{X}^{(i+1)} | \mathcal{X}^{(i)}, \mathcal{X}^{(i-1)}, \ldots, \mathcal{X}^{(0)} = \mathcal{X}^{(i+1)} | \mathcal{X}^{(i)}
\label{eq:markov_property}
\end{equation}
Put differently, the future value depends on the past only through the present value \cite{Geyer2011} and Sokal.

% Ingredients
A discrete-state Markov chain is fully defined by its joint probability (cite Sokal).
\begin{equation}
	\begin{split}
  \mathbb{P}(\mathcal{X}^{(i+1)} & = x^{(i+1)}, \mathcal{X}^{(i)} = x^{(i)}, \ldots, \mathcal{X}^{(0)} = x^{(0)}) = \\
	& \mathbb{P}(\mathcal{X}^{(0)} = x^{(0)}) \cdot \mathbb{P}(\mathcal{X}^{(1)} = x^{(i+1)} | \mathcal{X}^{(0)} = x^{(i+1)}) \cdot \ldots \\
	& \cdot \mathbb{P}(\mathcal{X}^{(i+1)} | \mathcal{X}^{(i)}) \cdot
	\end{split}
\label{eq:markov_chain_joint_probability}
\end{equation}
The specification consists of three main components:
\begin{itemize}

	\item The state space $\mathcal{S}$, all the possible outcomes of the random variables $\{\mathcal{X}^{(i)}\}$.
	\marginpar{Discrete state space}
	The state space considered here is discrete with $D$ elements, $\mathcal{S} = \{x_1, x_2, \ldots, x_D \}$.
	
	\item The initial probability distribution $\pi^{(0)}$.
	\marginpar{Initial distribution}
	This is the (marginal) probability distribution of $\mathcal{X}^{(0)}$ (or the marginal distribution of the chain at $i = 0$). That is,
	\begin{equation}
		\mathbb{P}(\mathcal{X}^{(0)} = x_d) = \pi^{(0)}_d \,\,\, d = 1, \ldots, D
	\label{eq:markov_chain_initial_distribution}
	\end{equation}
	In discrete-state Markov chain, the distribution can be expressed as a $D$-dimensional vector.
	
	\item The \emph{transition probability matrix} $P$, an $D \times D$ with elements $p_{x,y} \geq 0.0$ and $\sum_y p_{x,y} = 1.0$.
	\marginpar{Transition probability}
	Each element is the conditional probability between two states. That is,
	\begin{equation}
		p_{x,y} = \mathbb{P}(\mathcal{X}^{(i+1)} = y | \mathcal{X}^{(i)} = x) \,\,\, \forall x,y \in \mathcal{S}
	\label{eq:markov_chain_transition_matrix_element}
	\end{equation}
	Transition probability matrix is said to be \emph{stationary} if it does not depend on a particular step $i$.
	\marginpar{Stationary transition probability}
	In practice, most \gls[hyper=false]{mcmc} algorithms rely on a stationary transition probability \cite{Geyer2011}.
\end{itemize}

% Example of Chain
As an example of a discrete-state Markov chain, consider a $3$-state Markov chain representing changes of human health condition with $\mathcal{S} = \{\text{Healthy}, \text{Sick}, \text{Dead}\}$ and a transition probability matrix $P$,
\begin{equation}
  P =  
    \begin{pmatrix}
      \mathbb{P}(H | H)  & \mathbb{P}(S | H) & \mathbb{P}(D | H)\\
      \mathbb{P}(H | S)  & \mathbb{P}(S | S) & \mathbb{P}(D | S)\\
      \mathbb{P}(H | D)  & \mathbb{P}(S | D) & \mathbb{P}(D | D)\\
    \end{pmatrix} =
		\begin{pmatrix}
		  0.75  & 0.20 & 0.05\\
      0.65  & 0.15 & 0.20\\
      0.00  & 0.00 & 1.00\\
		\end{pmatrix}
\label{eq:app_transition_probability_matrix}
\end{equation}
The Markov chain is graphically represented in Fig.~\ref{fig:app_markov_chain} using a \emph{state transition diagram}.
\begin{figure}[bth]
	\centering
	\includegraphics[width=0.9\textwidth]{../figures/chapter5/figures/markov_chain.pdf}
	\caption[Illustration of a $3$-State Markov Chain]{An illustration of a $3$-State Markov chain with the transition probability given by the matrix $P$ in Eq.~(\ref{eq:app_transition_probability_matrix}).}
	\label{fig:app_markov_chain}
\end{figure}

% Marginal Distribution at n-th transition, !!!Citation Needed!!!
The probability A transition from state $x$ to $y$ in one step is given by,
\begin{equation}
	\mathbb{P}(\mathcal{X}^{(i+1)} = y) = \sum_{x \in S} \mathbb{P}(\mathcal{X}^{(i+1)} = y | \mathcal{X}^{(i)} = x) \cdot \mathbb{P}(\mathcal{X}^{(i)} = x)
\label{eq:app_markov_chain_one_step}
\end{equation}
Thus, given those three components, the marginal probability distribution at any given step can be defined.
\marginpar{Marginal distribution of state at step $n$}
Furthermore, the expression can be written in compact form as follows.
For instance $1$-step transition from initial distribution $\pi^{(0)}$ by transition probability matrix $P$ yield the distribution,
\begin{equation}
	\pi^{(1)} = \pi^{(0)} P
\label{eq:app_markov_chain_one_step}
\end{equation}
Where $\pi^{(1)}$ is the probality distribution of states at step $1$.
In general,
\begin{equation}
	\begin{split}
		\pi^{(2)} & = \pi^{(1)} P \\
		          & \vdots \\
		\pi^{(n)} & = \pi^{(n-1)} P \\
		          & = \pi^{(0)} P^n\\
	\end{split}
\label{eq:app_markov_chain_one_step}
\end{equation}
where the $n$-th power of $P$ is called the $n$-step transition probability matrix.
\marginpar{$n$-step transition probability}
In other words, the state at step $n$ is distributed as the $n$-times transition of the initial distribution.
Moreover, due to the stationarity of the transition probability,
this result also holds for an $n$-step transition of a distribution at any starting point,
that is $\pi^{(i+n)} \sim \pi^{(i)} P^{(n)}$.  

% Irreducible Chain, !!!Citation Needed!!!
A Markov chain is said to be \emph{irreducible} if each state in the state space $\mathcal{S}$ can be reached eventually from any other state.
\marginpar{Irreducibility}
Irreducibility is a property of the transition probability matrix $P$ (i.e., having an irreducible transition probability matrix).
Formally,
\begin{equation}
	\forall x, y \in \mathcal{S}, \, \exists n \geq 0 \,\, \text{for which} \,\, p_{x,y}^{(n)} > 0
\label{eq:app_irreducibility}
\end{equation}
Based on this definition the transition matrix of Eq.~(\ref{eq:app_transition_probability_matrix}) is not irreducible as the state of being \emph{Dead} does not allow transition to any of the two other states.
An example of irreducible chain is given in a graphical representation of Fig.~\ref{fig:app_markov_chain_irreducible}.
Note that while state $B$ is not directly connected to state $A$,
the state can eventually be reached from state $B$ through the connection of state $C$ (in this case, $n$ is equal to $2$).
\begin{figure}[bth]
	\centering
	\includegraphics[width=1.0\textwidth]{../figures/chapter5/figures/markov_chain_irreducible.pdf}
	\caption[Illustration of an irreducible $3$-State Markov Chain]{An illustration of an irreducible $3$-State Markov chain.}
	\label{fig:app_markov_chain_irreducible}
\end{figure}

% Period of a Chain and Aperiodicity, !!!Citation Needed!!!
A period of a state $x \in \mathcal{S}$ denoted as $d_x$ is defined for each state in the chain as follow,
\marginpar{period of a state}
\begin{equation}
	d_x = \text{GCD}\,\,\{n: p_{x,x}^{(n)} > 0, n > 0\} \,\, \forall x \in \mathcal{S}
\label{eq:app_markov_chain_period}
\end{equation}
where $\text{GCD}$ stands for the \emph{Greatest Common Divisor} of the set.
In other words, it is the minimum number of transition for any given state to return to the state.

In an arbitrary discrete-state Markov chain, different states might have different periods.
\marginpar{periodic, aperiodic chain}
A state is called \emph{aperiodic} if its period is equal to $1$ and it is called \emph{periodic} otherwise.
If a chain has the same period $d > 1$ for each of its states then the chain is called \emph{periodic} (see Fig.~\ref{fig:app_markov_chain_periodic} for a periodic chain with period $3$).
A periodic chain exhibits a non-stochastic behavior in their dynamic.
On the contrary, a chain having the same period of $1$ for each of its states is called a \emph{aperiodic} chain (see Fig.~\ref{fig:app_markov_chain_aperiodic} for an example of a aperiodic chain).
\normdoublefigure[pos=tbhp,
                  mainlabel={fig:app_markov_chain_periodicity},
                  maincaption={Examples of periodic and aperiodic chains. (Left) an example of a periodic $3$-state Markov chain. In this case, all states have the the same period of $3$ steps. (Right) an example of aperiodic $3$-state Markov chain, that is all states are aperiodic.},%
									mainshortcaption={Examples of periodic and aperiodic chains.},
                  leftopt={width=0.45\textwidth},
                  leftlabel={fig:app_markov_chain_periodic},
                  leftcaption={Periodic chain},
                  %leftshortcaption={},%
                  rightopt={width=0.45\textwidth},
                  rightlabel={fig:app_markov_chain_aperiodic},
                  rightcaption={Aperiodic chain},
                  %rightshortcaption={},
                  %spacing={\hfill}
                 ]
{../figures/chapter5/figures/markov_chain_periodic}
{../figures/chapter5/figures/markov_chain_aperiodic}

% Stationary Distribution, !!!Citation Needed!!!
Some distribution are \emph{stationary} with respect to a transition probability matrix.
\marginpar{Stationary distribution}
Specifically, $\pi^*$ is \emph{stationary for} $P$ if,
\begin{equation}
	\pi^* = \pi^* P
\label{eq:app_markov_chain_stationary}
\end{equation}	
Put differently, the distribution is \emph{invariant} under transition.
Consequently, if stationary distribution exists,
once the chain reaches the stationary distribution, it will remain there and the chain itself becomes stationary. 
Stationary distribution need not exist for a given $P$,
but in the application of \gls[hyper=false]{mcmc} algorithms,
the existence of stationary distribution is guaranteed \cite{Geyer2011}.

% Example of Stationary Distribution
As an example of a stationary distribution, consider once more the transition probability matrix $P$ in Eq.~(\ref{eq:app_transition_probability_matrix}).
For this transition, the distribution $\pi = [0.0, 0.0, 1.0]$ is stationary with respect to $P$ such that
\begin{equation}
	\pi = \pi P \Leftrightarrow [0.0, 0.0, 1.0] = [0.0, 0.0, 1.0] 		\begin{pmatrix}
		  0.75  & 0.20 & 0.05\\
      0.65  & 0.15 & 0.20\\
      0.00  & 0.00 & 1.00\\
		\end{pmatrix}
\label{eq:app_markov_chain_stationary_example}
\end{equation}
Stating that the stationary distribution is being dead, eventually and definitely.

% Markov Chain Convergence
The notions of irreducibility, aperiodicity, and stationarity are cobbled together to arrive at an important result in the discrete-state Markov chain and it is stated here without proof (citation needed).
\marginpar{Fundamental theorem of Markov chain}
Let $P$ be a transition probability matrix, irreducible and aperiodic, then $P$ has exactly one stationary distribution $\pi^*$ and for any initial distribution $\pi^{(0)}$
\begin{equation}
	\lim_{t \rightarrow \infty} |\pi^{(0)}P^t - \pi^*| = 0
\label{eq:app_markov_chain_convergence}
\end{equation}
That is, the chain converges \emph{in distribution} to the stationary distribution regardless its initial distribution.
The theorem also indicates the existence of a \emph{limiting distribution} $\lim_{t \rightarrow \infty} \pi^{(0)}P^t$.

% MCMC Algorithm
The theorem justifies the use of Markov chain to generate samples for Monte Carlo application from an arbitrary distribution of interest.
\marginpar{Markov Chain Monte Carlo}
This is what \gls[hyper=false]{mcmc} algorithms do.
In such an algorithm the task is to come up with transition probability such that the limiting distribution of the Markov chain, over many iterations, converges to the distribution of interest (the so-called \emph{target} distribution).
Stated differently, the stationary distribution of the Markov chain is aimed to be the distribution of interest.

%-----------------------------------------------------
\subsection{Continuous-State}\label{app:mc_continuous}
%-----------------------------------------------------

% Markov Chain

% Ingredients

% Transition Kernel

% Ergodic Theorem

% Some difficulty

